\documentclass[10pt, oneside]{article} 
\usepackage{amsmath, amsthm, amssymb, calrsfs, wasysym, verbatim, bbm, color, graphics, geometry, textcomp}

\geometry{tmargin=.75in, bmargin=.75in, lmargin=.75in, rmargin = .75in}  

\newcommand{\R}{\mathbb{R}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\Cdot}{\boldsymbol{\cdot}}
\newcommand{\iidsim}{\overset{\text{iid}}{\sim}}
\newcommand{\ind}{\perp\!\!\!\!\perp} 

\newtheorem{thm}{Theorem}
\newtheorem{defn}{Definition}
\newtheorem{conv}{Convention}
\newtheorem{rem}{Remark}
\newtheorem{lem}{Lemma}
\newtheorem{cor}{Corollary}

\usepackage[hidelinks]{hyperref}

\title{ECON 320}
\author{Alexander Liu}
\date{Fall 2024}

\begin{document}

\fontsize{12pt}{15pt}\selectfont

\maketitle
\tableofcontents

\vspace{.5in}

\section{Review}
\subsection{Introduction}
\begin{itemize}
    \item All lectures are self-contained (just have to follow lectures to do perfect in the class)
\end{itemize}
\subsection{What is a Random Variable?}
\begin{itemize}
    \item What's out of our control even if we control all the parameters of our problem.
    \item \textbf{A R.V. is a function mapping a sample space (domain: probability space) into real line $\R$ (range)}. In other words, the sample space is the input and real line is the range of outputs (domains/range from algebra)
    \begin{itemize}
        \item Lay explanation: unknown variable that takes in an input and produces an output
    \end{itemize}
    \item Real line: the range of values the R.V. can map to
    \item Two types of R.V.: Discrete and Continuous
    \begin{itemize}
         \item Discrete R.V.: X takes on one of a finite or countably infinite (ie. the integers) number of values
        \item Continuous R.V.: $P\{X=x\}=0, \forall x \in \R$
    \end{itemize}
\end{itemize}
\subsection{CDF (cumulative distribution function, $F_X(x)$) and PDF (probability density function, $f_X(x)$)}
\begin{itemize}
    \item Derivative of CDF is PDF ($f_X(x)=\frac{dF(x)}{dx})$, integral of PDF is CDF ($F_X(x)=\int^x_{-\infty} f_X(z)dz$)
    \item A pdf can have values \textbf{GREATER} than 1, but cdf \textbf{MUST} sum to $1$
    \item Properties of Probability Density Functions (pdf):
    \begin{itemize}
        \item $f_X(x)\geq 0,$ and either
        \item $\sum^n_{i=1}f_X(x_i)=1$ (discrete)
        \item $\int^{\infty}_{\-\infty} f_X(x)dx = 1$ (continuous)
    \end{itemize}
\end{itemize}
\subsection{Expected Values}
\begin{itemize}
    \item $E(X)=\int_{-\infty}^{\infty} xf(x)dx\equiv \mu_X$
    \item $E(g(X))=\int_{-\infty}^{\infty} g(x)f(x)dx$
    \item $E$ can be thought as an integral
    \begin{itemize}
        \item $E[f(x)] =\int_{-\infty}^{\infty} f(x)dx$
    \end{itemize}
    \item an Expectation can be thought of as linear
    \item $E[aX+bX] = aE[X] + bE[X]$
    \item \textbf{IMPORTANT}: $E[XY] \neq E[X]E[Y]$ \textbf{UNLESS THEY ARE INDEPENDENT}
    \item $E[X\varepsilon]$, $\varepsilon$ is a R.V. In this case, the expectation is a double integral (integrate with respect to X, and with respect to $\varepsilon$). So, $\iint x\cdot \varepsilon x    f(x,\varepsilon)dxd\varepsilon$, (joint function times joint density function)
    \item Properties of Expectation Operator
    \begin{itemize}
        \item $E[a] = a, \forall \text{ constants a}$
        \item $E[bX] = bE[X], \forall \text{ constants } b, R.V.\text{ }X$
        \item $E[g(X) + h(X)] = E[g(X)] + E[h(X)], \forall g(\cdot), h(\cdot) \Rightarrow E[a+bX]=a+bE[X]$
        \item Variance of a r.v. X (see derivation below): $var(X)=E(X^2)-\mu_X ^2$ 
        \item $Var(aX)=E[(aX)^2]-E([aX])^2=a^2 E[X^2]-a^2(E[X])^2$, where (a,b) are constants
    \end{itemize}
\end{itemize}
\subsection{Variance}
\begin{itemize}
    \item $\sigma^2 _X = var(X)=E[(X-\mu_X)^2]=E(X^2-2X\mu_X+\mu^2_X)=E(X^2)-2\mu_X E(X)+\mu^2 _X = E(X^2)-\mu^2_X$ (remember $-2\mu_X E[X] = -2\mu_X \cdot \mu_X\text{, since } E[X] = \mu_X$)
    \item Standard Deviation: $\sigma_X=\sqrt{\sigma^2 _X}=\sqrt{var(X)}$
    \item $var(a+bX) =b^2var(X)$, where a,b are constants
    \begin{itemize}
        \item $Var(aX)=E[(aX)^2]-E([aX])^2=a^2 E[X^2]-a^2(E[X])^2$
    \end{itemize}
\end{itemize}
\subsection{Bivariate/Multivariate R.V's}
\begin{itemize}
    \item X,Y have the joint cdf $F_{X,Y}(x,y)=P(X\leq x, Y\leq y)=\int^{x}_{-\infty}\int^{y}_{-\infty} f_{X,Y} (u,v) dvdu$
    \item \textbf{Marginal cdf}: $F_X(x) =P(X\leq x)=\int _{-\infty} ^ x\int^{\infty}_{-\infty} f_{X,Y}(u,v)dvdu\equiv \int^{x}_{-\infty} f_X(u)du$
    \item \textbf{Marginal pdf}: $f_X(x) =P(X\leq x)=\int _{-\infty} ^ {\infty} f_{X,Y}(u,v)dv$. In other words, take integral (a slice) along one axis
    \item \textbf{Independence:} X and Y are independent iff
    \begin{itemize}
        \item $f_{X,Y} (x,y)=f_X(x) f_Y(y), $ so \\
        $P(X\leq x, Y\leq y) = \int^{x}_{-\infty}\int^{y}_{-\infty} f_X(u)f_Y(v) dvdu=P(X\leq x)P(Y\leq y)$
    \end{itemize}
    \item \textbf{Conditional pdf}: $\frac{\text{joint distribution}}{\text{marginal distribution}}$
    \begin{itemize}
        \item Conditional pdf of $Y$ given $X=x:f_{Y|X}(y|x) =\frac{f_{X,Y} (x,y)}{f_X (x)}, [=\frac{\text{joint}}{\text{marginal}}]$
        \item \textbf{Conditional pdf and independence}: Y and X are independent iff $f_{Y|X} (y|x) =f_Y (y)$
        \item \textbf{Covariance:} $cov(X,Y)=E[(X-\mu_X)(Y-\mu_Y)]=E(XY)-\mu_X\mu_Y$
    \end{itemize}
\end{itemize}
\subsection{Covariance}
\begin{itemize}
    \item $cov(X,Y)=E[(X-\mu_X)(Y-\mu_Y)]=E(XY)-\mu_X\mu_Y$ $(a^2-b^2) = E(XY) - E(X)E(Y)$
\end{itemize}

\subsection{General Rules for Means, Variances, and Covariances}
\begin{itemize}
    \item 1) $E(a+bX+bY+dZ+...)=a+bE[X]+cE[Y]+dE[Z]+...$
    \item 2) $cov(a+bX, c+dY)=bdcov(X,Y)$
    \item 3) \textbf{IMPORTANT (like binomial formula $(a+b)^2$):} $var(a+bX+cY)=b^2 var(X)+ c^2 var(Y)+2bccov(X,Y)$
    \item $X$ and $Y$ are independent $\Rightarrow cov(X,Y)=0$ [Note: Converse is not true]
\end{itemize}
\subsection{Standardized Variables and Correlation}
\begin{itemize}
    \item treat the bar as indicating standardized variables here
    \item Let $\tilde{X} = \frac{X-\mu_X}{\sigma_x}$ and $\tilde{Y}
    =\frac{Y-\mu_Y}{\sigma_Y}$ $\leftarrow$ like Normalization layer in Transformer architecture, containing range
    \item Let (standardization): $E(\tilde{X})=0=E(\tilde{Y})$
    \item Why?
    \begin{itemize}
        \item $E(\tilde X) = E[\frac{X-\mu_X}{\sigma_X}]=\frac{1}{\sigma_X}E[X-\mu_X]=\frac{1}{\sigma_X} E[X] -\mu_X=\frac{1}{\sigma_X}(\mu_X-\mu_X)=0$
        \item $var(\tilde X) = \sigma ^{-2} var(X)=1=var(\tilde Y)$
    \end{itemize}
    \item $var(\tilde X) = 1= var(\tilde Y)$ \textbf{check if X is supposed to be barred}
    \item Why?
    \begin{itemize}
        \item $var(\tilde X)=\sigma^{-2} var(X-\mu_X) \text{ (factoring out constant from var() squares it)}=\sigma^{-2} var(\tilde X)= \sigma^{-2} \times \sigma^2 = 1 = var(\tilde Y)$
    \end{itemize}
    \item The \textbf{Correlation} between X and Y is $\rho_{X,Y}=\frac{cov(X,Y)}{\sigma_X \sigma_Y} = cov(\tilde X, \tilde Y)$. 
    \begin{itemize}
        \item Correlation is the covariance between standardized variables. Since mean doesn't matter here (constants removed in cov()), we only need to standardize with $\sigma$
    \end{itemize}
\end{itemize}
\subsection{Normal Distribution (Gaussian)}
\begin{itemize}
    \item $X \sim N(\mu,\sigma ^2)$
    \begin{itemize}
        \item Standard Normal: $X\sim N(0,1)$
    \end{itemize}
    \item $f_X(x)=\frac{1}{\sqrt{2\pi\sigma^2}}exp\{-\frac{1}{2}(\frac{x-\mu}{\sigma})^2\}$
\end{itemize}
\subsection{Useful Results for Gaussian}
Emphasis on results 3, 4, 6:
\begin{itemize}
    \item 1) Sum of Normals are Normals
    \item 2) $X_i \sim N(0,1) \Rightarrow X_i^2 \sim \chi_1^2$ (chi dist., 1 dof)
    \item \textbf{3)} $X_i \sim N(0,1), i=1,...,n$ independent $\Rightarrow \sum_{i=1}^n X_i^2 \sim \chi_n^2$ (n dof)
    \item \textbf{4)} $X\sim N(0,1)$ and $U\sim \chi_n^2$ with X, U independent $\Rightarrow \frac{Z}{\sqrt{U/n}} \sim t_n$, Normal Distribution / Chi-Square = t distribution
    \item Below are F-test/distribution observations:
    \item 5) $U\sim \chi_{n_1}^2$ and $V\sim \chi_{n_2}^2$ with U, V independent $\Rightarrow \frac{U/n_1}{V/n_2} \sim F_{n_1, n_2}$ (independent chi-distributions normalized by dof create F-distribution)
    \item \textbf{6)} (based on (2) and (5)) $X\sim N(0,1)$ and $U\sim \chi_n^2$ w/ X,U independent $\Rightarrow \frac{X^2}{U/n}\sim F(1,n)$ (normal dist. \textbf{squared} over chi dist. forms F-dist)
\end{itemize}


End first-day end page 3. Useful Results for Gaussian incomplete

\subsection{Convergence in Probability}
\begin{itemize}
    \item Supposed $Y_1, Y_2,...$ is a sequence of random variables. This sequence converges to a number $b$ if the probability distribution of $Y_n$ becomes more and more concentrated around $b$ as $n\rightarrow \infty$. $lim_{n\rightarrow \infty} Y_n = b$
    \begin{itemize}
        \item Formally, The sequence $\{Y_n\} \text{ converges in probability to } b \text{ if }\forall \epsilon > 0, P(|Y_n-b| < \epsilon) \rightarrow 1 \text{ as } n\rightarrow \infty$
    \end{itemize}
\end{itemize}

\subsection{Law of Large Numbers}
\begin{itemize}
    \item The sample mean of R.V. X approaches population mean $\mu = E[X_i]$ as sample size $n$ increases
    \item Formally, suppose $X_1, ..., X_n$ form a random sample form a distribution for which the mean is $\mu =E(X_i)$, and let $\bar X_n $ denote the sample mean. Then $plim \bar X_n = E(X_i)$
\end{itemize}

\subsection{Central Limit Theorem}
\begin{itemize}
    \item Formally, suppose $X_1, ..., X_n$ denotes n independently and identically distributed (iid) random variables with the same pdf with mean $\mu$ and variance $\sigma^2$. As $n\rightarrow \infty, \bar X_n \sim N(\mu, \frac{\sigma^2}{n})$
    \item So, sample mean $\bar X_n $ approaches the normal distribution with mean $\mu$ and variance $\frac{\sigma^2}{n}$, regardless of the form of the pdf of the $X_i$'s, (so even when it is NOT normal)
\end{itemize}

\section{Estimators and the Linear Model}

\subsection{Estimators}
\begin{itemize}
    \item We need to find parameters such as $\mu$ or $\sigma^2$
    \item An estimator is a mathematical expression that approximates the values of these parameters in terms of available observations
    \item Important: The estimator is a R.V., since it is a function of other random variables
\end{itemize}

\subsection{Criteria of Estimators}
\begin{itemize}
    \item An estimator $\hat \mu$ (hat indicates estimator) of $\mu$ is \textbf{unbiased} if $E(\hat \mu) = \mu$
    \item An estimator is \textbf{efficient} if $var(\hat \mu) \leq var(\tilde{\mu})\text{ } \forall \text{ unbiased estimators } \tilde \mu$. Let $\sim$ signify unbiased estimator
    \item Think of ``bias" as being related to mean and ``efficient" as being related to variance
    \item \textbf{Mean Square Error} of $\hat \mu$ is $MSE(\hat \mu)\equiv E[(\hat \mu -\mu)^2]$
    \begin{itemize}
        \item $MSE(\hat \mu) \equiv (E[\hat \mu] -\mu)^2 + var(\hat \mu) = [Bias(\hat \mu)]^2 + var(\hat \mu)$
    \end{itemize}
    \item Estimators discussed so far don't concern the size of the sample. But some may prefer large samples (can shrink bias and/or variance). In such cases, estimators may be judged on its asymptotic properties - properties in very large samples
\end{itemize}

\subsection{The Simple Linear Model}
\begin{itemize}
    \item $Y_i =\beta_1 + \beta_2 X_i + \epsilon_i$
    \begin{itemize}
        \item We want to find a line that minimizes $\sum \epsilon_i$ (line of ``best fit")
        \item We assume a line that is supplemented by our assumption. Only $X_i$ is observed
        \item Only thing that is ``random" by nature is $\epsilon_i$
        \begin{itemize}
            \item In other words: $Y=f(\epsilon_i)$. NEVER prove anything with $Y_i$. ALWAYS plug in the formula and work with $\epsilon_i$, since it's the only thing we have to address due to its randomness
        \end{itemize}
    \end{itemize}
\end{itemize}

\subsection{Basic Assumptions about Linear Model}
\begin{itemize}
    \item Homoskedasticity: $E(\epsilon_i^2) = var(\epsilon_i) = \sigma^2, \forall i$. The dispersion of each data point remains constant
    \item No Serial Correlation: $E(\epsilon_i \epsilon_j) = 0$ if $i\neq j$. No relationship between data
\end{itemize}

\subsection{Some Implications of Basic Assumptions}
\begin{itemize}
    \item Always focus on reducing $Y_i$ to $\epsilon_i$
    \item $E(Y_i)=E(\beta_1 + \beta_2 X_i + \epsilon_i) =\beta_1 + \beta_2 X_i + E(\epsilon_i) =\beta_1+\beta_2 X_i$. Expectation of $\beta_1 + \beta_2 X_i$ is itself since it is nonrandom. $E[\epsilon_i]=0$
    \item $var(Y_i) = E[(Y_i -EY_i)^2] = E[(\beta_1+\beta-2 X_i + \epsilon_i -\beta_1 -\beta_2 X_i)^2]=E(\epsilon_i ^2) \textbf{(homoskedasticity)}= \sigma ^2$
    \item $cov(Y_i, Y_j) = 0, \forall i \neq j$
\end{itemize}

EPSILON IS AN ASSUMPTION AND IS \textbf{NEVER} OBSERVED. ALL WE KNOW IS $E[\epsilon_i] = 0$

\subsection{Estimation of the regression coefficients $\beta_1, \beta_2$}
\begin{itemize}
    \item How do we choose the ``line of best fit"? Broadly speaking, minimize error, but what is error? Also broadly speaking, it is an estimator of $\epsilon_i$, which we prefer to minimize.
    \begin{itemize}
        \item Estimator for $\epsilon$, $\hat \epsilon_i$
        \item Minimize the squares of the $\epsilon_i$
        \item \textbf{IMPORTANT: $min_{\beta_1, \beta_2} \sum _{i} \hat \epsilon_i ^2  $}
        \begin{itemize}
            \item $\sum _{i} \hat \epsilon_i ^2 =\sum_i (Y_i -\beta_1 -\beta_2 X_i)^2$. In English, we plug in our regression guess at each iteration
        \end{itemize}
        Algorithmic approach (we arrive at a closed-form solution later)
        \begin{itemize}
            \item Start with initial guess, then assign estimators $\hat \epsilon_1$ for first observation, and so on. Square each estimator
            \item Why square it?
            \begin{itemize}
                \item MAD (mean absolute deviation) $\hat \epsilon_i$
                \item Above is bad due to reaching zero gradient (similar to vanishing gradient problem)
                \item Convex function (parabolic matching $x^2$), so if it is 0, we know it's the minimum
            \end{itemize}
        \end{itemize}
    \end{itemize}
    \item 3 possibilities (for the time being)
    \begin{itemize}
        \item $min_{\beta_1, \beta_2} \sum ^n _{i=1} (Y_i - \beta_1 -\beta_2 X_i)^2$ (``Least Squares", aka ``Ordinary Least Squares" estimators $\hat \beta_1, \hat \beta_2$ amounts to choosing the line that minimizes the sum of squared deviations. \textbf{We use this})
        \item $min_{\beta_1, \beta_2} \sum ^n _{i=1} |Y_i - \beta_1 -\beta_2 X_i|$, ``Least Absolute Deviations" aka ``mean absolute deviation" (MAD) estimators $\tilde \beta_1, \tilde \beta_2$
        \item $min_{\beta_1, \beta_2} \sum ^n _{i=1} (X_i + \frac{\beta_1}{\beta_2} - \frac{Y_i}{\beta_2})^2$, ``Reverse Least Squares" estimators $\bar \beta_1, \bar \beta_2$
    \end{itemize}
\end{itemize}

\section{Obtaining Our Linear Estimators (the betas)}

\subsection{Ordinary Least Squares - Gauss-Markov Theorem}
\begin{itemize}
    \item Follows the following assumptions
    \item $\hat \beta_1, \hat \beta_2$ are BLUE (best linear unbiased estimators, where best means most efficient) under Basic Assumptions
    \item If the errors are normal, $\hat \beta_1, \hat \beta_2$ are MVUE (minimum variance unbiased estimators) (even better than BLUE, since it considers all, not just linear-unbiased estimators)
    \item $\hat \beta_1, \hat \beta_2$ are consistent, asymptotically normal, and, under normality of the error term, asymptotically efficient
\end{itemize}

We often test $\beta_2 = 0$. Papers want a non-zero slope ($\beta_2$) because it means there may be some predicted relationship

\subsection{Derivation of OLS Estimators}
\begin{itemize}
    \item We don't need gradient descent because we have an easier way to get OLS estimators $\beta_1$ and $\beta_2$
    \item Approach
    \begin{itemize}
        \item Minimize sum of squared residuals: $min_{\beta_1, \beta_2} \sum ^n _{i=1} (Y_i - \beta_1 -\beta_2 X_i)^2$
        \begin{itemize}
            \item Comes from $\epsilon =Y_i - \beta_1 -\beta_2 X_i,$ so minimize $\sum \epsilon^2$
        \end{itemize}
        \item Take first derivative (partial derivatives with respect to both betas, remember chain rule for $\beta_2$) and set it to 0
        \item $\beta_1: \frac{\partial S}{\partial \beta_1} = \sum_{i=1} ^ n -2(Y_i -\hat \beta_1 -\hat \beta_2 X_i) = 0$
        \item $\beta_2: \frac{\partial S}{\partial \beta_2} = \sum_{i=1} ^ n -2X_i(Y_i -\hat \beta_1 -\hat \beta_2 X_i) = 0$
        \item Why the hats?
        \begin{itemize}
            \item Philosophical: We get numbers that will be our estimators. In practice, not strictly necessary.
        \end{itemize}
        \item We can condense the partial derivatives to be even simpler
        \item We call the 2 equations we have to solve for $\hat \beta_1, \hat \beta_2$ the \textbf{``normal equations"}. Rewriting and letting $\hat \epsilon_i=Y_i -\hat \beta_1 -\hat \beta_2 X_i \text{ (and diving both sides by constant -2)},$ we have
        \begin{itemize}
            \item $\frac{1}{n}\sum _{i=1} ^ n \hat \epsilon_i = 0$
            \item $\frac{1}{n} \sum_{i=1} ^ n X_i \hat \epsilon_i =0$
            \begin{itemize}
                \item $E[X_i \epsilon_i] = 0, E[\epsilon_i] = 0$. Law of large numbers, converges
            \end{itemize}
        \end{itemize}
        \item What's the intuition behind the normal equations (part of Gauss Markov-Assumptions)?
        \begin{itemize}
            \item $\hat y_i = \hat \beta_1 + \hat \beta_2 X_i$. Why no $\epsilon$? Our $\hat y_i$ is a prediction. $\hat \epsilon_i = y_i - \hat y_i$
            \item We cancel out the -2, since the other side is 0.
            \item Why $\frac{1}{n}$? Same as before, just a notational thing averaging the residuals.
            \item We call estimators of error terms $\hat \epsilon_i$ residuals
        \end{itemize}
    \end{itemize}
\end{itemize}

\subsection{Closed Form Solution to get $\hat \beta_2$ and $\hat \beta_1$}
\begin{itemize}
    \item \textbf{SUPER IMPORTANT. MOST IMPORTANT FORMULAS OF THE CLASS}
    \item Solve for $\hat \beta_2$ first, then plug back in for $\hat \beta_1$
    \item $\hat \beta_2 = \frac{\frac{1}{n} \sum(X_i - \bar X) (Y_i - \bar Y)}{\frac{1}{n} \sum (X_i - \bar X)^2} = \frac{\hat \sigma_{XY}}{\hat \sigma^2 _X} = r_{XY} \frac{\hat \sigma_Y}{\hat \sigma_X}$. $\hat \beta_2 = \frac{cov(Y,X)}{var(X)}$
    \begin{itemize}
        \item Empirical covariance over squared variance of x
        \item $r_{XY}$ is the correlation coefficient, where $r_{XY}=\hat \rho_{XY}$ and since \\$\rho_{XY}=\frac{\sigma_{XY}}{\sigma_X \sigma_Y}, \sigma_{XY}=\rho_{XY}\sigma_X \sigma_Y$
    \end{itemize}
    \item $\hat \beta_1 = \bar Y - \hat \beta_2 \bar X$. 

\end{itemize}

To show minimum (minimum squared errors $S$), take second-order partial derivative of $\sum \epsilon^2$ and show it's zero (indicates local min/max)

\section{Proving the Validity of the Linear Model: Gauss-Markov (BLUE)}

\subsection{Linearity of $\hat \beta_1, \hat \beta_2$}
\begin{itemize}
    \item Can write $\hat \beta_1=\sum_{i=1} ^n a_i Y_i$ and $\hat \beta_2 =\sum_{i=1} ^n b_iY_i$
    \begin{itemize}
        \item Where $b_i = \frac{\frac{1}{n} (X_i-\bar X)}{\frac{1}{n} \sum(X_i-\bar X)^2}$ and $a_i = \frac{1}{n} - b_i \bar X$
        \begin{itemize}
            \item $\hat \beta_2 = \frac{\sum_i (Y_i -\bar Y) (X_i -\bar X)}{\sum_i (X_i -\bar X)^2}=\text{(FOIL $\rightarrow$)} \frac{\sum_i Y_i (X_i -\bar X)-\sum_i \bar Y (X_i -\bar X)}{\sum_i (X_i -\bar X)^2}=\frac{\sum_i Y_i (X_i-\bar X)}{\sum _i (X_i - \bar X)^2} - \frac{\sum_i \bar Y (X_i-\bar X)}{\sum _i (X_i - \bar X)^2}$ \\
            $\sum(X_i -\bar X)$ is zero (the denominator is not because it is sum of squared differences), so $\frac{\sum_i Y_i (X_i-\bar X)}{\sum _i (X_i - \bar X)^2} = 0$, we can pull out $\bar Y$ from the summation in the second fraction. At this point we notice the denominator is variance (a \textbf{constant}). Factor out $Y_i$ so we can find a constant linear coefficient to prove linearity.
            \item $\sum_i \frac{1}{\sum_j (X_i - \bar X)^2} Y_i (X_i -\bar X) =\sum_i Y_i \frac{(X_i -\bar X)}{\sum_i (X_i -\bar X)^2}$. Isolate $Y_i$ from this, as $\hat \beta_2 = b_i Y_i$. Hence we prove that $\hat \beta_2$ and linear regression maintains a linear relationship with y values. \textbf{In other words, we prove that $\hat \beta_2$ is a linear estimator}
        \end{itemize}
    \end{itemize}
\end{itemize}

\subsection{Unbiasedness of OLS Estimators}
\begin{itemize}
    \item How do we prove unbiasedness? $E[\epsilon_i]=0$. 
    \item Step 1: Setup $E[\hat \beta_2 ]$. To do this, lets work backwards by plugging in the estimator $E[\hat \beta_2]=E\left[\frac{\sum_i (Y_i -\bar Y) (X_i -\bar X)}{\sum_i (X_i -\bar X)^2}\right]$ (recall covariance over variance). We need to make the epsilon show up 
    \item Step 2: Plug in the model for $Y_i, \bar Y$. Substitute $Y_i=\beta_1 + \beta_2 X_i + \epsilon_i$ and $\bar Y=\beta_1 +\beta_2 \bar X + \bar \epsilon$ into equations to add $\hat \beta_1, \hat \beta_2$ by expanding $(Y_i -\bar Y)$ \\
    $=E\left[\frac{\frac{1}{n} 
    \sum_{i=1} ^ n(\beta_1 + \beta_2 X_i + \epsilon_i - \beta_1 - \beta_2 \bar X -\bar \epsilon)(X_i -\bar X)}{\hat \sigma_X ^2}\right]$. $\beta_1$ is canceled out, can pull out $\beta_2$
    \item $E\left[\frac{\frac{1}{n} \sum_{i=1} ^ n (\beta_2 (X_i -\bar X) + \epsilon_i -\bar \epsilon)(X_i -\bar X)}{\hat \sigma_X ^2}\right]$
    \item $=E\left[\frac{\frac{1}{n} \sum_{i=1}^n \beta_2 (X_i - \bar X)^2 + (\epsilon_i -\bar \epsilon) (X_i -\bar X)}{\hat \sigma_X ^2}\right]$. Separate into two expectations in next step
    \item $=E\left[\frac{\frac{1}{n}\sum_{i=1} ^n \beta_2 (X_i -\bar X)^2}{\frac{1}{n} \sum_{i=1} ^n (X_i - \bar X)^2}\right] + E\left[ \frac{\frac{1}{n}\sum_{i=1} ^ n (X_i -\bar X) (\epsilon_i -\bar \epsilon)}{\frac{1}{n} \sum_i(X_i -\bar X)^2}\right]$
    \item $=E[\beta_2] + E\left[ \frac{\frac{1}{n}\sum_{i=1} ^ n (X_i -\bar X) (\epsilon_i -\bar \epsilon)}{\frac{1}{n} \sum_i(X_i -\bar X)^2}\right]$. We can take out $\beta_2$ out of expectation since it's a constant. \textbf{To show unbiasedness, we must show that the second expectation is 0}.
    \item If unbiased, $E[\hat \beta_2] = \beta_2$
\end{itemize}

IF we show that estimators are both linear, unbiased, and also BLUE, we KNOW that they are the BEST estimators we can have

\textbf{Gauss-Markov BLUE: Best Linear Unbiased Estimators}
\begin{itemize}
    \item Linearity
    \item Homoskedascity (constant varaiation)
    \item Unbiasedness
    \item No Serial Correlation (uncorrelated epsilon terms)
\end{itemize}
\subsection{Variance}
Context
\begin{itemize}
    \item $E[\epsilon_i] =0$. Unbiasedness is a first moment measure. We just have to show this for unbiasedness.
    \item $Y_i = \beta_1 + \beta_2 X_i +\epsilon_i$. $\hat \beta_2 =f(\epsilon_i)$.
    \item Apply this to variance: $var(\hat \beta_2) = var(\epsilon_i)$
    \item General form: $Var(Z)=E[(Z-E[Z])^2]$
\end{itemize}

Variance of $\hat \beta_2: var(\hat \beta_2) = E[(\hat \beta_2 - \beta_2 )^2] \rightarrow var(\hat \beta_2) = E[(\frac{\sum (X_i - \bar X) (\epsilon_i - \bar \epsilon)}{\sum (X_i - \bar X)^2})^2]=\frac{E([\sum(X_i - \bar X) (\epsilon_i -\bar \epsilon)]^2)}{[\sum(X_i -\bar X)^2]^2}$. $E[\hat \beta_2 ] = \beta_2$
\begin{itemize}
    \item $E[\frac{1}{(\sum_i (X_i -\bar X)^2 )^2} \cdot (\sum(X_i-\bar X)( \epsilon_i -\bar \epsilon)]$ Can pull out first part (variance, constant) from expectation
    \item $var(\hat \beta_2) = var(\hat \beta_2 - \beta_2)$, since shifting by a constant doesn't affect variance. We have constant $\beta_2$ here to start cancelling things out.
    \item Next step: plug in $\hat \beta_2$. $var(\frac{\sum_i (Y_i - \bar Y)(X_i - \bar X)}{\sum_i (X_i -\bar X)^2} - \beta_2)=var(\frac{\sum_i (\beta_1 + \beta_2 X_i + \epsilon_i -\beta_1 -\beta_2 \bar X-\bar \epsilon) (X_i -\bar X)}{\sum_i (X_i -\bar X)^2} - \beta_2)$
    \item $=var(\frac{\sum_i [\beta_2(X_i -\bar X) + (\epsilon_i -\bar \epsilon)] (X_i -\bar X)}{\sum_i (X_i -\bar X)^2} -\beta_2)\leftarrow $ (FOIL, distribute $(X_i -\bar X)$)
    \item $=Var(\frac{\beta_2 \sum_i (X_i-\bar X)^2}{\sum_i (X_i -\bar X)^2} + \frac{\sum_i (\epsilon_i -\bar \epsilon)(X_i -\bar X)}{\sum_i (X_i -\bar X)^2} - \beta_2)$. Now we get $\beta_2 -\beta_2$ which cancels, now we're left with:
    \item $var(\frac{\sum_i (\epsilon_i -\bar \epsilon)(X_i -\bar X)}{\sum_i (X_i -\bar X)^2})$. Move denominator out (constant)
    \item $\frac{1}{\sum_i (X_i -\bar X)^2} var(\sum_i (\epsilon_i -\bar \epsilon)(X_i -\bar X))$
    \item Apply Gauss Markov assumptions (unbiasedness: E[$\epsilon_i] = 0$. Homoskedascity (no serial correlation): $E[\epsilon_i ^2] = \sigma^2, E[\epsilon_i \epsilon_j] = 0$]). These assumptions allows us to pull out variance.
    \begin{itemize}
        \item If $E[\epsilon_i \epsilon_j] = 0$ then Var($\sum_i \cdots) = \sum_i var(\cdots)$
        \item $var(a\cdot \epsilon_i) = a^2 var(\epsilon_i)$
        \item $var(\sum_i \epsilon_i)=\sum_i var(\epsilon_i)$ only if $\epsilon_i$ are uncorrelated. In this class they will always be uncorrelated $=\frac{\sigma^2}{\sum_i (X_i -\bar X)^2}$
    \end{itemize}
    \item $\frac{1}{\sum_i (X_i -\bar X)^2}\sum_i var( (\epsilon_i -\bar \epsilon)(X_i -\bar X))$. We can pull out $\epsilon$ as constant from $\bar \epsilon(X_i - \bar X)$. Recall $\sum_i (X_i - \bar X)=0$. Thus remove the term and simplify
    \item $\frac{1}{\sum_i (X_i -\bar X)^2}\sum_i var( (\epsilon_i)(X_i -\bar X))$.
    \item $\frac{1}{(\sum_i (X_i -\bar X)^2)^2} \sum_i (X_i -\bar X) ^2 var(\epsilon_i)$
    \begin{itemize}
        \item $var(\epsilon_i) = E[\epsilon_i^2] - (E[\epsilon_i])^2 = \sigma^2 - 0 =\sigma^2$
    \end{itemize}
    \item Intuitively. Variance of $\beta_2$ is variance of our $\epsilon$ divided by variance of data points
\end{itemize}

\subsection{Variance Summary}
\begin{itemize}
    \item $var(\hat \beta_2) = \frac{\sigma^2}{n} \left[\frac{1}{\hat \sigma^2 _X}\right]$
    \item $var(\hat \beta_1 ) = \frac{\sigma^2}{n}\left[1+\frac{\bar X^2}{\hat \sigma^2 _X}\right]$
    $cov(\hat \beta_1 ,\hat \beta_2) = \frac{\sigma^2}{n}\left[\frac{-\bar X}{\hat \sigma^2 _X}\right]$
    \item $\sigma^2$: represents variance of $\epsilon$'s
    Empirical variance: $\hat \sigma^2_X  = \frac{1}{n} \sum_i (X_i -\bar X)^2$
    \end{itemize}

\subsection{Variance Remarks}
\begin{itemize}
    \item Larger $\sigma^2 = E(\epsilon_i ^2) \rightarrow $ Larger $var(\hat \beta_1), var(\hat \beta_2)$
    \item Larger $\hat \sigma^2 _X \rightarrow $ Smaller $var(\hat \beta_1), var(\hat \beta_2)$
    \item $var(\hat \beta_1) \geq var(\bar \epsilon)$, equal iff$cov(\hat \beta_1, \hat \beta_2) =0$
    \item Larger $n\rightarrow $ Smaller $var(\bar \beta_1), var(\hat \beta_2)$ (consistency, Law of Large Numbers)
\end{itemize}

$\hat \sigma_X ^2$: empirical variance. $=\frac{\sigma^2}{\sum_I (X_i -\bar X)^2}$

\subsection{Covariance between $\hat \beta_1$ and $\hat \beta_2$}
$cov(z,w) = E[(z-E[z])(w-E[w]))$

\begin{itemize}
    \item Because betas are unbiased: $cov(\hat \beta_1,\hat \beta_2)=E[(\hat \beta_1 -\beta_1)(\hat \beta_2 -\beta_2)$. Remember: the betas being unbiased is a necessary condition for this
    \item The covariance should be correlated with $\epsilon$ since it affects both
\end{itemize}

How to (intuitively) know whether no heteroskedascity is a good assumption
\begin{itemize}
    \item Check dispersion of $\epsilon$ around our line. If the dispersion changes, then we have an issue
\end{itemize}

\subsection{Estimation of $\sigma^2$}
$E[\epsilon_i] ^2 = \sigma^2$. We hope that this good expectation is a good representation of the average: $\frac{1}{n} \sum_i \hat \epsilon_i ^2$. $\hat e_i \rightarrow \hat \epsilon_i$
\begin{itemize}
    \item When $\sigma^2$ is unknown
    \item $s^2 \equiv \hat \sigma^2 = \frac{1}{n-2}\sum \hat e_i ^2 = \frac{1}{n-2} \sum(Y_i -\hat \beta_1 -\hat \beta_2 X_i)^2$ (n-2 degrees of freedom, because $\hat \beta_1, \hat \beta_2$)
    \item \textbf{It can be shown that ????????}
\end{itemize}

\subsection{Gauss-Markov Theorem}
When we have betas with hats, we assume they are outputs for OLS regression.
$(\hat \beta_1, \hat \beta_2)$ are BLUE (Best Linear Unbiased Estimators) of ($\beta_1, \beta_2$)
\begin{itemize}
    \item That is, for any $k_1$ and $k_2 \in \R$, if $\tilde \beta_1$ and $\tilde \beta_2$ are any linear unbiased estimators of $(\beta_1,\beta_2)$
    \begin{itemize}
        \item $var(k_1\hat \beta_1 + k_2 \hat \beta_2) \leq var(k_1 \tilde \beta_1 + k_2 \tilde \beta_2)$ (minimize variance)
    \end{itemize}
\end{itemize}

\subsection{Gauss-Markov Assumptions}
{Assumptions from Unbiasedness}
\begin{itemize}
    \item $E[\epsilon_i] = 0$
    \item $E[\hat \beta_1] = \beta_1$
    \item $E[\hat \beta_2] = \beta_2$
\end{itemize}
{No serial correlation/homoskedascity}
\begin{itemize}
    \item $E[\epsilon_i\epsilon_j] = 0 \leftarrow $ no serial correlation
    \item $E[\epsilon_i^2] = \sigma^2 \leftarrow $ homoskedascity (constant variance)
\end{itemize}
The Gauss-Markov Assumptions are weak
\begin{itemize}
    \item Gauss-Markov only cares about distributions where mean = 0 and any variance (not type of variation). If these conditions apply we can throw Gauss-Markov assumptions at it.
\end{itemize}

\subsection{How good is the fit? (Introducing $R^2$)}
One measure: $RSS\equiv \sum(Y_i-\hat Y_i)^2 = \sum(Y_i -\hat \beta_1 -\hat \beta_2X_i)^2 = \sum \hat e_i^2$
\begin{itemize}
    \item RSS is the ``Residual Sum of Squares"
    \item Problem with RSS: It depends on units - if $(Y_i, X_i)$ all multiplied by some (scalar) $K$, RSS multiplied by $K^2$. It quadratically blows up values, needs to be normalized. TSS can accomplish this. (``invariant to scale")
\end{itemize}
Difference between RSS and TSS: $\hat Y$ vs $\bar Y$.
\begin{itemize}
    \item The RSS is what you \textbf{CANNOT} explain with your model. It contains the residuals
    \begin{itemize}
        \item $ESS$ (``Explained Sum of Squares") = $TSS-RSS$
    \end{itemize}
    \item TSS does not care about X values (intuitively think that all Y points are projected onto Y-axis and differences are summers and squared)
\end{itemize}

Better measure: normalize measure
\begin{itemize}
\item Let $TSS=\sum (Y_i -\bar Y)^2 \equiv n\hat \sigma^2 _Y,$ where TSS is the ``Total Sum of Squares", then:
    \item $R^2\equiv 1-\frac{RSS}{TSS}$ measures ``goodness of fit"
    \item $R^2$ is intuitively a value that tells you how good your linear assumption is
    \begin{itemize}
        \item From a minimization perspective, $\hat \beta = argmin_\beta, $ where we minimize $R^2$, the ``Residual Sum of Squares"
    \end{itemize}
\end{itemize}

\subsection{Variance Decomposition: (Why $R^2$ is reasonable)}
RECALL: Normal Equations (Derivation of OLS Estimators)
\begin{itemize}
    \item $\frac{1}{n} \sum_{i=1} ^n \hat \epsilon_i = 0$. By construction, the sum of residuals after we do OLS is 0
    \begin{itemize}
        \item The sum of squares of residuals are minimized by OLS. If we get the residuals (NOT SQUARED) and sum them, the total will be 0
    \end{itemize}
    \item $\frac{1}{n}\sum_i X_i \hat \epsilon_i = 0$
\end{itemize}

\begin{itemize}
    \item $TSS=\sum_i (Y_i -\bar Y) = \text{add and subtract $\hat Y_i$}\sum(Y_i-\hat Y_i + \hat Y_i -\hat Y)^2 \text{ \textbf{(FOIL)} }= $\\
    $=\sum(Y_i - \hat Y_i)^2 + \sum(\hat Y_i -\bar Y)^2 + 2\sum(Y_i -\hat Y_i) (\hat Y_i -\bar Y)$ (\textbf{LAST TERM: } $2\sum(Y_i -\hat Y_i) (\hat Y_i -\bar Y)$ is gone because of 2nd normal equation)
    \begin{itemize}
        \item Decompose this to $TSS =\sum(Y_i - \hat Y_i)^2 + \sum(\hat Y_i -\bar Y)^2$, which represents $TSS = RSS + ESS$
    \end{itemize}
    \item But $\sum(Y_i -\hat Y_i) (\hat Y_i)-\bar Y)=\sum \hat e_i (\hat Y_i -\hat Y)=$ (\textbf{Plug in} $\hat Y_i = \hat \beta_1 + \hat \beta_2 X_i$ \textbf{and} $\bar Y = \hat \beta_1 + \hat \beta_2 \bar X$)\\
    $=\sum \hat e_i (\hat \beta_1 + \hat \beta_2 X_i - \bar Y)=$\\
    $=\sum \hat e_i[\hat \beta_2 (X_i -\bar X)]$ since $\hat \beta_1 =\bar Y-\hat \beta_2 \bar X$\\
    $=\hat \beta_2 \{\sum\hat e_i X_i -\bar X\sum\hat e_i\}=$\\
    $=0$ by the normal equations
\end{itemize}

Thus $TSS = \sum - \sum(Y_i -\hat Y_i)^2 + \sum(\hat Y_i -\bar Y)^2 = RSS+ESS$\\
where ESS is the ``Explained Sum of Squares", as explained by the regression, while the RSS, or ``Residual Sum of Squares" is the variation in Y that is not explained by the regression. 

[Total Sum of Squares = Residual Sum of Squares + Explained Sum of Squares]

Thus
$$R^2 = 1 -\frac{RSS}{TSS} = \frac{TSS-RSS}{TSS}=\frac{ESS}{TSS}$$

Important formula (considering the variables that form the `relationship' in this equation):
$$R^2 = \frac{ESS}{TSS}=\frac{\frac{1}{n} ESS}{\hat \sigma^2 _Y} =\hat \beta_2 ^2 (\frac{\hat \sigma^2 _X}{\hat \sigma ^2 _X})$$

Why define $R^2$ as $\frac{ESS}{TSS}=1-\frac{RSS}{TSS}$, and not just as $R^2 = r^2_{XY}?$
\begin{itemize}
    \item It would break in multiple regression
\end{itemize}

\begin{itemize}
    \item $E[\epsilon_i] = 0, E[\epsilon_i \epsilon_j ] = 0, E[\epsilon_i ^2 ] = \sigma^2$
\end{itemize}

\section{The Linear Model with Gaussian Errors}
Before:
\begin{itemize}
    \item We claimed we could fit a Normal Linear Model: $Y_i = \beta_1 + \beta_2 X_i +\epsilon_i \leftarrow$ $min_{\beta_1,\beta_2} \sum_i (Y_i -\beta_1 -\beta_2 X_i)^d$, $E[\epsilon_i] = 0$
    \begin{itemize}
        \item As long as $\epsilon_i \overset{\text{iid}}{\sim} N(0,\sigma^2)$ (iid meant uncorrelated, error can be fit into a normal distribution)
    \end{itemize}
    \item With this assumption, the OLS estimators $\hat \beta_1,\hat \beta_2$ are the \textbf{minimum variance unbiased estimators} (MVUE). That is, they have minimum variance among the class of all unbiased estimators, whether linear or not. his is stronger than being BLUE, which is restricted to linear estimators
    \item The OLS estimator is the best linear, unbiased estimator (BLUE)
    \item If we prove that our data is unbiased, without serial correlation, with error fitting into Gaussian Distribution, we have MVUE?

\end{itemize}

\section{Distribution of $\hat \beta_1, \hat \beta_2$}
\begin{itemize}
    \item If $\epsilon_i \overset{iid}{\sim} N (0, \sigma^2)$
    \item $\hat \beta_ 2 = f(x_i, y_i) \leftrightarrow Y_i = \beta_1 + \beta_2X_i + \epsilon_i$
    \item How does the distribution of $\epsilon_i$ affect the distribution of $\hat \beta_2$?
    \begin{itemize}
        \item It is a normal distribution (`sums of normals are normals': linear functions of gaussian are gaussian). This also transfers to $Y_i$
        \item We now know both $\hat \beta_1,\hat \beta_2$ are normally distributed
    \end{itemize}
    \item Unbiasedness: hope ($E[\hat \beta_2 ] = \beta_2$)
    \item $\hat \beta_2 \sim N(0, \frac{\sigma}{}$We expect to get the correct $sl^2{n})$
    \item $\hat \beta_1 \sim N(\beta_1, \frac{\sigma^2}{n}(1 + \frac{X^2}{\hat \sigma ^2 _X}))$
    \item We want $\hat \beta_2 \sim N(\beta_2, Var(\hat \beta_2))$. Problem: seems circular, we need the estimator itself.$\hat \beta_2 \rightarrow f(\hat \beta_2, X_i, Y_i ) \sim N(0,1)$
    \item $\hat \beta_2 \sim N(\beta_2, var(\hat \beta_2)) = N(\beta_2, \frac{\sigma^2}{n} (\frac{1}{\hat \sigma^2 _X}))$
    \item $\frac{\hat \beta_2 - \beta_2}{\sqrt{\frac{\sigma^2}{n} (\frac{1}{\hat \sigma^2_X})}}$. The denominator here is called ``standard error''
    \begin{itemize}
        \item This is achieved by normalizing $N(\beta_2, Var(\hat \beta_2)) \rightarrow N(0,1)$
        \item Subtract $\beta_2$ to normalize the mean, divide by square root of variance of $\hat \beta_2$. 
        \item The square root of the variance of the OLS estimator (the denominator) is called the standard error: $\hat \beta_2$
        \item Is the $\sigma^2$ a problem? No, we can estimate it (replace it with $s^2$, see `Estimation of $\sigma^2$'
        \item We have an estimator for $\beta_2$, so we can test for specific values to see if $\hat \beta_2 \neq \beta_2$. For example, we can plug in $\beta_2 = 0$. If we see the resulting data not fitting ($\sim N(0,1)$), then we can reject the premise that $\hat \beta_2 = \beta_2$
        \item When we sub in $s^2$ for $\sigma^2$, we affect the distribution since $s^2$ since $s^2$ has its own distribution.
        \begin{itemize}
            \item If we know $\sigma^2$ then we are fine
            \item If we don't, we need to estimate it via $s^2$
            \item $s^2$ is \textbf{NOT} a Gaussian Distribution, because of the squared term. It is a \textbf{Chi-Square Distribution} (with n-2 degrees of freedom)
            \item What is a Chi-Square? (number 3 of `Useful Results')
            \begin{itemize}
                \item Gaussian distribution can have negative values. Chi-Square only supports non-negative numbers (due to it being squared)
            \end{itemize}
            \item Using Number 4 of `Useful Results', we find that we have standard error fitting the t-distribution?
        \end{itemize}
    \end{itemize}
\end{itemize}

\subsection{Estimation of $\sigma^2$}
\begin{itemize}
    \item As before, $s^2 \equiv \frac{1}{n-2} \sum (Y_i -\hat \beta_1 -\hat \beta_2 X_i)^2$
    \begin{itemize}
        \item We assume that via Law of Large Numbers, we get $E[\epsilon_i^2] = \sigma^2$
    \end{itemize}
\end{itemize}

\subsection{Standard Error of $\hat \beta_1, \hat \beta_2$}
\begin{itemize}
    \item The Standard Error is the square root of the estimated variance
    \item $SE(\hat \beta_1) = \sqrt{\frac{s^2}{n} (1+ \frac{\bar X}{\hat \sigma^2_X})}$
    \item \textbf{IMPORTANT:} If I know my $\sigma^2$, we have a standard normal distribution. If I don't know my $\sigma^2$ (have to estimate it), it is a t-distribution
    \item t-ratio: $t\equiv \frac{\hat \beta_2 - \beta_2}{SE(\hat \beta_2)}=\frac{\hat \beta_2 - \beta_2}{\sqrt{\frac{\sigma^2}{n} (\frac{1}{\hat \sigma ^2_X})}} / \frac{s^2}{\sigma^2} = \frac{Z}{\sqrt{W/(n-2)}}$\\
    where $Z\sim N(0,1),W\sim \chi_{n-2}^2$ (related to Useful Results 3), with Z, W independent
    \item So $\frac{\hat \beta_2 - \beta_2}{SE(\hat \beta_2)} \sim t_{n-2}$ (See Useful Results 4). Similarly for $\hat \beta_1$. n-2 because of 2 degrees of freedom (remember two beta estimators)
\end{itemize}


Setup: do linear prediction, put a line through, have good properties of the line (gauss markov is all i need)


\section{Non-normal Errors}
\begin{itemize}
    \item $\hat \beta_2 = \frac{\sum_i (Y_i - \bar Y)(X_i - \bar X)}{\sum _i (X_i -\bar X)^2}$
    \item $\hat \beta_2 = \beta_2 + \frac{\sum_i (X_i - \bar X) \epsilon_i}{ \sum_i (X_i - \bar X)^2}$
    \begin{itemize}
        \item Multiply everything by $\frac{1}{\sqrt n}$
        \item As $n\rightarrow \infty$,the fraction becomes zero. The Numerator converges to a normal distribution $N(0, Var(\epsilon_i))$ because $\epsilon_i$ is normally distributed
        \item If we scale both sides by $\frac{1}{\sqrt n}$, we allude to Central Limit Theorem, so with no unbiasedness, no homoskedascity, no serial correlation (Gauss-Markov assumptions), we have a normal distribution, proving the central limit theorem for $\hat \beta_2$
    \end{itemize}
    \item Even if errors are non-normal, as we have infinitely many points, then we approach a normal distribution for our beta estimators
    \item By a generalization of the Central Limit Theorem, as long as \{$\epsilon_i$\} are iid\\
    $\hat \beta_1 \overset{a}{\sim} N(\beta_1, \frac{\sigma^2}{n}(1+ \frac{\bar X^2}{\hat \sigma^2 _X})$ and $\hat \beta_2 \overset{a}{\sim} N(\beta_2, \frac{\sigma^2}{n} (\frac{1}{\hat \sigma^2 _X}))$
    \item where ``a" refers to asymptotically or in the limit as $n\rightarrow \infty$
    \item Also, since $\underset{n\rightarrow \infty}{lim \text{ }s^2} = \sigma^2$
    \item $\frac{\hat \beta_1 -\beta_1}{SE(\hat \beta_1)} \overset{a}{\sim}$
\end{itemize}

OLS: minimize sum of squares via gauss markov assumptions, testing by assuming gaussian distribution, have a `pivot' so we can estimate beta


\section{Hypothesis Testing}
\begin{itemize}
    \item Hypothesis: An assumption about distribution of ``population"
    \item Objective of hypothesis testing: Given a random sample from a population, is there enough evidence to contradict some assertion about that population? (Karl Popper: science can only be done by proving yourself wrong)
    \item Null hypothesis ($H_0$): The statement to be tested; often the hypothesis of ``no effect" or ``no relationship"
    \item Alternative hypothesis ($H_A$): some other possibilities than the null hypothesis, e.g. ``some effect" or ``some positive effect''
    \item We have one-sided and two-sided hypothesis tests, which influence our null and alternative hypotheses
    \item Where do $H_0, H_A$ come from?
    \begin{itemize}
        \item Prior research, theoretical expectations, etc.
    \end{itemize}
    \item Possible States of Nature: ``$H_0$" is either true or false
\end{itemize}
Types of Error
\vspace{0.1in}

\begin{tabular}{ | c | c | c| }
\hline
Decision/Nature & $H_0$ True & $H_0$ false\\
\hline
Accept $H_0$ & Correct decision & Type II error\\
\hline
Reject $H_0$ & Type I error & Correct decision\\
\hline
\end{tabular}
\begin{itemize}
    \item False positive: Type I error
    \begin{itemize}
        \item We consider this to be more severe.
    \end{itemize}
    \item False negative: Type II error
\end{itemize}
We can think of the p-value as the percentage of times we may commit a Type I error
\begin{itemize}
    \item Example: If we have alpha value $\alpha = 0.05$, this means the probability of making a Type I error is 5\%)
\end{itemize}
Other terms
\begin{itemize}
    \item Significance level: $\alpha = P(\text{Type I }| H_0\text{ true})$
    \item Confidence Level: $1-\alpha = P(\text{Accept }H_0 | H_0\text{ true})$
    \item ``Operating characteristic": $\beta = P(\text{Type II } | H_0 \text{false})$
\end{itemize}

\subsection{Hypothesis Testing: The test of significance approach}
Assumption (for now): $\epsilon_i \overset{iid}{\sim} N(0, \sigma^2), \sigma^2 unknown$
Contains Gauss-Markov assumptions
\begin{itemize}
    \item Unbiased: $E[\epsilon_i] = 0$
    \item No serial correlation: $E[\epsilon_i\epsilon_j] = 0$
    \item $E[\epsilon_i ^2] = \sigma^2$
    \item $X_i$ deterministic
    \begin{itemize}
        \item $E[\epsilon_i X_i ] = 0$
        \item $X_i E[\epsilon_i] = 0$
    \end{itemize}
\end{itemize}
Our next job is to turn statistics into proving the assumptions\\
Recall:
\begin{itemize}
    \item $\hat \beta_2 \sim N(\beta_2, Var(\hat \beta_2))$
    \item $\hat \beta_2 - \beta_2 \sim N(0, Var(\hat \beta_2))$
    \item $\frac{\hat \beta_2 - \beta_2}{\sqrt{Var(\hat \beta_2}} \sim N(0,1)$. We call the denominator the standard error of $\hat \beta_2$ ($SE[\hat \beta_2]$)
\end{itemize}
If our Gauss-Markov assumptions are correct: $\frac{\hat \beta_2 - \beta_2}{SE(\hat \beta_2) \sim t_{n-2}}$ and $\frac{\hat \beta - \beta_1}{SE(\hat \beta_1)} \sim t_{n-2}$
\begin{itemize}
    \item If the value of $\beta_2 (\beta_1)$ is specified under the null hypothesis, the t-value can be obtained from the sample, and can serve as a test statistic (a random variable that allows you to test a hypothesis). Since this test statistic follows the t-distribution, we can make confidence interval statements as follows:
    \begin{itemize}
        \item $P\left(-t_{dof, \frac{\alpha}{2}} \leq \frac{\hat \beta_2 - \beta^*_2}{SE(\hat \beta_2)}\leq t_{dof, \frac{\sigma}{2}}\right) = 1- \alpha$ (2-tailed)
        \begin{itemize}
            \item Where $\beta_2^*$ is a value of $\beta_2$ under $H_0$ and $-t_{dof, \frac{\sigma}{2}}, t_{dof, \frac{\sigma}{2}}$ are ``critical" t-values from t-table for $\alpha$ level of significance and $n-2$ degrees of freedom
        \end{itemize}
        \item Rearrange
        \item $P\left(\beta^*_2 - t_{dof, \frac{\sigma}{2}} SE(\hat \beta_2 ) \leq \hat \beta_2 \leq \beta_2 ^* + t_{dof, \frac{\sigma}{2}} SE(\hat \beta_2)\right) = 1-\alpha$
    \end{itemize}
\end{itemize}
\textbf{IMPORTANT:} $t = \frac{\hat \beta_2 - \beta^* _2}{\sqrt{Var(\hat \beta)}}$
\begin{itemize}
    \item If $\hat \beta_2 - \beta_2^* \neq 0$, we probably have the wrong $\beta$
    \item \textbf{In other words:} The further $\hat \beta_2$ is away from our initial guess $\beta_2 ^*$, and the smaller $SE(\hat \beta_2)$, the larger $|t|$ (focus on absolute value, since t can be positive or negative). How large is large enough to reject some $H_0$ depends on the choice of level of significant and the dof
    \item \textbf{The larger the difference, the greater the t statistic is}
    \begin{itemize}
        \item \textbf{Important intuition:} $t$-statistic is ratio of distance between predicted beta and actual beta compared to normalized variance of beta parameter
        \begin{itemize}
            \item $|t| = \frac{\hat \beta_2 -\beta_2 ^*}{\sqrt{Var(\hat \beta_2)}}$
        \end{itemize}
    \end{itemize}
    \item Summary of decision rules\\\\
    \begin{tabular}{|c|c|c|c|}
        \hline
        Type of hypothesis & $H_0$ & $H_A$ & Decision: Reject $H_0$ if \\
        \hline
        two-sided & $\theta = \theta^*$ & $\theta \neq \theta^*$ & $|t| > t_{dof, \frac{\alpha}{2}}$\\
        \hline
        one-sided (right tail) & $\theta = \theta^*$ & $\theta > \theta^*$ & $t > t_{dof, \alpha}$\\
        \hline
        one-sided (left tail)& $\theta = \theta^*$ & $\theta < \theta^*$ & $t < -t_{dof, \alpha}$\\
        \hline
    \end{tabular}
    \item Alternatives
    \begin{itemize}
        \item $\epsilon_i \overset{iid}{\sim} N(0, \sigma^2), \sigma^2$ known $\Rightarrow$ use $N(0,1)$
        \item Asymptotically $(n\rightarrow \infty) \Rightarrow N(0,1)$
    \end{itemize}
\end{itemize}
A linear regression implicitly does an ANOVA (analysis of variance), basically a t-test on a coefficient

\subsection{Relationship or $R^2 = r^2 _{XY}$ to t-test}
\begin{itemize}
    \item Sum of squares that entire the $R^2$ and their dof in an \textbf{analysis of variance (ANOVA)} approach:
    \item Remember that $R^2 = \frac{ESS}{TSS} = 1- \frac{RSS}{TSS}$
    \item $RSS = \sum (Y_i - \hat Y_i)^2$ has n-2 dof
    \item $ESS = \sum (\hat Y_i - \bar Y)^2=\hat \beta_2 ^2 \sum (X_i - \bar X)^2$ has 1 dof (since X's are fixed, only $\hat \beta_2$ varies as sample changes)
    \begin{itemize}
        \item Not going over derivation in class (5-10 min?)
    \end{itemize}
    \item Linear regression model and \textbf{ONLY UNDER THE TEST $\beta_2$ AND $H_A : \beta\neq 0$} (two-tailed), can we use the following test:
    \item To test $H_0 : \beta_2 = 0$ vs $H_A:\beta_2 \neq 0$, would reject if
    \begin{itemize}
        \item $\left| \frac{\hat \beta_2}{SE(\hat \beta_2-0)}\right | > t_{dof,\frac{\alpha}{2}}$
        \begin{itemize}
            \item Can also reject if $(n-2)\frac{R^2}{1-R^2} > F_{1,n-2,\alpha}$ since \\ $(n-2) \frac{R^2}{1-R^2} = (n-2) \frac{ESS/TSS}{RSS/TSS} = \frac{ESS}{\frac{1}{n-2} RSS} = \frac{\hat \beta_2 ^2 \sum(X_i - \bar X)^2}{\frac{1}{n-2} \sum (Y_i - \hat Y_i )^2} = \frac{\hat \beta_2 ^2 \sum (X_i - \bar X)^2}{\frac{1}{n-2} \sum (Y_i -\hat \beta_1 -\hat \beta_2 X_i )^2} = \frac{\hat \beta_2 ^2}{\frac{s^2}{n} \frac{1}{\hat \sigma_X ^2}} = \left(\frac{\hat \beta_2}{SE(\hat \beta_2)}\right)^2$
            \item A F-test is a squared t-distribution
            \item and (\textbf{IMPORTANT}) $F_{1,n-2,\alpha} = (t_{n-2, \frac{\alpha}{2}})^2$
            \item That is, $\frac{(n-2)}{1}\frac{R^2}{1-R^2} \sim F_{1,n-2}$ under $H_0$
            \item This is valid \textbf{only} under this specific $H_0$ and $H_1$ in a regression with intercept. Why do this in addition to t-test? Useful when have multiple regressors
            \item With the \textbf{F-test, we can only test two-sided alternatives}, because when we square the t-statistic, we CANNOT tell whether the original t-statistic was negative or positive
        \end{itemize}
    \end{itemize}
\end{itemize}

\subsection{General F-test approach}
\begin{itemize}
    \item Joint hypothesis: $H_0 : \beta_1 = \beta_1 ^*$ and $\beta_2 = \beta_2 ^*$ vs.\\ $H_1: \beta_1 \neq \beta_1 ^*$ and/or $\beta_2 \neq \beta_2^*$
    \item \textbf{Important: F-test alternative always ``two-sided"}\\
    \begin{tabular}{|c|c|c|}
        \hline
         & one-sided allowed & only two-sided \\
         \hline
         only single variable ($\beta_1, \beta_2$) & t-test & t-test/F-test \\
         \hline
         multiple variables with multiple hypotheses & ?? & F-test\\
         \hline
    \end{tabular}
    \begin{itemize}
        \item t-tests are marginal distributions $\hat \beta_1, \hat \beta_2$
        \item F-test uses joint distributions (of normal distributions)
        \item Marginal distributions are Gaussian here. t-tests use marginal distributions
    \end{itemize}
    \item Computation of F-statistic:
    \begin{itemize}
        \item Compute $RSS_U$ (unrestricted) by estimating $\hat \beta_1, \hat \beta_2$ under alternative hypothesis (ie, unrestricted) $\Rightarrow RSS_U = \sum (Y_i -\hat \beta_1 -\hat \beta_2 X_i )^2$
        \begin{itemize}
            \item The restriction comes from $H_0$ (we pretend the null hypothesis is true when testing against it). Our $RSS$ is unrestricted because we have conducted our tests and rejected the null hypothesis
        \end{itemize}
        \item Compute $RSS_R$ by imposing null hypothesis restrictions and estimating any unrestricted parameters
        \begin{itemize}
            \item RSS is restricted: Solving OLS with an additional restriction (the null hypothesis)
            \item \textbf{Example:} $H_0:\beta_2 = 0$ vs $H_A:\beta_2 \neq 0$
            \item $RSS_U\leftarrow$ just run OLS on the mode (without any hypothesis) $$Y_i = \beta_1 + \beta_2 X_i + \epsilon_i$$
            $$\hat \beta_2 = \frac{\sum_i (Y_i -\bar Y)(X_i - \bar X)}{\sum_i (X_i - \bar X)^2}$$
            \item $RSS_r$, (assume null hypothesis that $\beta_2=0$ is true, so assume $\beta_2 = 0$) run a regression where $\beta_2$ doesn't show up. $$Y_i =\beta_1 + \epsilon_i$$
            \item Comparing errors: $RSS_r \geq RSS_u$
            \item Why?
            \begin{itemize}
                \item It's a minimization problem, restrictions impede that and at best have no impact
            \end{itemize}
        \end{itemize}
        \item Compute $F= \frac{(RSS_R - RSS_U) / r}{RSS_U / (n-k)}$
    \end{itemize}
\end{itemize}

\subsection{Functional Forms of Regression}
Before, we focused on $Y_i = \beta_1 + \eta_2 X_i + \epsilon_i$. Now let's focus on some alternative linear models
\begin{itemize}
    \item We can use OLS on non-parametric predictions (i.e. machine learning). We can put non-linear terms in our linear approximation
    \item Linearity in parameters $\beta_1, \beta_2$
    \item Important log properties (assuming $A$ and $B$ are positive and $k$ is some constant)
    \begin{itemize}
        \item $ln(AB) = lnA + ln B$
        \item $ln(\frac{A}{B}) = lnA - ln B$
        \item $ln(A^k) = klnA$
        \item Derivative: $\frac{d(lnX)}{ dX} = \frac{1}{X}$ or $d(ln X) = \frac{dX}{X}$
        \item Note following terms:
        \begin{itemize}
            \item Absolute change: $X_t - X{t-1}$
            \item Relative or proportional change: $\frac{(X_t - X_{t-1})}{X_{t-1}}$
            \item Percentage change or percent growth rate: $\frac{(X_t - X_{t-1})}{X_{t-1}} * 100$
            \item \textbf{Change in log variable gives a percentage change}: $ln X_t - lnX_{t-1} \approx \frac{X_t - X_{t-1}}{X_{t-1}}$
        \end{itemize}
    \end{itemize}
\end{itemize}
\subsection{``How to measure elasticity": The log-log model}
Intuitively, think of logs as `elasticities' due to percentage change
\begin{itemize}
    \item Consider the following (non-linear regression model):
    \begin{itemize}
        \item $Y_i = \beta_1 X ^{\beta_2} exp(\epsilon_i)$
        \item Taking a (natural) log on both sides: $ln Y_i = \beta_1 X^{\beta_2} exp(\epsilon_i)$
        \item Now, this model (via a new linear relationship) may alternatively be expressed as 
        \begin{itemize}
            \item $ln Y_i = ln\beta_1 + \beta_2 lnX_i + \epsilon_i$
            \begin{itemize}
                \item ``Identification": How we do we identify what we really want ($\beta_1$) by getting something else?
            \end{itemize}
            \item Then let \\ $Y_i ^* = \alpha + \beta_2 X_i ^* + \epsilon_i$ where $Y_i ^* = lnY_i , X_i ^* = lnX_i,$ and $\alpha = ln \beta_1$
            \item Assuming that the Basic Assumptions i) - vii) are satisfied for this model, the OLS estimators $\hat \alpha, \hat \beta_2$ will be BLUE of $\alpha, \beta_2$ (note that $\hat \beta_1 = exp(\hat \alpha$)) will be a biased estimator of $\beta_1$)
            \item \textbf{Important:} $\beta_2$ measures the \textbf{elasticity} of Y with respect to X (the percentage change in Y for a given (small) percentage change in X) (since $\beta_2 = \frac{d(lnY)}{d(lnX)} = \frac{dY/Y}{dX/X}$)
        \end{itemize}
        \item Implication of linear mode: $Y_i = \beta_1 + \beta_2 X_i + \epsilon_i$
        \begin{itemize}
            \item Take derivative $\frac{dY}{dX} = \beta_2$. The slope is constant $\forall X \in \R$. We love linear regression so much because linearity means no change in $\beta_2$
            \item Let $\overset{\sim}{ \beta_1} = ln(\beta_1)$
            \item Identification: To define our $\beta_1$, we need to take an exponential: $exp(\overset{\sim}{\beta_1}) = \beta_1 = 1$
        \end{itemize}
    \end{itemize}
    \item Constant elasticity model: The model assumes that the elasticity coefficient between $Y$ and $X, \beta_2$, remains constant (linearity) throughout
    \begin{itemize}
        \item $Y_i = \beta_1 X ^{-\beta_2}$
    \end{itemize}
    \item The double-log transformation: $ln Y_i = ln\beta_1 - \beta_2 ln X_i$
    \item Expressing in units (remember log means percentage change)
    \begin{itemize}
        \item $\widehat{Y_t} = 0.7774 - 0.2530 ln(\widehat{X_t})$
        \item $0.2530\%$ percentage change in $Y_t$ with a $1\%$ increase in $X_t$
    \end{itemize}
    \item Recall: Think of $R^2$ as a prediction parameter: Taking log-log model can improve $R^2$ due to shape of the data
    \item Our $\epsilon_i$ is set up so that our $\epsilon_i \sim N(0, \sigma)$ (we get a Gaussian distribution in order to satisfy our Gauss-Markov theorem)
\end{itemize}

\subsection{How to measure the growth rate: The log-lin (Log Linear) model}
\begin{itemize}
    \item Using the compound interest formula, we can write the following model: \[Y_t = Y_0 (1+r)^t exp(\epsilon_t)\] where, for instance, $Y_t = $ real GDP at time $t, Y_0 = $ initial value of real GDP, and where r is the compound (i.e. over time) rate of growth of Y.
    \item Taking the natural log, we can rewrite the model in the following way: \[ln(Y_t) = ln(Y_0) + tln(1+r) + \epsilon_t\]
    \begin{itemize}
        \item $Y^*=ln(Y_t)$
        \item $\beta_1 = ln(Y_0)$
        \item $\beta_2 = ln(1+r)$
        \item $X = t$
    \end{itemize}
    \item Identification: identify what we want (r) by working towards getting other parameter ($\beta_2$)
    \item Once again, assuming Basic Assumptions i) - vii) are satisfied for this model, the OLS estimators $\hat \beta_1, \hat \beta_2$ will be BLUE of $\beta_1, \beta_2$
    \item $\beta_2$, the slope coefficient, measures the constant proportional or relative change in Y for a given absolute change in the value of the regressor (here, time) (since $\beta_2 = \frac{d(lnY)}{dt} = \frac{dY/Y}{dt})$
    \item Note that it is NOT possible to get an unbiased estimator of r (nonlinear)
\end{itemize}

\section{Note on the nature of the error term}
\begin{itemize}
    \item Consider a regression model without the error term: 
    \begin{itemize}
        \item $Y_i = \beta_1 X_i ^{\beta_2}$
        \item For estimation purposes, we could express this model in at least 3 different ways:
        \begin{itemize}
            \item $Y_i = \beta_1 X_i ^{\beta_2} \epsilon_i$
            \item $Y_i = \beta_1 X_i ^{\beta_2} exp(\epsilon_i)$
            \item $Y_i =\beta_1 X_i ^{\beta_2}+ \epsilon_i$
        \end{itemize}
    \end{itemize}
    \item NOTE: $ln\epsilon \not \sim N(0, \sigma^2)$
    \item $ln Y_i = ln\beta_1 + \beta_2 lnX_i + \epsilon_i \leftarrow \epsilon_i \sim N(0, \sigma^2)$
\end{itemize}

ESS: explained variation in Y
TSS: sum of explained and residual

Multiple hypotheses:
\begin{itemize}
    \item $H_0: \beta_2, \beta_3, \beta_4, \cdots = 0$
    \item $H_A:$ at least one $\neq 0$
\end{itemize}

\section{Matrix Algebra}
Recall
\begin{itemize}
    \item Matrix addition is commutative and associative
    \item Scalar multiplication
    \begin{itemize}
        \item Distributive property
    \end{itemize}
\end{itemize}

Consider model: $Y_i = \beta_1 + \beta_2 X_{i2} + \beta_3 X_{i3} + \cdots + \beta_k X_{ik} + \epsilon_i \leftarrow \hat \beta = (X^T X) ^{-1} X^T y$
\begin{itemize}
    \item $X^T X:$ Var(X)
    \item $X^T y: $ cov(X,Y)
\end{itemize}
$\beta_1$

IMPORTANT: $\hat \beta = (X^TX) ^{-1} X^T y$
% \begin{itemize}
%     \item $X^T X$ is SPD (I think)
% \end{itemize}
Other properties
\begin{itemize}
    \item Matrix multiplication is not commutative
    \item Matrix multiplication is associative
    \item Matrix multiplication is associative
    \item Note: for transpose, we will write $A^\prime$ (prime is used instead of T for econometrics)
    \begin{itemize}
        \item $(A^\prime)^\prime = A$
        \item $(A+B)^\prime = A^\prime + B^\prime$
        \item $(AB)^\prime = B^\prime A^\prime$
    \end{itemize}
    \item Inverse
    \begin{itemize}
        \item $(A^{-1})^\prime = (A\prime)^{-1}$
        \item det$(A^{-1}) = \frac{1}{det(A)}$
    \end{itemize}
    \item Idempotency
    \begin{itemize}
        \item A is said to be idempotent iff $A^2 = A$.
        \begin{itemize}
            \item Only square matrices can be idempotent
        \end{itemize}
    \end{itemize}
\end{itemize}
Matrix Differentiation
\begin{itemize}
    \item Let $a$ and $x$ be $n\times 1$ vectors and A be an $n\times n$ matrix
    \begin{itemize}
        \item $\frac{\partial (a^\prime x)}{\partial x_1} = a$
        \item $\frac{\partial (x^\prime Ax)}{\partial x} = (A+A^\prime) x$
        \item $\frac{\partial (x^\prime Ax)}{\partial x \partial x^\prime} = A+A^\prime$
    \end{itemize}
\end{itemize}
\subsection{Derivation of Least Squares Estimators}
\begin{itemize}
    \item $\sum \hat \epsilon_i ^2 = \sum(Y_i - \hat \beta_1 - \hat \beta_2 X_{i2} - \cdots - \hat \beta_k X_{ik})^2$
    \item $min \sum \hat \epsilon_i ^2$ over $\hat \beta_1 , \cdots, \hat \beta_k$ UGLY without matrix notation $\Rightarrow \underset{k\times 1}{\hat \beta} $ minimizes $\underset{1\times n}{\hat \epsilon ^\prime} \underset{n \times 1}{\hat \epsilon}$ where $\underset{n\times 1}{\hat \epsilon} = \underset{n\times 1}{Y} - \underset{n\times k}{X} \underset{k\times 1}{\hat \beta}$
    \item $\underset{(n\times 1)}{Y} = \underset{(n\times k)}{ X} \underset{(k\times 1)}{\beta} + \underset{(n\times 1)}{\epsilon}$, where each row represents an observation (so n total observations)
    \item Assumptions we need
    \begin{itemize}
        \item $E[\epsilon_i] = 0$
        \item $E[\epsilon_i^2] = \sigma^2$ (heteroskedascity)
        \item $E[\epsilon_i \epsilon_j] = 0$ (no serial correlation)
    \end{itemize}
    \item How do we represent these assumptions in matrix notation
    \begin{itemize}
        \item $E[\epsilon \epsilon^\prime] = \sigma^2 \underset{(n\times n)}{I}$. In other words, we must get a diagonal matrix to prove no serial correlation (outer product)
        \begin{itemize}
            \item Diagonal is $\sigma^2$ in order to prove constant variance/heteroskedasticity
            \item The fact that the matrix is diagonal proves no serial correlation
        \end{itemize}
        \item For first condition of unbiasedness: $E[\epsilon] = \vec 0_k$
        \item To conclude, we have two conditions
        \begin{itemize}
            \item $E[\epsilon] = \vec 0_k$
            \item $E[\epsilon \epsilon^\prime] = \sigma^2 \underset{(n\times n)}{I}$
        \end{itemize}
    \end{itemize}
\end{itemize}
\textbf{Actual Derivation}
\begin{itemize}
    \item $Y = X\beta + \epsilon \leftrightarrow \epsilon = Y- X\beta$
    \item $\sum_{i=1}^k \epsilon_i ^2 = \epsilon\prime \epsilon$ (dot product)
    \item Goal: Find $\underset{k\times 1}{\hat \beta}$ that minimizes $\hat \epsilon^\prime \hat \epsilon$ (dot product sum) where $\hat \epsilon = Y - X\hat \beta$
    \item $\underset{\hat \beta}{min(Y-X\hat \beta)^\prime(Y-X\hat \beta)}$ (subbed in for epsilon)
    \item $\underset{\hat \beta}{min} (Y^\prime - \hat \beta ^\prime X^\prime) (Y-X\hat \beta)$ (applied transpose)
    \item $\underset{\hat \beta}{min} (Y^\prime Y - \hat \beta^\prime X^\prime Y - Y^\prime X \hat \beta + \hat \beta^\prime X^\prime X\hat \beta)$ (distributed)
    \item $\underset{\hat \beta}{Y^\prime Y - 2Y^\prime X \hat \beta + \hat \beta^\prime X^\prime X \hat \beta)}$. Take partial derivative with respect to beta
    \begin{itemize}
        \item For $Y^\prime Y$ partial derivative of this is 0
        \item First matrix diff. rule: $-2Y^\prime X \hat \beta$
        \item Use second matrix diff. rule on: $\hat \beta^\prime X^\prime X \hat \beta$
        \item Recall: $(x^\prime x)^\prime = x^\prime x$ (helps with simplifying)
    \end{itemize}
    \item $-2 \frac{\partial}{\partial \hat \beta} (Y^\prime X\hat \hat \beta) + \frac{\partial}{\partial \hat \beta} (\hat \beta^\prime X^\prime X \hat \beta)=0$. $-2 X^\prime Y + 2(X^\prime X)\hat \beta = 0$ (``normal equations")
    \item $-2 X^\prime Y + 2(X^\prime X)\hat \beta = 0$
    \item Simplify via matrix algebra
    \item $\hat \beta = (X^\prime X)^{-1} X^\prime Y$
    \begin{itemize}
        \item $X^\prime Y \leftarrow$ empirical covariance between $X$ and $Y$.
        \item $X^\prime X$ is $Var(X)$
        \item \textbf{VERY IMPORTANT INTUITION: }Linear regression is the $\hat \beta = \frac{Cov(X,Y)}{Var(X)}$
        \begin{itemize}
            \item ``How much does Y vary if we only vary X?"
        \end{itemize}
    \end{itemize}
\end{itemize}
\subsection{Expectation of $\hat \beta$}
\begin{itemize}
    \item $\hat \beta = (X^\prime X)^{-1} X^\prime y = (X^\prime X)^{-1} X^\prime (X\hat \beta + \epsilon)$ (subbed in y)
    \item $\Rightarrow \hat \beta = (X^\prime X)^{-1} (X^\prime X)\beta + (X^\prime X)^{-1} X^\prime \epsilon = \beta + (X^\prime X)^{-1} X^\prime \epsilon$
    \item Take expectation: $E(\hat \beta) = \beta + E[(X^\prime X)^{-1} X^\prime \epsilon] = \beta + (X^\prime X)^{-1} X^\prime E(\epsilon) = \beta$. RECALL: $E(\epsilon) = 0$ via Gauss-Markov assumptions, leaving only $\beta$
\end{itemize}

\subsection{Covariance Matrix of $\hat \beta$}
\[\underset{k\times k}{cov(\hat \beta)} = \begin{bmatrix}
    var(\hat \beta_1) & cov(\hat \beta_1, \hat \beta_2 ) &\cdots\\
    cov(\hat \beta_2, \hat \beta_1) & var(\hat \beta_2) & \cdots \\
    \cdots & \cdots & \cdots
\end{bmatrix} = E[(\hat \beta- \beta)(\hat \beta - \beta)^\prime]\]

\textbf{TODO}

\subsection{Efficiency}
\begin{itemize}
    \item Recall: $\overset{\sim }{\beta} = \sum_i b_i Y_i$. Replace it with matrix C
    \item $\hat \beta$ is BLUE (Gauss-Markov Theorem) $\Rightarrow$ for any $\underset{k\times 1}{\overset{\sim}{\beta}} = \underset{k\times n}{C}\underset{n\times 1}{Y}$
\end{itemize}

\subsection{Asymptotitc Normality}
\begin{itemize}
    \item Under our 4 Gauss-Markov assumptions
    \item $\hat \beta \sim a N(\beta , \sigma^2 (X^\prime X)^{-1}$
    \item If $\epsilon \sim N(0, \sigma^2 I)$, this is exact distribution, not just asymptotically
\end{itemize}

\subsection{Estimation of $\sigma^2$}
\begin{itemize}
    \item Unbiased and consistent estimator for $\sigma^2$
    \item $s^2 = \frac{\hat \epsilon ^\prime \hat \epsilon}{n-k}$ where $\hat \epsilon^\prime \hat \epsilon$
\end{itemize}

\subsection{Dummy Variables and Chow Test}
Dummy Variable
\begin{itemize}
    \item A regressor $X_{ij}$ which only takes either 0 or 1 as a value is a dummy variable; indicator functions (discontinuous/discrete function that is either 1 or 0)
    \item Interaction term ($X_{i4}$ here, $X_{i4} = X_{i2} \cdot X_{i3} , \text{ 
 } X_{i2}:\text{ age}, X_{i3}: \text{ isEmory}$): Will allow to pick out and ``control for" age differences in the two groups. Age times isEmory (dummy).
    \begin{itemize}
        \item When we are NOT in the treatment group, we get the number 0, generating an interaction term
        \item We want to control for age in the treatment group (isEmory)
        \item The dataset has been split to 2 based on dummy variable
    \end{itemize}
    \item $Y_i = \beta_1 + \beta_2 X_{i2} + \beta_3 X_{i3} + \beta_4 X_{i4} + \epsilon_i \Rightarrow  \\\begin{cases}
        \beta_1 + \beta_2 X_{i2} + \epsilon_i \text{ if not Emory grad}\\
        (\beta_1 + \beta_3 ) + (\beta_2 + \beta_4) X_{i2} + \epsilon_i \text{ if Emory grad (dummy variable is 1, so simplify)}
    \end{cases}$
    \item Run two separate regressions, since we now have 2 (non-Emory is our control group, Emory is our treatment Group)
    \item Let $\alpha_1 = \beta_1 + \beta_3,\alpha_2 = \beta_2 + \beta_4$ equal our new parameters for the treatment group
    \item Recall: identification: $\beta_3 \rightarrow \alpha_1 - \beta_1, \beta_4 \rightarrow \alpha_2 - \beta_2$, where the $\beta_1, \beta_2$ come from our first regression
    \item age is a confounder. 
    \item Possible $H_0: \beta_3 = \beta_4 = 0 \Leftrightarrow$ being Emory grad has no effect on salary
    \begin{itemize}
        \item Normally, 2 tests (each beta equals 0). However, we can do this in 1 test with F-test
        \item $\beta_3 = 0 \rightarrow \alpha_1 = \beta_1$ (is the intercept of non-Emory grads the same as Emory grads)
        \item Same goes for $\beta_4$ with slope and other alpha
    \end{itemize}
    \item Notes:
    \begin{itemize}
        \item IMPORTANT (multicollinearity): When the regression contains an intercept, and there are m groups, only include m-1 dummies, for identification (otherwise case of perfect multicollinearity - discussed later), to avoid ``dummy variable trap"
        \item Multicollinearity
        \begin{itemize}
            \item Linear dependence: (design) matrix must be independent (recall: no column can be expressed as linear combination of others)
            \item If not independent, matrix is not invertible. Think full-rank matrix
            \item `Perfect multicollinearity': columns are linearly dependent
            \item If there is no intercept, we can have $m$ dummies for $m$ groups without multicollinearity. Otherwise, we need to stick to at most $m-1$
        \end{itemize}
    \end{itemize}
    \item Chow Test: Two regression equations (will not be tested on final)
    \begin{itemize}
        \item When dealing with time-series, we have a `regime shift' due to an external influence. Run linear regression on data before time T (when regime shift occurred) and after time T
        \item Change-point detection: setting T
        \item Problem: can only test one or two types of regressions
        \item In a Chow test, test that both intercept and the slope are the same for the two regressions
    \end{itemize}
\end{itemize}

\subsection{Multicollinearity}
\begin{itemize}
    \item Basic Assumptions used so far:
    \begin{itemize}
        \item $Y=X\beta + \epsilon$
        \item X fixed (non-random)
        \begin{itemize}
            \item Will relax up until end of class
        \end{itemize}
        \item X full rank (invertibility)
        \begin{itemize}
            \item Relax now
        \end{itemize}
        \item $E(\epsilon)=0$
        \item $var(\epsilon) = \sigma^2 I$
        \begin{itemize}
            \item Relax the 2 Gauss Markov assumptions in Section 11
        \end{itemize}
    \end{itemize}
\end{itemize}
GLS (generalization of OLS)
\begin{itemize}
    \item If Gauss-Markov not satisfied, then OLS not best estimator. Best lowest variance estimator is now GLS
    \item If interested, look at lecture notes
\end{itemize}
\section{Endogeneity}
\subsection{Omitted Variables/Omitted Variable Bias}
\begin{itemize}
    \item The \textbf{omission of one or more relevant variables} from the regression causes the estimates of the included variables to be \textbf{biased}. \textbf{So: $E(\tilde \beta)\neq \beta$}
    \item \textbf{Direction of the bias }depends on covariances between included and excluded variables and on the signs of the excluded variables' coefficients
    \begin{itemize}
        \item This is \textbf{bad}: we don't even know the direction of the bias. For example, with omitted variables, we can predict that chocolate consumption leads to longevity
    \end{itemize}
    \item If \textbf{every excluded variable is orthogonal} to every included variable (matrix algebra sense), then the OLS estimates of the included variables will be unbiased
    \begin{itemize}
        \item Implication of orthogonal: $\frac{Cov(X,Z)}{Var(X)} = 0$
    \end{itemize}
    \item \textbf{Endogeneity}
    \begin{itemize}
        \item X is random (itself)
        \begin{itemize}
            \item We can't do this anymore (pull out X): $E[\epsilon X] = XE[\epsilon] = 0$, since it requires X to be nonrandom. If X is not random, then it is not correlated with anything
            \item Need to deal with covariance now: $cov(X,\epsilon) = E[X\epsilon] + E[X]E[\epsilon]$ (considering first moment). We still assume $E[\epsilon] = 0$.
            \item Note: in matrix form, $E[X^\prime \epsilon] \leftarrow$ we need to transpose X to match up dimensions
            \item $E[X\epsilon] = 0$. X is an exogenous regressor iff. it is an uncorrelated regressor
            \item Endogeneity is the opposite: it says that $X$ is correlated. From now on, we will mostly handle endogeneity
            \item Endogeneity is a \textbf{first-order problem} (our estimator is biased). It doesn't deal with the best estimator in terms of best variance, we are saying we don't get the proper estimator if we don't address it ($E(\tilde \beta) \neq \beta$)
        \end{itemize}
        \item \textbf{True Model:} $Y_i = \beta X_i + \gamma Z_i + \epsilon \Rightarrow \hat \beta$
        \begin{itemize}
            \item If we have an omitted variable, we think that this is the model we should consider
            \item What we wish we could do
        \end{itemize}
        \item \textbf{Estimated model:} $Y_i = \beta X_i + u_i \Rightarrow \tilde\beta$
        \begin{itemize}
            \item What we're forced to do. This is the wrong model
            \item \textbf{Important:} respond by running an estimator on the estimated model, we call the new estimator $\tilde \beta$
            \item $\tilde \beta = \frac{\sum X_iY_i}{\sum X_i^2} = \text{Plug in the true model} \frac{\sum X_i (\beta X_i + \gamma Z_i + \epsilon_i)}{\sum X_i^2} = \beta + \gamma \frac{\sum X_i Z_i}{\sum X_i ^2} + \frac{\sum X_i \epsilon_i }{\sum X_i^2}$
            \begin{itemize}
                \item  (in order to see what we want, we need to compare it to the truth). We want to see what we're doing wrong 
                \item Take an expectation on both sides
            \end{itemize}
            \item $E\hat \beta = \beta + \gamma \frac{\sum X_i Z_i}{\sum X_i ^2} + \frac{\sum X_i E(\epsilon_i )}{ \sum X_i^2} = \beta + \gamma \frac{\hat \sigma_{XZ}}{\hat \sigma^2 _X} \neq \beta$ where $\hat \sigma_{XZ} = \frac{1}{n} \sum X_iZ_i$ (covariance) and $\hat \sigma^2 _X = \frac{1}{n}\sum X_i^2$ (variance of X)
            \item \textbf{IMPORTANT TERM}: $\gamma \frac{\hat \sigma _{XZ}}{\hat \sigma_X ^2}$. When is $\gamma = 0$ or covariance is 0? When variable has no impact, unbiased.
            \item We control for Z: we have
            data on Z we can include in our regression
        \end{itemize}
        \item An omitted variable is only bad if it affects both the outcome and affects the regressor. In other words, we are taking the effects of one regressor (which we are removing) and mistakenly attributing it to another (Causal Inference). 
        \item If the variables are uncorrelated, then there's no issue
        \item Keeping in the Z regressor controls for Z
    \end{itemize}
    \item Solution to omitted variable bias?
    \begin{itemize}
        \item No solution really, include as many terms as possible
    \end{itemize}
\end{itemize}

\subsection{Irrelevant Variables}
Context
\begin{itemize}
    \item Inclusion of irrelevant variables does NOT cause the OLS estimators of the other variables to be biased. So $E\tilde \beta = \beta$. So, it doesn't lead us to a first-order problem, but it does lead to a second-order problem.
    \item The problem that including irrelevant variables does cause is that it reduces the precision with which you estimate the coefficients on the other, relevant variables. So $var(\tilde \beta) > var(\hat \beta)$ (similar to overfitting)
    \begin{itemize}
        \item Issue: affects standard error, which is going to screw up with tests. Recall $t = \frac{\tilde \beta}{SE[\tilde \beta]}$
        \item ``kitchen sinking" regressions (throw everything into regressions, doesn't matter if it's relevant or not)
    \end{itemize}
    \item This efficiency loss does not occur when the irrelevant variable s are orthogonal to the remaining variables. So when $X^\prime Z=0$ (where X are the included relevant variables, and Z are the included irrelevant variables) $\Rightarrow \tilde \beta = \hat \beta, var(\tilde \beta) = var(\hat \beta)$
    \item \textbf{Example}: Assume that $\bar X =\bar Z = \bar Y = 0$
    \begin{itemize}
        \item True model: $Y_i = \beta X_i + \epsilon_i \Rightarrow \hat \beta$
        \item Estimated model: $Y_i = \beta X_i + \gamma Z_i + u_i \Rightarrow \tilde \beta$
        \item In our derivation, remember to plug in the true model. Note $Z_i, X_i$ are nonrandom. If Z and X are uncorrelated with epsilon: $E[Z_i \epsilon_i] = 0, E[X\epsilon_i] = 0$ (no correlation with error), then we are fine (NO PROBLEM). If there is a correlation with epsilon, then we do have a higher variance
    \end{itemize}
\end{itemize}

\subsection{Measurement Error}
\begin{itemize}
    \item $Y_i^* = \beta_1 + \beta_2 X_i ^* +\epsilon_i$
    \item The $X_i ^*$ are assumed fixed, but $X_i ^*, Y_i ^*$ are not observed directly. Instead, we observe the mismeasured $X_i, Y_i: Y_i^* + \nu_i$
    \item Likewise, we assume $X_i = X_i ^* + \omega_i$
    \item Even though the $X_i^*$ are fixed, the $X_i$ are random
    \item Greater the measurement error, the greater the variance of your regression terms, the worse your regression will be
    \item $Y_i = \beta_1 + \beta_2 X_i + \epsilon_i$
    \item True model: $Y_i = \beta_1 + \beta_2 X_i + \epsilon_i$
    \item Bad model: $Y_i^* + v_i = \beta_1 + \beta_2 (X_i ^* + \omega_i) + \epsilon_i$
    \item \textbf{Classical Measurement Error}
    \begin{itemize}
        \item We make the following assumptions for this model:
        \begin{itemize}
            \item As usual $E(\epsilon_i) = 0$
            \item $E(\nu_i)=0, E(\nu_i^2) = \sigma^2 _\nu, E(\nu_i \nu_j) = 0\forall i\neq j$
            \item $E(\omega_i) = 0, E(\omega_i ^2) = \sigma^2_\omega, E(\omega_i\omega_j) = 0\forall i\neq j$
            \begin{itemize}
                \item We need to assume that $\sigma^2 > 0$ for variance and randomness (not a point-mass)
            \end{itemize}
            \item $E(\nu_i\omega_j) = E(\nu_i \omega_j) = E(\omega_i \epsilon_j) = 0\forall i,j$
        \end{itemize}
        \item Expectation of product of error terms is 0, because uncorrelated (independent). We expect error to be 0. These assumptions need not be correct, these are modeling assumptions.
        \item $(Y_i - \nu_i ) = \beta_1 + \beta_2(X_i -\omega_i) + \epsilon_i \leftarrow $ true model: $Y^*_i = \beta_1 + \beta_2 X_i^* + \epsilon_i$
        \item $Y_i = \beta_1 + \beta_2 X_i + \epsilon_i + \nu_i - \beta_2 \omega_i$, let $\eta_i = \epsilon_i + \nu_i - \beta_2 \omega_i$. This $\eta_i$ is our new error term
        \item \textbf{Recall:} The error term accounts for everything we cannot see
        \item Compute covariance of $Cov(X_i,Y_i) = E[X_i\eta_i] - E[X_i] E[\eta_i]$
        \begin{itemize}
            \item Plug in true model
            \item \textbf{Note that (using true model):} $E(X_i \eta_i) = E[(X_i^* + \omega_i )(\epsilon_i + \nu _i - \beta_2 \omega_i)] = \beta_2 \sigma_\omega^2$
            \begin{itemize}
                \item $E[X_i^* \epsilon_i] + E[X_i^* \nu_i ] - E[X_i^* \beta_2 \omega_i] + E[\epsilon_i \omega_i] + E[\omega_i \nu_i] - \beta_2 E[\omega_i^2]$ (distributed and multiplied out everything)
                \item \textbf{Recall:} we assume that there is ZERO correlation between error terms and $X$, which reflects in those expectations being 0. This leaves: $E[\epsilon_i \omega_i] + E[\omega_i \nu_i] - \beta_2 E[\omega_i ^2]$
            \end{itemize}
        \end{itemize}
        \item \textbf{Recall:} $E[\hat \beta_2] = E\left[\frac{\sum_i (Y_i - \bar Y)(X_i - \bar X)}{\sum_i (X_i - \bar X)^2}\right]$
        \begin{itemize}
            \item Plug in our model $Y_i = \beta_1 + \beta_2 X_i + \eta_i$
            \item We want to see $E[\hat \beta] = \beta$
            \item $\tilde \beta_2 = \frac{\sum X_i Y_i }{\sum X_i^2} = \frac{X_i (\beta X_i + \eta_i)}{\sum X_i^2} = \beta_2 + \frac{\sum X_i \eta_i}{\sum X_i ^2}) $ \textbf{TODO}
            \item At the end, we show that $plin \tilde \beta = \beta + \frac{-\beta \sigma^2 _\omega}{\sigma^2 _{X^*} + \sigma^2 _\omega}$, so our estimator plus our bias term
            \item What does the bias term stand for?
            \begin{itemize}
                \item True $\beta_2$ times variance of $X_i = X_i^* + \omega_i$, where $E[\omega_i] = \sigma^2_\omega$. In denominator, $\sigma^2_{X^*}$ should be $0$, since it is deterministic
            \end{itemize}
            \item \textbf{IMPORTANT:} This is called the ``Iron Law of Econometrics"
                \begin{itemize}
                    \item $\hat \beta_2 = \beta[1- \frac{\sigma^2 _\omega}{\omega^2_{X^*} + \sigma^2 _\omega}]$
                    \item Let's call the bias term $A$, where $A\in (0,1]$. The bias term \textbf{decreases} the slope, meaning our $\hat \beta_2 \Rightarrow$ biased towards 0 
                    \item If $\sigma^2_\omega \text{ Large } \rightarrow \text{Lots of noise in data} \rightarrow \text{ undershoot the true value in magnitude!}$
                    \item \textbf{Note:} We \textbf{don't} include the measurement error in $Y_i$ as represented by $\sigma^2_\nu$, meaning that measurement error in $Y_i$ \textbf{DOES NOT} bias $\hat \beta_2$ in any way
                    \item Why? Since, $Y-i = Y^*_i +\nu_i$ gets lumped in with our $\epsilon_i$: $Y_i = \beta_1 + \beta_2 X_i^* + \nu_i + \epsilon_i$. Measurement error in $Y_i$ matters for variance, but is an independent term from $\beta_2$, so it does not affect bias
                    \item Large variance leads to undershooting on the t-statistic test, since we decrease SE[$\hat \beta$], as $SE[\hat \beta] = \sqrt{Var(\hat \beta)}$
                \end{itemize}
        \end{itemize}
    \end{itemize}
\end{itemize}

\section{Simultaneous Equation Models}
\begin{itemize}
    \item Model: $Y=\beta X + \epsilon$, but now X is random and $E(X_\epsilon) = Cov(X,\epsilon)\neq 0$
    \begin{itemize}
        \item $\hat \beta = \frac{Cov(Y,X)}{Var(X)}$
        \item $r^2 = \frac{Cov(X,Y)}{\sqrt{Var(X)} \sqrt{Var(Y)}}$
        \item A regression measures the impact of X on Y: $\bar X \rightarrow \bar Y$. It cannot tell us the opposite error where Y influences X
        \item Endogeneity: 
        \item Basic example: supply and demand
        \begin{itemize}
            \item $Q_t^D = \alpha + \beta P_t + \epsilon_t$ (quantity demanded)
            \item $Q_t^S = \gamma + \delta P_t + u_t$ (quantity supplied)
            \item $Q=Q^D = Q^S$: equilibrium
            \item Problem: $P_t$ determined by $Q^S = Q^D$, so \[P_t = \frac{\gamma - \alpha}{\beta - \delta} + \frac{u_t - \epsilon_t}{\beta - \delta} = u_P + \nu_t\]
            We basically got the difference in intercepts over difference in slopes (we can call it $\mu_P$), plus an error term (never observed), we can call it $\nu_t$, so $P_t = \mu_P + \nu_t$
            \begin{itemize}
                \item This error term introduces an endogeneity problem
                \item The error terms of $P_t$ are related
                \item Why? Because they are both a function of $P_t$
                \item Returning original equation, we can rewrite quantity equations in terms of price: $P_t = \frac{Q_t - \gamma - u_t}{\delta}$
                \item Problem: We have a feedback loop where $P_t$ leads to a change in $Q_t$ which leads to a change in $P_t$
                \item The idea: we have two equations that are interlinked, creating a feedback loop that screws us with endogeneity problems
                \item This is our gateway to causal inference. We can intuitively phrase causal inference problems in terms of positive feedback loops
                \item When do we know that we don't have this issue? Gauss-Markov Assumptions are met:
                \item $E[\epsilon_i] = 0$
                \item $E[P_t \epsilon_i] = 0$
            \end{itemize}
            \item This implies that \[ Cov(P_t\epsilon_t) \neq 0\text{ and }Cov(P_t,u_t) \neq 0\]
        \end{itemize}
    \end{itemize}
\end{itemize}
Reduced Form: write the equations out without containing the other variable
\begin{itemize}
    \item Solving the system (1) and (2) for $P_t$ and $Q_t$. Eliminate independent variable
    \item In this case we solve both equations for isolated $P_t$ and set them equal to get rid of $P_t$
    \item $P_t = \frac{\gamma - \alpha}{\beta - \delta} + \frac{u_t - \epsilon_t}{\beta - \delta} - u_t + \nu_t$
    \item $Q_t = \frac{\gamma \beta - \delta \alpha}{\beta - \gamma} + \frac{\beta u_t - \delta \epsilon_t}{\beta - \delta}$ = $\mu_Q + \eta_t$
    \item Now we have the following equations\[P_t = \mu_P + \nu_t\]\[Q_t = \mu_Q + \eta_t\]
    \item Now we can get $\hat \mu_P$ and $\mu_Q$ through linear regression. But we're after the parameters in the original equations
    \item Now we have an identification problem: can we solve backward for $\alpha, \beta, \gamma, \delta$??
    \item Identification Problem:
    \begin{itemize}
        \item Can we get the structural parameters given our parameters from our reduced equations?
        \item Add an exogenous shifter: fixes one of the lines, but varies the other \textbf{(geometric intuition: like a raytracer)}
    \end{itemize}
    \item Instrument: an additional variable that helps solve our problem
\end{itemize}
By definition, $\hat \beta = \frac{Cov(Y,X)}{Var(X)}$
\begin{itemize}
    \item But why?
    \begin{itemize}
        \item Intuition: If I vary X, how does it affect Y?
        \item Regression assumes X is exogenous $E[X\epsilon] = 0$
        \begin{itemize}
            \item Recall (half of Gauss Markov), $E[\epsilon_i] -, E[X_i\epsilon_i] = 0\Rightarrow E[\hat \beta] = \beta$
            \item Concept of `Transferability': $X\rightarrow Y$: we can generalize these results where one variable predicts a quantified effect on another. But, we have to confirm that it is one-directional since OLS doesn't handle this by default (causal inference and Simultaneous Equation Models)
        \end{itemize}
    \end{itemize}
\end{itemize}
Returning to causal inference and exogenous shifters, \textbf{we need more data}
\begin{itemize}
    \item Get the data where we hold one line constant, and the other changing: exogenous shifters!
\end{itemize}
How do we formalize this?
\begin{itemize}
    \item Suppose we have two or more variables available: $W_t$ and $Y_t$. The key assumption for those variables is that they must be exogenous \& independent of ($P_t, Q_t$). The new model then takes the form \[Q_t^D = \alpha = \alpha + \beta P_t + \phi Y_t + \epsilon_t \text{(quantity demanded)}\]\[Q_t^S = \gamma + \delta P_t + \psi W_t + u_t \text{(quantity supplied)}\]
    \item Notice that Y is only in one equation (otherwise it would be endogenous, changing both lines when we want to fix one)
    \item It is a \textbf{necessary condition} that the introduced variables Y and W are only showing up in one of these equations
    \item This gives us the \textbf{new reduced form equations} (rewriting equation only in terms of exogenous things)
    \[P_t = \pi_1 + \pi_2 Y_t + \pi_3 W_t + \nu_t\]
    \[Q_t = \rho_1 + \rho_2 Y_t + \rho_3 W_t + \eta_t\]
    \item Exogenous assumption (independent of distribution, uncorrelated with error term): $E[Y_t \nu_t] = 0$
    \begin{itemize}
        \item We need variables to be uncorrelated with error terms in order to consider them exogenous variables
    \end{itemize}
    \item How do we get these equations?
    \begin{itemize}
        \item Start with equilibrium condition: $Q_t ^D = Q_t ^S = Q$
        \item We are solving for the first equation in this example ($P_t$)
        \item $\alpha + \beta P_t + \phi Y_t + \epsilon_t = \gamma + \delta P_t + \Psi W_t + u_t$
        \item Solve for $P_t$
        \item $\beta P_t - \delta P_t = \gamma - \alpha + \Psi W_t - \phi Y_t + u_t - \epsilon_t$
        \item Simplify and solve
        \item We plug in the result for $P_t$ back into one of the $Q$ equations
    \end{itemize}
    \item Our introduced R.V.'s can be highly correlated
    \begin{itemize}
        \item As long as they're not 100\% correlated (perfect multicollinearity) we are fine
    \end{itemize}
    \item \textbf{The reduced form equations (derived)}:\[P_t = \frac{\gamma - \alpha}{\beta - \delta} - \frac{\phi}{\beta - \delta} Y_t + \frac{\Psi}{\beta - \delta} W_t + \frac{u_t - \epsilon_t}{\beta-\delta}\]
    \[Q_t = \frac{\beta \gamma - \alpha \delta}{\beta - \delta} + \frac{-\delta \phi}{\beta - \delta} Y_t + \frac{\beta \Psi}{\beta - \delta} W_t + \frac{\beta u_t - \delta \epsilon_t}{\beta - \delta}\]
    \item We have 6 different equations: $\pi_1, \pi_2, \pi_3, \rho_1, \rho_2 \rho_3$
    \item With these reduced form equations, we introduce `Indirect least squares' (introduced in 1980s). We have built new regressions with reduced form equations
    \item Indirect least squares
    \begin{itemize}
        \item Get reduced form regression
        \item Check how many parameters we have
        \item Back out one variable from the others, and identify it
    \end{itemize}
\end{itemize}

\section{Exogenous shifters}
Observations
\begin{itemize}
    \item If a variable is irrelevant, then it cannot be an exogenous variable
\end{itemize}
Other cases
\begin{itemize}
    \item $\phi=0$: can still identify demand (just identified). Cannot identify supply (underidentified)
    \item $\psi = 0$: Can still identify supply (just identified). Cannot identify demand (underidentified)
\end{itemize}
Underidentified
\begin{itemize}
    \item We don't know enough to identify all variables
\end{itemize}
Just identified
\begin{itemize}
    \item Can figure out all variables in exactly one way (single unique solution)
\end{itemize}
Overidentified
\begin{itemize}
    \item More than one way to get values for the variables
    \item Good, because more than one way to stress-test model (more information is better)
\end{itemize}
When does a unique solution fail? (when can I not identify the model?)
\begin{itemize}
    \item When $\phi = 0$ or $\psi = 0$ (i.e. when the exogenous variable does not belong in the equation.) $\Rightarrow$ The exogenous shifter is an \textbf{instrument}:
    \begin{itemize}
        \item $CoV(Q_t,W_t) \neq 0$ \textbf{(Relevance condition)}. 
        \item $CoV(u_t, W_t ) = 0$ \textbf{(Exogeneity condition)}. Uncorrelated with all variables from the other equation
        \item $W_t$ is excluded from the demand equation (doesn't need to be an instrument for itself)
    \end{itemize}
    \item Instrument: a variable that helps us identify something
    \begin{itemize}
        \item An exogenous shifter is a prime example of an instrument
    \end{itemize}
    \item Instrumental variable regression
\end{itemize}

\section{Instrumental variables}
Recall: Core endogeneity problem:
\begin{itemize}
    \item Given $Y= X\beta + \epsilon, CoV(X,\epsilon)\neq 0 \Rightarrow E[\hat \beta] \neq \beta$
    \item Recall: $\epsilon$ represents affects of what we have not observed
\end{itemize}
Judea Pearl Visualizations
\begin{itemize}
    \item Box means observable, circle means unobservable
    \item Helps illustrate causal inference
    \item Variables can be confounding (giving an endogeneity problem)
    \item Directed Acyclical Graph
    \item ``Book of Why"
    \item self selection
    \begin{itemize}
        \item Heckman
        \item Bias or effects induced by self-selecting into treatments or categories, fuelling a positive feedback loop
        \item self selection is a classic way to introduce an endogeneity problem
    \end{itemize}
\end{itemize}

\textbf{Motivation:} Omitting relevant variables that are correlated with regressors lead to \textbf{biased estimates of the true effect.} Examples seen in this class:
\begin{itemize}
    \item ``Classical omitted variable bias"
    \item Measurement error on the regressor
    \item Simultaneous equation models
\end{itemize}
To resolve the OVB one needs additional information. One method is to use \textbf{instrumental variables} (recall the exogenous shifters from SEM)\\
Instrument
\begin{itemize}
    \item Takes as much covariance out of a variable while being exogenous itself
\end{itemize}

\subsection{A Basic Example (of self-selection): Drug trial with incompliance}
\begin{itemize}
    \item ``Mediator": refers to a variable where the effects of another external variable is represented in this variable
    \item Josh Angrist
    \item We \textbf{randomly assign} a drug treatment to patients. The non-treated patients do not get the drug.
    \begin{itemize}
        \item $X_i = 1$ if received treatment with drug, 0 otherwise
        \item $Y_i = 1$ if health improved, 0 otherwise
    \end{itemize}
    \item \textbf{Potential issue:} We cannot force people to take the drug. Some people might refuse. Omitted variable bias? $\Rightarrow$ Use the additional information of the initially assigned treatment $Z_i$
    \begin{itemize}
        \item While the groups were initially randomized, the groups got together and pooled their drugs and placebos (sharing doses, invalidating treatment and control groups) or just didn't take it
    \end{itemize}
    \item ``intent to treat" variable $\rightarrow$ is a nice instrumental variable for this problem
    \item What did Josh Angrist do?
    \begin{itemize}
        \item Offered drug instead of assigning group (not explicitly said but implied)
    \end{itemize}
    \item Numerical example:
    \begin{itemize}
        \item \begin{tabular}{|c|c|c|}
            \hline
            Offered Drug Z & Received drug X & Improved health Y \\
            \hline
            Yes&60\%&40\%\\
            No & 0\% & 10\% \\
            \hline
        \end{tabular}
        \item Causal effect on health of being offered drug: $40\%-10\% = 30\%$
        \item This number underestimates the causal effect of receiving the drug, since only 60\% accept the treatment offer
        \item Adjust to contain the     causal effect of actual treatment X on health Y:
        \[\frac{\partial Y}{\partial X} = \frac{\partial Y/\partial Z}{\partial X/\partial Z} = \frac{30\%}{50\%} = 50\%\]
        In English: $\frac{Z\rightarrow Y}{Z \rightarrow X} \Rightarrow X\rightarrow Y$. The effect of Z on Y with respect to the effect of Z on X
        \item We use Z as a control to fix X for feedback, creating a causal effect, $\epsilon$ does not produce any bias since $Z\ind \epsilon$
        \item Implicit assumption: Z only affects Y through X
    \end{itemize}

\end{itemize}
\subsection{The simplest IV model}
\[Y_i = \beta_0 + \beta_1 X_i + u_i\]
\begin{itemize}
    \item We are not willing to assume $E[u_i|X_i] = 0$ so a regression of $Y_i$ on $X_i$ may give biased estimates
    \item Two problems now
    \begin{itemize}
        \item \textbf{Exogeneity:} $Cov(u_i, Z_i) = 0$. So, $Z_i$ is ``as good as randomly assigned", and $Z_i$ is only related to $Y_i$ through $X_i$
        \item \textbf{Relevance:} $cov(X_i, Z_i) \neq 0$. So when $Z_i$ varies, $X_i$ also varies (on average).
        \item Recall: $E[\hat \beta_1] \neq \beta_1$ if $Cov(X_i, Z_i) \neq 0$
        \item Due to the exogeneity assumption, we have
        \[cov(u_i,Z_i) = E[u_iZ_i] - E[u_i]E[Z_i] = 0\]
        Hence, 
        \[cov(Y_i, Z_i) = cov(\beta_0 + \beta_1 X_i + u_i, Z_i)\]
        \[=\beta_1 cov(X_i, Z_i) + cov(u_i, Z_i)\]
        \[=\beta_1 cov(X_i, Z_i)\]
        Assume $cov(u_i, Z_i) = 0$ since we assume $Z_i$ is an exogenous variable. Using the relevance assumption, we obtain
        \[\beta_1 = \frac{cov(Y_i, Z_i)}{cov(X_i, Z_i)}\]
        \item How does this relate to $\frac{\partial Y / \partial Z}{\partial X / \partial Z} = \frac{\frac{Cov(Y,Z)}{Var(Z)}}{\frac{Cov(X,Z)}{Var(Z)}}$?
        \begin{itemize}
            \item The $Var(Z)$ terms cancel out
        \end{itemize}
        The derivation suggests the estimator \textbf{(Wald IV estimator)}
        \[\hat \beta_1 = \frac{\widehat{cov}(Y_i, Z_i)}{\widehat{cov} (X_i, Z_i)} = \frac{\sum_{i=1}^n (Y_i - \bar Y)(Z_i - \bar Z)}{\sum_{i=1} ^n (X_i - \bar X)(Z_i - \bar Z)}\]
        \item If we can find an estimator that is both exogenous and relevant, then we have a new correct estimator. We can solve endogeneity problems using OLS linear regression with this estimator
        \begin{itemize}
            \item Covariance of Y and Z normalized by covariance of X and Z
        \end{itemize}
        \item What's the caveat?
        \begin{itemize}
            \item It's hard to find Z, but if we do, we get a correct result
            \item Note: \textbf{RECALL} that Z CANNOT have a direct effect on Y, because it is only allowed to have an effect via X, which makes it a great estimator
        \end{itemize}
        \item We can also write the estimator as (2 regressions at once)
        \[ \hat \beta_1 = \frac{\sum_{i=1}^N (Y_i - \bar Y) (Z_i - \bar Z) / \sum_{i=1}^n (Z_i - \bar Z)^2}{\sum_{i=1}^N (X_i - \bar X) (Z_i - \bar Z) / \sum_{i=1}^n (Z_i - \bar Z)^2} = \frac{\hat \pi_1}{\hat \gamma_1}\]
        where
        \begin{itemize}
            \item $\hat \pi_1$ is the slope coefficient in a regression of $Y_i$ on $Z_i$ (and a constant). This is called the \textbf{reduced-form regression}
            \begin{itemize}
                \item We first heard of reduced-form for supply and demand
            \end{itemize}
            \item $\hat \gamma_1$ is the slope coefficient in a regression of $X_i$ on $Z_i$ (and a constant). This is called the \textbf{first-stage regression}
            \item Instrumental variable is reduced-form regression over first-stage regression
        \end{itemize}
        \item We use the first-stage coefficient to adjust the reduced-form coefficient for the fact that $X_i$ may not move 1-for-1 with $Z_i$
        \item \textbf{First-stage:} $\gamma_1$: effect of X on Z
        \item \textbf{Second-stage:} $\beta_1$: effect of Y on X, which what we want to determine, but we need X to `mediate' the effect of Z on Y to determine X
        \item \textbf{Reduced-form coefficient:} $\pi_1 = \gamma_1 \beta_1$ (first-stage coefficient times second-stage coefficient)
        \begin{itemize}
            \item An intuitive way to think about this, effect of Y on Z. The product comes from chain rule
            \item $\frac{\partial}{\partial Z} Y(X(Z)) = \frac{\partial Y}{\partial X} \frac{\partial X}{\partial Z} = \beta_1 \gamma_1$
        \end{itemize}
        \item In the case where $Z_i$ is binary (0 or 1), one can show that the estimator equals \[\hat \beta_1 = \frac{}{}\]
    \end{itemize}
    \item it can also be shown that $var(\tilde \beta) > var(\hat \beta)$ unless $\sum X_i Z_i = 0$
    \item If $\sum X_i Z_i = 0,$ however, we see that\[\tilde \beta = \frac{(\sum Z_i^2 ) (\sum X_i Y_i)}{}\]
\end{itemize}
\subsection{2-SLS (2-stage Least Squares Regression)}
Diff from iV regressor
\begin{itemize}
    \item IV is reduced-form over first-stage. This is a more general form
    \item Since we only regress X on a single exogenous variable Z, X becomes exogenous itself, so we regress endogenous variable Y on exogenous variable X
    \item We need at least one instrument for each endogenousvariable
\end{itemize}
The 2-stage least-squares
\[Y_i = \beta_0 + \beta_1 X_i + u_i \leftarrow E[X_i u_i] \neq 0\]
\begin{itemize}
    \item There's another way of thinking about the Wald IV estimator
    \[X_i = \gamma_0 + \gamma_1 Z_i + \nu_i\]
    \item Often represented as a systems of equations, including assumptions $E[X_i u_i] \neq 0$ (X is endogenous), $E[Z_i u_i ] = 0$ (Z is exogenous), and naturally $E[Z_i \nu_i] = 0$
    \[Y_i = \beta_0 + \beta_1 X_i + u_i\]
    \[X_i = \gamma_0 + \gamma_1 Z_i +\nu_i\]
    \item We impose the assumptions:
    \begin{itemize}
        \item \textbf{Exogeneity:} $E[u_i | Z_i] = E[\nu_i | Z_i] = 0$. Or (same thing), $E[u_iZ_i] = E[\nu_i Z_i ] = 0$
        \item \textbf{Relevance:} $\gamma_1 \neq 0$. This is equivalent to $cov(X_i, Z_i) \neq 0$
    \end{itemize}
    \item Substitute $X_i$ to get \textbf{reduced form:} $Y_i = \beta_0 + \beta_1 (\gamma_0 + \gamma_1 Z_i + \nu _i ) + u_i$
    \item Here, we can draw the system/DAG to show why Z MUST be exogenous (NO parent nodes)
    \item Suppose we observed just the ``exogenous part" of $X_i$: \[\bar X_i \overset{def}{=} \gamma_0 + \gamma_1 Z_i\]
    \item By the exogeneity assumptions (todo)
    \item \textbf{IMPORTANT:} This suggests the following \textbf{two-stage least-squares (2SLS)} estimator:
    \begin{itemize}
        \item Regress $X_i$ on $Z_i$ and on an intercept. Construct the fitted values $\hat X_i = \hat \gamma_0 + \hat \gamma_1 Z_i$ for all observations $i$
        \item Regress $Y_i$ on $\hat X_i$ and an intercept. The coefficient on $\hat X_i$ is the estimator $\hat \beta_1$ of $\beta_1$
        \item We have gone through all of this to produce a more generalizable model. How is this generalized?
        \begin{itemize}
            \item We include an intercept
            \item We don't require X to be exogenous
            \begin{itemize}
                \item We don't care about $\hat \gamma_0$ or $\hat \gamma_1$, we just care about the fact they predict $\hat X_i$
                \item By using Z to fix/hold X, we prevent it from being susceptible to other exogenous terms like $U$ (error term). We want as high of a correlation as possible between $Z_i$ and $X_i$: $CoV(Z_i, X_i)\uparrow$
            \end{itemize}
        \end{itemize}
        \item We can envision X as a vector on a plane Z. If X is highly correlated with Z, then way smaller angle with Z plane. The height of X over the Z plane is other exogenous factors influencing X
        \item This 2-SLS works well in overidentified systems
    \end{itemize}
\end{itemize}

\subsubsection{The matrix version of the 2-SLS estimator}
Now, consider the model \[Y=X\beta + \epsilon\]and assume that all regressors in $X$ (an $n\times p$) matrix are endogenous. We also assume that we have potential instruents $Z=(Z_1^T , ..., Z_q^T)$. In order to hav unbiased estimates, \textbf{each endogenous $x_{ij}$ needs to have at least one valid instrument $Z_{ij}$}
\begin{itemize}
    \item How do we combine the information of the instruments $Z_j$ to obtain unbiased estimators
    \item If only a part of X is endogenous, the exogenous variables can be used as their own instruments
    \item Assume we have $p$ regressors in our model:
    \[Y_i = \beta_1 X_{i1} + ... + \beta_{p-1} X_{i(p-1)}\]
    \item Let $q$ represent numbe  of instruments. We require: $q > p$
\end{itemize}
If we have \textbf{exactly one} instrument $Z_{ij}$ for each $X_{ij}$ i.e. if both Z and X are of dimension $n\times p$ (\textbf{just identified}), we can write the Wald estimator (Recall: $\hat \beta = \frac{Cov(\hat Y, Z)}{Cov(\hat X,Z)}$, you should be able to intuitively see this in matrix form: $Z^TX$ and $Z^T Y$ are analogous to covariance) as \[\hat \beta = (Z^TX)^{-1}Z^T Y\]
If $q > p$, the matrix $Z^TX$ is $q\times p$ and not invertible, so the Wald IV estimator does not work. The 2-SLS estimator still does work, however, we can still do the 2-SLS
\begin{itemize}
    \item Picture the first step as projecting hte information of $X$ onto $Z$, and then use this projected information in a second stage to run the full regression: \[P_Z X\equiv Z(Z^TZ)^{-1} Z^T\]
    \begin{itemize}
        \item $Z^TX = z_1 x_1 + z_2 x_2 + ... \Rightarrow \frac{1}{n-2}\sum_i z_i x_i \Rightarrow Cov(\hat Z_i, X_i ) = \frac{1}{n-2} \sum_i (Z_i -\bar Z) (X_i - \bar X)$
    \end{itemize}
    \item The fitted values of the first regression can be written as \[\hat X= P_ZX = Z(Z^TZ)^{-1} Z^T X\]
    \begin{itemize}
        \item Why an additional Z at the beginning: $\hat X = Z \hat \gamma$
        
    \end{itemize}
    \item Note that $P_Z = Z(Z^TZ) Z^T$ is a projection, because it is:
    \begin{itemize}
        \item Symmetric $(P_Z^T = P_Z)$ and 
        \item Idempotent $(P_ZP_Z = P_Z)$. A projection of a projection will just be the same point. Recall: idempotent definition in matrices
        \item General closed-form 2-SLS (2-stage least squares) formula
        \[\hat \beta^{2SLS} = (X^T Z(Z^T Z)^{-1} Z^T X)^{-1} X^T Z(Z^TZ)^{-1} Z^T Y\]
        \begin{itemize}
            \item Combines reduced-form over first-stage regression (Wald IV estimator)
            \item This is an expanded form analogous to $\tilde \beta \rightarrow Y = X\beta + \epsilon$
            \item If we have endogeneity problems (i.e. $CoV(X, \epsilon) \neq 0$)
        \end{itemize}
        \item Finally, we can talk about identification in the general IV setting, as in the simultaneous equation models setting:
        \begin{itemize}
            \item The model is \textbf{underidentified} if $p>q$ (fewer instruments) [Use the pigeonhold principle]
            \item The model is \textbf{just identified} if $p=q$. [In this setting, the Wald IV estimator works]
            \item The model is \textbf{overidentified} if $p < q$ and each $X_{ij}$ has at least one corresponding instrument $Z_{ij}$. [We have more ways than one to identify at least one of the endogenous variables)]
        \end{itemize}
    \end{itemize}
    \item Prediction vs estimation
    \begin{itemize}
        \item Given parameters, we estimate where to place a point. We don't care about the mechanism (it could be a black box for all we care)
        \item Estimation: goes above and beyond to produce a mechanism for prediction
    \end{itemize}
\end{itemize}
\textbf{NOTE: The following will not be on the final exam}

\section{A brief introduction to DiD (Difference-in-Differences) and SC (Synthetic Controls)}
Motivation
\begin{itemize}
    \item So far, we have looked at IV's as one solution to a specific version of omitted variable bias. An omitted variable bias (endogeneity problem, RECALL: $CoV(X,\epsilon) \neq \epsilon$) can also arise from \textbf{changes over time} if a treatment is not instantaneous (i.e. time-lag).
    \item We haven't covered dynamic problems at all before in this class. We want to now use those dynamics to get rid of our endogeneity problem
    \item Difference-in-Differences (DiD) allows us to obtain a causal effect under a strong main assumption
\end{itemize}
\subsection{Intermezzo: Rubin's counterfactual notation}
\begin{itemize}
    \item In causal inference, we want to answer questions of ``What if?"
    \item Let's say I ate a cookie
    \item Counterfactual: We care about a world where something doesn't happen (counter to actual fact)
    \begin{itemize}
        \item e.g. ``What if I had not eaten that cookie?"
    \end{itemize}
    \item Since we can't observe both outcomes, we have to impute the counterfactual effect (the treatment doesn't happen)
    \item Fundamental problem of Causal Inference (1986) Holland: We can only observe ONE world (the treatment world), not both, $Y(1)$ (treatment), $Y(0)$ (no treatment)
    \item \textbf{We can only observe one outcome of the two, so we impute the other unobservable state of the world}
    \item One way to deal with this: instrumental variable
    \item \textbf{How do we impute?}
    \begin{itemize}
        \item \textbf{Averages}
        \item Notation: Y is outcome whether treatment is taken, T is assigned treatment
        \item If there are a lot of people who have received a treatment, Y(1), and a lot who haven't (aka control group, Y(0)) we can take the average of their outcomes
        \item Problem: here might be confounders: $\epsilon$, stuff that affects both the treatment and the outcome. Even worse here, we don't assume a linear regression. The only thing we assume here is: \[E[Y_i | T_i = 1] - E[Y_i | T_i = 0] = 0 \]
        \[= (E[Y_i (1) | T_i = 1] - E[Y_i (0) | T_i = 1]) + (E[Y_i (0) |T_i = 1] - E[Y_i(0) |T_i=0])\]
         $\text{We add and subtract $E[Y_i (0) | T_i = 1]$}$
        \begin{itemize}
            \item We cannot observe $E[Y_i (0) | T_i = 1]$
            \item Incompliance: $E[Y_i (0) | T_i = 1]$. Randomness gets rid of it and makes it 0
            \item In other words: $E[Y_i (0) | T_i = 1] = 0$, means perfect compliance
        \end{itemize}
        \item The first term in the above expression is the average treatment effect (ATE) on the treated, i.e.:
        \[ ATT = E[Y_i (1)|T_i = 1] - E[Y_i (0) | T_i = 1]\]
        \item \textbf{ATT: average treatment effect on treated}. Synthetic controls can only give us this, which is worse than ATE (which randomization can create)
        \item $ATE = E[Y_i(1)] - E[Y_i(0)]$
        \item When can we do this?
        \begin{itemize}
            \item Randomization
            \item If we randomize irrespective of characteristics, we eliminate the effect of the confounder: $\epsilon$ CANNOT have an impact on treatment $T$ UNLESS there are compliance issues 
            \item $E[T_i \epsilon_i ] = 0$
            \item Treatment $T_i$ is a dummy variable
            \item $Y_i = \beta_1 + \beta_2T_i + \epsilon$
            \item Due to dummy variable property (0/1), we have two models now:
            \item $Y_i = \beta_1 + \beta_2 + \epsilon_i$
            \item $Y_i = \beta_1$
            \item Now, we can let $\pi_2 = \beta_1 + \beta_2 \Rightarrow E[Y_i | T_i = 1]$ (expected outcome of treatment), $\pi_0 = \beta_1, E[Y_i | T_i = 0]$. Recall, regressing an constant parameters gives the average
            \item $\sum_{not treated} Y_i = \hat \beta_1 \rightarrow \hat E[Y_i | T_i = 0]$
            \item $\sum_{treatment} Y_i = \hat \beta_2 + \hat \beta_1 \rightarrow \hat E[Y_i | T_i = 1]$
            \item $\hat \beta_2 = \frac{1}{treated}\sum_{treated} Y_i - \frac{1}{non-treated} \sum_{non-treated} Y_i$
            \item $\hat \beta_2 = E[Y_i (1) ] - E[Y_i (0)]$
        \end{itemize}
    \end{itemize}
\end{itemize}

\subsection{Difference In Differences}
\begin{itemize}
    \item We use this if we can't use randomization (it's costly), we use this when people can select into treatments they want
    \item Benefit: we don't need an instrument, we just need a treatment and control group
    \item How do we know we estimate the actual effect of a treatment over time and not just the natural trend (natural drift) that the outcome variable follows irrespective of any treatment?
    \item In order to isolate the true causal effect of a treatment, we need to account for the natural trend the treated group would have undergone had it not received treatment. Below is a picture
    \item We look at where time series diverge, time T, when treatment was assigned
    \item Recall: $E[Y_i (0) | T_i = 1]$ is unobservable
    \begin{itemize}
        \item Solution: We take the control group: $E[Y_i (0) | T_i = 0]$, to make the \textbf{Parallel trend assumption}
        \item Parallel trend assumption: Says that $E[Y_i(0) | T_i = 0] \leftarrow E[Y_i | T_i = 0] \Rightarrow E[Y_i (0)|T_i = 0], E[Y_i(0)|T_i = 1]$ are parallel
        \item In English, we take the control group and assume that it is parallel to the $E[Y_i (0) |T_i = 1]$, or incompliance group that was assigned the treatment but did not actually undergo it
        \item The difference-in-differences compares the change in trends to make a conclusion
    \end{itemize}
    \item We need to at least be sure that there are parallel trends before the treatment $T$
    \item We impute for ATT: what we care about (we can't get ATE from this)
    \item \textbf{Implementation of the DiD estimator via regression}
    \begin{itemize}
        \item The DiD estimator can be implemented by the regression that corresponds to the model \[Y_{it} = \alpha_0 + \alpha_1 Post_{jt} + \alpha_2 T_i + \alpha_3 T_i \cdot Post_{it} + u_{it},\]where $i=$ individual subscript, $t=$ time periods (pre- and post- period),\[Post_{it} = \begin{cases}
            1\text{ if }t\geq T\\0\text{ if }t < T
        \end{cases}\]and $T_i$ is the treatment indicator and all the $u_{it}$ are standard idiosyncratic error terms potentially with serial correlation and/or heteroskedasticity
    \end{itemize}
\end{itemize}

\section{Synthetic controls}
Synthetic controls (Alberto Abadie), what if we don't have/cannot assume parallel trends

\vspace{0.15in}
What if we don't see parallel trends between control and treatment groups before treatment?
\begin{itemize}
    \item We introduce synthetic controls
    \item If we don't have parallel trends, we construct them
    \begin{itemize}
        \item Example California Proposition 99 (which made it harder to sell cigarettes), and its effect on per-capita cigarette sales
        \item We know that randomization would normally get rid of endogeneity problems, but California selected itself into treatment (NO randomization)
        \begin{itemize}
            \item Let's try DiD. But two problems
            \begin{itemize}
                \item No parallel trends
                \item we have aggregate values (time series), no individual sampling
            \end{itemize}
        \end{itemize}
        \item Convert rest of the U.S. to a synthetic California control
    \end{itemize}
    \item We artificially parallelize the trends before treatment via synthetic controls
    \begin{itemize}
        \item This is done via optimization \textbf{NOT} regression
        \item We call it synthetic controls because have constructed a control group that does not exist
    \end{itemize}
    \item In our example,
    \begin{itemize}
        \item Find the data from each state at a point of time
        \item Find the largest convex set that contains all of our points. We project our point (e.g. real California) onto the \textbf{convex hull (see convex geometry, convex set definition}. Our new point is a synthetic California. In the case that California is in our hull already, then we can already perfectly replicate it
        \item The interior convex hull is represented by a weighted linear combination: $w_1 Y_1 + w_2 Y_2 + w_3 Y_3$, where $w_1, w_2, w_3 \geq 0$, $\sum_i {w_i} = 1$, where $Y_i$ is a vertex of our convex hull
        \item In our example, Each X represents in our example per-capital sales of cigarettes by a state at a certain point in time
        \item \textbf{Note:} our projection introduces \textbf{sparse weights}. Weights for all $Y_i$ except for the vertex of the plane we are projecting to are 0
    \end{itemize}
    \item The synthetic controls estimator
    \[\hat \alpha_{1t} = Y_{1t} - \sum_{j=2}^{J+1} \lambda_j^* Y_{jt} \]
    \item Obtaining the optimal weights $\lambda$
    \item Caratheodory's Theorem
    \begin{itemize}
        \item For each pre-treatment period $t\leq T_0$, obtain the time-dependent optimal weights $\lambda_t ^*$ by \[\lambda_t^* = \underset{\lambda \in \Delta^{J}}{argmin} ||Y_0 - \sum_{j=2}^{J+1} \lambda_{jt}Y_{jt}||^2\] (J is a `simplex') where \[\Delta^J = \left\{\lambda \in \R^J : \lambda_j \geq 0 \text{ and } \sum_{j=2}^{J+1} \lambda_j = 1\right\}\]
        \begin{itemize}
            \item Very similar to linear regression but is not a regression. Why?
            \begin{itemize}
                \item Constraint with $\Delta^J$
                \item Sum is in the interior
            \end{itemize}
        \end{itemize}
        \item Apply this all to compute global weight $\lambda^*$ that will modify our control group to form a new synthetic control group: \[\lambda^* = \frac{1}{T} \sum_{t=0}^{T^*} \lambda_t^*\]
    \end{itemize}
    \item Using the computed weight $\lambda^* = \frac{1}{T} \sum_{t=0}^{T^*} \lambda_t^*$, we parallelize the control group before treatment and use the computed weights to impute control states
    \item Imputation formula:\[\sum_j \lambda^* Y_{jt}\]
\end{itemize}

%%% FOOTER %%%
\input{footer}

\end{document}
