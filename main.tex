\documentclass[10pt, oneside]{article} 
\usepackage{amsmath, amsthm, amssymb, calrsfs, wasysym, verbatim, bbm, color, graphics, geometry, textcomp}

\geometry{tmargin=.75in, bmargin=.75in, lmargin=.75in, rmargin = .75in}  

\newcommand{\R}{\mathbb{R}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\Cdot}{\boldsymbol{\cdot}}
\newcommand{\iidsim}{\overset{\text{iid}}{\sim}}

\newtheorem{thm}{Theorem}
\newtheorem{defn}{Definition}
\newtheorem{conv}{Convention}
\newtheorem{rem}{Remark}
\newtheorem{lem}{Lemma}
\newtheorem{cor}{Corollary}

\usepackage[hidelinks]{hyperref}

\title{ECON 320}
\author{Alexander Liu}
\date{Fall 2024}

\begin{document}

\fontsize{12pt}{15pt}\selectfont

\maketitle
\tableofcontents

\vspace{.5in}

\section{Review}
\subsection{Introduction}
\begin{itemize}
    \item All lectures are self-contained (just have to follow lectures to do perfect in the class)
\end{itemize}
\subsection{What is a Random Variable?}
\begin{itemize}
    \item What's out of our control even if we control all the parameters of our problem.
    \item \textbf{A R.V. is a function mapping a sample space (domain: probability space) into real line $\R$ (range)}. In other words, the sample space is the input and real line is the range of outputs (domains/range from algebra)
    \begin{itemize}
        \item Lay explanation: unknown variable that takes in an input and produces an output
    \end{itemize}
    \item Real line: the range of values the R.V. can map to
    \item Two types of R.V.: Discrete and Continuous
    \begin{itemize}
         \item Discrete R.V.: X takes on one of a finite or countably infinite (ie. the integers) number of values
        \item Continuous R.V.: $P\{X=x\}=0, \forall x \in \R$
    \end{itemize}
\end{itemize}
\subsection{CDF (cumulative distribution function, $F_X(x)$) and PDF (probability density function, $f_X(x)$)}
\begin{itemize}
    \item Derivative of CDF is PDF ($f_X(x)=\frac{dF(x)}{dx})$, integral of PDF is CDF ($F_X(x)=\int^x_{-\infty} f_X(z)dz$)
    \item A pdf can have values \textbf{GREATER} than 1, but cdf \textbf{MUST} sum to $1$
    \item Properties of Probability Density Functions (pdf):
    \begin{itemize}
        \item $f_X(x)\geq 0,$ and either
        \item $\sum^n_{i=1}f_X(x_i)=1$ (discrete)
        \item $\int^{\infty}_{\-\infty} f_X(x)dx = 1$ (continuous)
    \end{itemize}
\end{itemize}
\subsection{Expected Values}
\begin{itemize}
    \item $E(X)=\int_{-\infty}^{\infty} xf(x)dx\equiv \mu_X$
    \item $E(g(X))=\int_{-\infty}^{\infty} g(x)f(x)dx$
    \item $E$ can be thought as an integral
    \begin{itemize}
        \item $E[f(x)] =\int_{-\infty}^{\infty} f(x)dx$
    \end{itemize}
    \item an Expectation can be thought of as linear
    \item $E[aX+bX] = aE[X] + bE[X]$
    \item \textbf{IMPORTANT}: $E[XY] \neq E[X]E[Y]$ \textbf{UNLESS THEY ARE INDEPENDENT}
    \item $E[X\varepsilon]$, $\varepsilon$ is a R.V. In this case, the expectation is a double integral (integrate with respect to X, and with respect to $\varepsilon$). So, $\iint x\cdot \varepsilon x    f(x,\varepsilon)dxd\varepsilon$, (joint function times joint density function)
    \item Properties of Expectation Operator
    \begin{itemize}
        \item $E[a] = a, \forall \text{ constants a}$
        \item $E[bX] = bE[X], \forall \text{ constants } b, R.V.\text{ }X$
        \item $E[g(X) + h(X)] = E[g(X)] + E[h(X)], \forall g(\cdot), h(\cdot) \Rightarrow E[a+bX]=a+bE[X]$
        \item Variance of a r.v. X (see derivation below): $var(X)=E(X^2)-\mu_X ^2$ 
        \item $Var(aX)=E[(aX)^2]-E([aX])^2=a^2 E[X^2]-a^2(E[X])^2$, where (a,b) are constants
    \end{itemize}
\end{itemize}
\subsection{Variance}
\begin{itemize}
    \item $\sigma^2 _X = var(X)=E[(X-\mu_X)^2]=E(X^2-2X\mu_X+\mu^2_X)=E(X^2)-2\mu_X E(X)+\mu^2 _X = E(X^2)-\mu^2_X$ (remember $-2\mu_X E[X] = -2\mu_X \cdot \mu_X\text{, since } E[X] = \mu_X$)
    \item Standard Deviation: $\sigma_X=\sqrt{\sigma^2 _X}=\sqrt{var(X)}$
    \item $var(a+bX) =b^2var(X)$, where a,b are constants
    \begin{itemize}
        \item $Var(aX)=E[(aX)^2]-E([aX])^2=a^2 E[X^2]-a^2(E[X])^2$
    \end{itemize}
\end{itemize}
\subsection{Bivariate/Multivariate R.V's}
\begin{itemize}
    \item X,Y have the joint cdf $F_{X,Y}(x,y)=P(X\leq x, Y\leq y)=\int^{x}_{-\infty}\int^{y}_{-\infty} f_{X,Y} (u,v) dvdu$
    \item \textbf{Marginal cdf}: $F_X(x) =P(X\leq x)=\int _{-\infty} ^ x\int^{\infty}_{-\infty} f_{X,Y}(u,v)dvdu\equiv \int^{x}_{-\infty f_X(u)du}$
    \item \textbf{Marginal pdf}: $f_X(x) =P(X\leq x)=\int _{-\infty} ^ {\infty} f_{X,Y}(u,v)dv$. In other words, take integral (a slice) along one axis
    \item \textbf{Independence:} X and Y are independent iff
    \begin{itemize}
        \item $f_X,Y (x,y)=f_X(x) f_Y(y), $ so \\
        $P(X\leq x, Y\leq y) = \int^{x}_{-\infty}\int^{y}_{-\infty} f_X(u)f_Y(v) dvdu=P(X\leq x)P(Y\leq y)$
    \end{itemize}
    \item \textbf{Conditional pdf}: $\frac{\text{joint distribution}}{\text{marginal distribution}}$
    \begin{itemize}
        \item Conditional pdf of $Y$ given $X=x:f_{Y|X}(y|x) =\frac{f_{X,Y} (x,y)}{f_X (x)}, [=\frac{\text{joint}}{\text{marginal}}]$
        \item \textbf{Conditional pdf and independence}: Y and X are independent iff $f_{Y|X} (y|x) =f_Y (y)$
        \item \textbf{Covariance:} $cov(X,Y)=E[(X-\mu_X)(Y-\mu_Y)]=E(XY)-\mu_X\mu_Y$
    \end{itemize}
\end{itemize}
\subsection{Covariance}
\begin{itemize}
    \item $cov(X,Y)=E[(X-\mu_X)(Y-\mu_Y)]=E(XY)-\mu_X\mu_Y$ $(a^2-b^2)$
\end{itemize}

\subsection{General Rules for Means, Variances, and Covariances}
\begin{itemize}
    \item 1) $E(a+bX+bY+dZ+...)=a+bE[X]+cE[Y]+dE[Z]+...$
    \item 2) $cov(a+bX, c+dY)=bdcov(X,Y)$
    \item 3) Important (like binomial formula $(a+b)^2$): $var(a+bX+cY)=b^2 var(X)+ c^2 var(Y)+2bccov(X,Y)$
    \item $X$ and $Y$ are independent $\Rightarrow cov(X,Y)=0$ [Note: Converse is not true]
\end{itemize}
\subsection{Standardized Variables and Correlation}
\begin{itemize}
    \item treat the bar as indicating standardized variables here
    \item Let $\bar{X} = \frac{X-\mu_X}{\sigma_x}$ and $\bar{Y}
    =\frac{Y-\mu_Y}{\sigma_Y}$ $\leftarrow$ like Normalization layer in Transformer architecture, containing range
    \item \textbf{CHECK WITH PROF}: $E(\bar{X})=0=E(\bar{Y})$
    \item Why?
    \begin{itemize}
        \item $E(\bar X) = E[\frac{X-\mu_X}{\sigma_X}]=\frac{1}{\sigma_X}E[X-\mu_X]=\frac{1}{\sigma_X} E[X] -\mu_X=\frac{1}{\sigma_X}(\mu_X-\mu_X)=0$
        \item $var(\bar X) = \sigma ^{-2} var(X)=1=var(\bar Y)$
    \end{itemize}
    \item $var(\bar X) = 1= var(\bar Y)$ \textbf{check if X is supposed to be barred}
    \item Why?
    \begin{itemize}
        \item $var(\bar X)=\sigma^{-2} var(X-\mu_X) \text{ (factoring out constant from var() squares it)}=\sigma^{-2} var(\bar X)= \sigma^{-2} \times \sigma^2 = 1 = var(\bar Y)$
    \end{itemize}
    \item The \textbf{Correlation} between X and Y is $\rho_{X,Y}=\frac{cov(X,Y)}{\sigma_X \sigma_Y} = cov(\bar X, \bar Y)$. 
    \begin{itemize}
        \item Correlation is the covariance between standardized variables. Since mean doesn't matter here (constants removed in cov()), we only need to standardize with $\sigma$
    \end{itemize}
\end{itemize}
\subsection{Normal Distribution (Gaussian)}
\begin{itemize}
    \item $X \sim N(\mu,\sigma ^2)$
    \begin{itemize}
        \item Standard Normal: $X\sim N(0,1)$
    \end{itemize}
    \item $f_X(x)=\frac{1}{\sqrt{2\pi\sigma^2}}exp\{-\frac{1}{2}(\frac{x-\mu}{\sigma})^2\}$
\end{itemize}
\subsection{Useful Results for Gaussian}
\begin{itemize}
    \item 4) Normal Distribution / Chi-Square = t distribution
\end{itemize}


End first-day end page 3. Useful Results for Gaussian incomplete

\subsection{Convergence in Probability}
\begin{itemize}
    \item Supposed $Y_1, Y_2,...$ is a sequence of random variables. This sequence converges to a number $b$ if the probability distribution of $Y_n$ becomes more and more concentrated around $b$ as $n\rightarrow \infty$. $lim_{n\rightarrow \infty} Y_n = b$
    \begin{itemize}
        \item Formally, The sequence $\{Y_n\} \text{ converges in probability to } b \text{ if }\forall \epsilon > 0, P(|Y_n-b| < \epsilon) \rightarrow 1 \text{ as } n\rightarrow \infty$
    \end{itemize}
\end{itemize}

\subsection{Law of Large Numbers}
\begin{itemize}
    \item The sample mean of R.V. X approaches population mean $\mu = E[X_i]$ as sample size $n$ increases
    \item Formally, suppose $X_1, ..., X_n$ form a random sample form a distribution for which the mean is $\mu =E(X_i)$, and let $\bar X_n $ denote the sample mean. Then $plim \bar X_n = E(X_i)$
\end{itemize}

\subsection{Central Limit Theorem}
\begin{itemize}
    \item Formally, suppose $X_1, ..., X_n$ denotes n independently and identically distributed (iid) random variables with the same pdf with mean $\mu$ and variance $\sigma^2$. As $n\rightarrow \infty, \bar X_n \sim N(\mu, \frac{\sigma^2}{n})$
    \item So, sample mean $\bar X_n $ approaches the normal distribution with mean $\mu$ and variance $\frac{\sigma^2}{n}$, regardless of the form of the pdf of the $X_i$'s, (so even when it is NOT normal)
\end{itemize}

\section{Estimators and the Linear Model}

\subsection{Estimators}
\begin{itemize}
    \item We need to find parameters such as $\mu$ or $\sigma^2$
    \item An estimator is a mathematical expression that approximates the values of these parameters in terms of available observations
    \item Important: The estimator is a R.V., since it is a function of other random variables
\end{itemize}

\subsection{Criteria of Estimators}
\begin{itemize}
    \item An estimator $\hat \mu$ (hat indicates estimator) of $\mu$ is \textbf{unbiased} if $E(\hat \mu) = \mu$
    \item An estimator is \textbf{efficient} if $var(\hat \mu) \leq var(\tilde{\mu})\text{ } \forall \text{ unbiased estimators } \tilde \mu$. Let $\sim$ signify unbiased estimator
    \item Think of ``bias" as being related to mean and ``efficient" as being related to variance
    \item \textbf{Mean Square Error} of $\hat mu$ is $MSE(\hat \mu)\equiv E[(\hat \mu -\mu)^2]$
    \begin{itemize}
        \item $MSE(\hat \mu) \equiv (E\hat \mu -\mu)^2 + var(\hat \mu) = [Bias(\hat \mu)]^2 + var(\hat \mu)$
    \end{itemize}
    \item Estimators discussed so far don't concern the size of the sample. But some may prefer large samples (can shrink bias and/or variance). In such cases, estimators may be judged on its asymptotic properties - properties in very large samples
\end{itemize}

\subsection{The Simple Linear Model}
\begin{itemize}
    \item $Y_i =\beta_1 + \beta_2 X_i + \epsilon_i$
    \begin{itemize}
        \item We want to find a line that minimizes $\sum \epsilon_i$ (line of ``best fit")
        \item We assume a line that is supplemented by our assumption. Only $X_i$ is observed
        \item Only thing that is ``random" by nature is $\epsilon_i$
        \begin{itemize}
            \item In other words: $Y=f(\epsilon_i)$. NEVER prove anything with $Y_i$. ALWAYS plug in the formula and work with $\epsilon_i$, since it's the only thing we have to address due to its randomness
        \end{itemize}
    \end{itemize}
\end{itemize}

\subsection{Basic Assumptions about Linear Model}
\begin{itemize}
    \item Homoskedasticity: $E(\epsilon_i^2) = var(\epsilon_i) = \sigma^2 \forall i$. The dispersion of each data point remains constant
    \item No Serial Correlation: $E(\epsilon_i \epsilon_j) = 0$ if $i\neq j$. No relationship between data
\end{itemize}

\subsection{Some Implications of Basic Assumptions}
\begin{itemize}
    \item Always focus on reducing $Y_i$ to $\epsilon_i$
    \item $E(Y_i)=E(\beta_1 + \beta_2 X_i + \epsilon_i) =\beta_1 + \beta_2 X_i + E(\epsilon_i) =\beta_1+\beta_2 X_i$. Expectation of $\beta_1 + \beta_2 X_i$ is itself since it is nonrandom. $E[\epsilon_i]=0$
    \item $varY_i = E[(Y_i -EY_i)^2] = E[(\beta_1+\beta-2 X_i + \epsilon_i -\beta_1 -\beta_2 X_i)^2]=E(\epsilon_i ^2) \textbf{(homoskedasticity)}= \sigma ^2$
    \item $cov(Y_i, Y_j) = 0, \forall i \neq j$
\end{itemize}

EPSILON IS AN ASSUMPTION AND IS \textbf{NEVER} OBSERVED. ALL WE KNOW IS $E[\epsilon_i] = 0$

\subsection{Estimation of the regression coefficients $\beta_0, \beta_2$}
\begin{itemize}
    \item How do we choose the ``line of best fit"? Broadly speaking, minimize error, but what is error? Also broadly speaking, it is an estimator of $\epsilon_i$, which we prefer to minimize.
    \begin{itemize}
        \item Estimator for $\epsilon$, $\hat \epsilon_i$
        \item Minimize the squares of the $\epsilon_i$
        \item \textbf{IMPORTANT: $min_{\beta_1, \beta_2} \sum _{i} \hat \epsilon_i ^2  $}
        \begin{itemize}
            \item $\sum _{i} \hat \epsilon_i ^2 =\sum_i (Y_i -\beta_1 -\beta_2 X_i)^2$. In English, we plug in our regression guess at each iteration
        \end{itemize}
        Algorithmic approach (we arrive at a closed-form solution later)
        \begin{itemize}
            \item Start with initial guess, then assign estimators $\hat \epsilon_1$ for first observation, and so on. Square each estimator
            \item Why square it?
            \begin{itemize}
                \item MAD (mean absolute deviation) $\hat \epsilon_i$
                \item Above is bad due to reaching zero gradient (similar to vanishing gradient problem)
                \item Convex function (parabolic matching $x^2$), so if it is 0, we know it's the minimum
            \end{itemize}
        \end{itemize}
    \end{itemize}
    \item 3 possibilities (for the time being)
    \begin{itemize}
        \item $min_{\beta_1, \beta_2} \sum ^n _{i=1} (Y_i - \beta_1 -\beta_2 X_i)^2$ (``Least Squares", aka ``Ordinary Least Squares" estimators $\hat \beta_1, \hat \beta_2$ amounts to choosing the line that minimizes the sum of squared deviations. \textbf{We use this})
        \item $min_{\beta_1, \beta_2} \sum ^n _{i=1} |Y_i - \beta_1 -\beta_2 X_i|$, ``Least Absolute Deviations" aka ``mean absolute deviation" (MAD) estimators $\tilde \beta_1, \tilde \beta_2$
        \item $min_{\beta_1, \beta_2} \sum ^n _{i=1} (X_i + \frac{\beta_1}{\beta_2} - \frac{Y_i}{\beta_2})^2$, ``Reverse Least Squares" estimators $\bar \beta_1, \bar \beta_2$
    \end{itemize}
\end{itemize}

\section{Obtaining Our Linear Estimators (the betas)}

\subsection{Ordinary Least Squares - Gauss-Markov Theorem}
\begin{itemize}
    \item Follows the following assumptions
    \item $\hat \beta_1, \hat \beta_2$ are BLUE (best linear unbiased estimators, where best means most efficient) under Basic Assumptions
    \item If the errors are normal, $\hat \beta_1, \hat \beta_2$ are MVUE (minimum variance unbiased estimators) (even better than BLUE, since it considers all, not just linear-unbiased estimators)
    \item $\hat \beta_1, \hat \beta_2$ are consistent, asymptotically normal, and, under normality of the error term, asymptotically efficient
\end{itemize}

We often test $\beta_2 = 0$. Papers want a non-zero slope ($\beta_2$) because it means there may be some predicted relationship

\subsection{Derivation of OLS Estimators}
\begin{itemize}
    \item We don't need gradient descent because we have an easier way to get OLS estimators $\beta_1$ and $\beta_2$
    \item Approach
    \begin{itemize}
        \item Minimize sum of squared residuals: $min_{\beta_1, \beta_2} \sum ^n _{i=1} (Y_i - \beta_1 -\beta_2 X_i)^2$
        \begin{itemize}
            \item Comes from $\epsilon =Y_i - \beta_1 -\beta_2 X_i,$ so minimize $\sum \epsilon^2$
        \end{itemize}
        \item Take first derivative (partial derivatives with respect to both betas, remember chain rule for $\beta_2$) and set it to 0
        \item $\beta_1: \frac{\partial S}{\partial \beta_1} = \sum_{i=1} ^ n -2(Y_i -\hat \beta_1 -\hat \beta_2 X_i) = 0$
        \item $\beta_2: \frac{\partial S}{\partial \beta_2} = \sum_{i=1} ^ n -2X_i(Y_i -\hat \beta_1 -\hat \beta_2 X_i) = 0$
        \item Why the hats?
        \begin{itemize}
            \item Philosophical: We get numbers that will be our estimators. In practice, not strictly necessary.
        \end{itemize}
        \item We can condense the partial derivatives to be even simpler
        \item We call the 2 equations we have to solve for $\hat \beta_1, \hat \beta_2$ the ``normal equations". Rewriting and letting $\hat \epsilon_i=Y_i -\hat \beta_1 -\hat \beta_2 X_i \text{ (and diving both sides by constant -2)},$ we have
        \begin{itemize}
            \item $\frac{1}{n}\sum _{i=1} ^ n \hat \epsilon_i = 0$
            \item $\frac{1}{n} \sum_{i=1} ^ n X_i \hat \epsilon_i =0$
            \begin{itemize}
                \item $E[X_i \epsilon_i] = 0, E[\epsilon_i] = 0$. Law of large numbers, converges
            \end{itemize}
        \end{itemize}
        \item What's the intuition behind the normal equations (part of Gauss Markov-Assumptions)?
        \begin{itemize}
            \item $\hat y_i = \hat \beta_1 + \hat \beta_2 X_i$. Why no $\epsilon$? Our $\hat y_i$ is a prediction. $\hat \epsilon_i = y_i - \hat y_i$
            \item We cancel out the -2, since the other side is 0.
            \item Why $\frac{1}{n}$? Same as before, just a notational thing averaging the residuals.
            \item We call estimators of error terms $\hat \epsilon_i$ residuals
        \end{itemize}
    \end{itemize}
\end{itemize}

\subsection{Closed Form Solution to get $\hat \beta_2$ and $\hat \beta_1$}
\begin{itemize}
    \item \textbf{SUPER IMPORTANT. MOST IMPORTANT FORMULAS OF THE CLASS}
    \item Solve for $\hat \beta_2$ first, then plug back in for $\hat \beta_1$
    \item $\hat \beta_2 = \frac{\frac{1}{n} \sum(X_i - \bar X) (Y_i - \bar Y)}{\frac{1}{n} \sum (X_i - \bar X)^2} = \frac{\hat \sigma_{XY}}{\hat \sigma^2 _X} = r_{XY} \frac{\hat \sigma_Y}{\hat \sigma_X}$. $\hat \beta_2 = \frac{cov(Y,X)}{var(X)}$
    \begin{itemize}
        \item Empirical covariance over squared variance of x
        \item $r_{XY}$ is the correlation coefficient, where $r_{XY}=\hat \rho_{XY}$ and since \\$\rho_{XY}=\frac{\sigma_{XY}}{\sigma_X \sigma_Y}, \sigma_{XY}=\rho_{XY}\sigma_X \sigma_Y$
    \end{itemize}
    \item $\hat \beta_1 = \bar Y - \hat \beta_2 \bar X$. 

\end{itemize}

To show minimum (minimum squared errors $S$), take second-order partial derivative of $\sum \epsilon^2$ and show it's zero (indicates local min/max)

\section{Proving the Validity of the Linear Model: Gauss-Markov (BLUE)}

\subsection{Linearity of $\hat \beta_1, \hat \beta_2$}
\begin{itemize}
    \item Can write $\hat \beta_1=\sum_{i=1} ^n a_i Y_i$ and $\hat \beta_2 =\sum_{i=1} ^n b_iY_i$
    \begin{itemize}
        \item Where $b_i = \frac{\frac{1}{n} (X_i-\bar X)}{\frac{1}{n} \sum(X_i-\bar X)^2}$ and $a_i = \frac{1}{n} - b_i \bar X$
        \begin{itemize}
            \item $\hat \beta_2 = \frac{\sum_i (Y_i -\bar Y) (X_i -\bar X)}{\sum_i (X_i -\bar X)^2}=\text{(FOIL $\rightarrow$)} \frac{\sum_i Y_i (X_i -\bar X)-\sum_i \bar Y (X_i -\bar X)}{\sum_i (X_i -\bar X)^2}=\frac{\sum_i Y_i (X_i-\bar X)}{\sum _i (X_i - \bar X)^2} - \frac{\sum_i \bar Y (X_i-\bar X)}{\sum _i (X_i - \bar X)^2}$ \\
            $\sum(X_i -\bar X)$ is zero (the denominator is not because it is sum of squared differences), so $\frac{\sum_i Y_i (X_i-\bar X)}{\sum _i (X_i - \bar X)^2} = 0$, we can pull out $\bar Y$ from the summation in the second fraction. At this point we notice the denominator is variance (a \textbf{constant}). Factor out $Y_i$ so we can find a constant linear coefficient to prove linearity.
            \item $\sum_i \frac{1}{\sum_j (X_i - \bar X)^2} Y_i (X_i -\bar X) =\sum_i Y_i \frac{(X_i -\bar X)}{\sum_i (X_i -\bar X)^2}$. Isolate $Y_i$ from this, as $\hat \beta_2 = b_i Y_i$. Hence we prove that $\hat \beta_2$ and linear regression maintains a linear relationship with y values. \textbf{In other words, we prove that $\hat \beta_2$ is a linear estimator}
        \end{itemize}
    \end{itemize}
\end{itemize}

\subsection{Unbiasedness of OLS Estimators}
\begin{itemize}
    \item How do we prove unbiasedness? $E[\epsilon_i]=0$. 
    \item Step 1: Setup $E[\hat \beta_2 ]$. To do this, lets work backwards by plugging in the estimator $E[\hat \beta_2]=E[\frac{\sum_i (Y_i -\bar Y) (X_i -\bar X)}{\sum_i (X_i -\bar X)^2}]$. We need to make the epsilon show up 
    \item Step 2: Plug in the model for $Y_i, \bar Y$. Substitute $Y_i=\beta_1 + \beta_2 X_i + \epsilon_i$ and $\bar Y=\beta_1 +\beta_2 \bar X + \bar \epsilon$ into equations to add $\hat \beta_1, \hat \beta_2$ by expanding $(Y_i -\bar Y)$ \\
    $=E[\frac{\frac{1}{n} 
    \sum_{i=1} ^ n(\beta_1 + \beta_2 X_i + \epsilon_i - \beta_1 - \beta_2 \bar X -\bar \epsilon)(X_i -\bar X)}{\hat \sigma_X ^2}]$. $\beta_1$ is canceled out, can pull out $\beta_2$
    \item $E[\frac{\frac{1}{n} \sum_{i=1} ^ n (\beta_2 (X_i -\bar X) + \epsilon_i -\bar \epsilon)(X_i -\bar X)}{\hat \sigma_X ^2}]$
    \item $=E[\frac{\frac{1}{n} \sum_{i=1}^n \beta_2 (X_i - \bar X)^2 + (\epsilon_i -\bar \epsilon) (X_i -\bar X)}{\hat \sigma_X ^2}]$. Separate into two expectations in next step
    \item $=E[\frac{\frac{1}{n}\sum_{i=1} ^n \beta_2 (X_i -\bar X)^2}{\frac{1}{n} \sum_{i=1} ^n (X_i - \bar X)^2}] + E[ \frac{\frac{1}{n}\sum_{i=1} ^ n (X_i -\bar X) (\epsilon_i -\bar \epsilon)}{\frac{1}{n} \sum_i(X_i -\bar X)^2}]$
    \item $=E[\beta_2] + E[ \frac{\frac{1}{n}\sum_{i=1} ^ n (X_i -\bar X) (\epsilon_i -\bar \epsilon)}{\frac{1}{n} \sum_i(X_i -\bar X)^2}]$. We can take out $\beta_2$ out of expectation since it's a constant. \textbf{To show unbiasedness, we must show that the second expectation is 0}.
    \item If unbiased, $E[\hat \beta_2] = \beta_2$
\end{itemize}

IF we show that estimators are both linear, unbiased, and also GLUE, we KNOW that they are the BEST estimators we can have

Gauss-Markov BLUE: Best Linear Unbiased Estimators
\subsection{Variance}
Context
\begin{itemize}
    \item $E[\epsilon_i] =0$. Unbiasedness is a first moment measure. We just have to show this for unbiasedness.
    \item $Y_i = \beta_1 + \beta_2 X_i +\epsilon_i$. $\hat \beta_2 =f(\epsilon_i)$.
    \item Apply this to variance: $var(\hat \beta_2) = var(\epsilon_i)$
    \item General form: $Var(Z)=E[(Z-E[Z])^2]$
\end{itemize}

Variance of $\hat \beta_2: var(\hat \beta_2) = E[(\hat \beta_2 - \beta_2 )^2] \rightarrow var(\hat \beta_2) = E[(\frac{\sum (X_i - \bar X) (\epsilon_i - \bar \epsilon)}{\sum (X_i - \bar X)^2})^2]=\frac{E([\sum(X_i - \bar X) (\epsilon_i -\bar \epsilon)]^2)}{[\sum(X_i -\bar X)^2]^2}$. $E[\hat \beta_2 ] = \beta_2$
\begin{itemize}
    \item $E[\frac{1}{(\sum_i (X_i -\bar X)^2 )^2} \cdot (\sum(X_i-\bar X)( \epsilon_i -\bar \epsilon)]$ Can pull out first part (variance, constant) from expectation
    \item $var(\hat \beta_2) = var(\hat \beta_2 - \beta_2)$, since shifting by a constant doesn't affect variance. We have constant $\beta_2$ here to start cancelling things out.
    \item Next step: plug in $\hat \beta_2$. $var(\frac{\sum_i (Y_i - \bar Y)(X_i - \bar X)}{\sum_i (X_i -\bar X)^2} - \beta_2)=var(\frac{\sum_i (\beta_1 + \beta_2 X_i + \epsilon_i -\beta_1 -\beta_2 \bar X-\bar \epsilon) (X_i -\bar X)}{\sum_i (X_i -\bar X)^2} - \beta_2)$
    \item $=var(\frac{\sum_i [\beta_2(X_i -\bar X) + (\epsilon_i -\bar \epsilon)] (X_i -\bar X)}{\sum_i (X_i -\bar X)^2} -\beta_2)\leftarrow $ (FOIL, distribute $(X_i -\bar X)$)
    \item $=Var(\frac{\beta_2 \sum_i (X_i-\bar X)^2}{\sum_i (X_i -\bar X)^2} + \frac{\sum_i (\epsilon_i -\bar \epsilon)(X_i -\bar X)}{\sum_i (X_i -\bar X)^2} - \beta_2)$. Now we get $\beta_2 -\beta_2$ which cancels, now we're left with:
    \item $var(\frac{\sum_i (\epsilon_i -\bar \epsilon)(X_i -\bar X)}{\sum_i (X_i -\bar X)^2})$. Move denominator out (constant)
    \item $\frac{1}{\sum_i (X_i -\bar X)^2} var(\sum_i (\epsilon_i -\bar \epsilon)(X_i -\bar X))$
    \item Apply Gauss Markov assumptions (unbiasedness: E[$\epsilon_i] = 0$. Homoskedascity (no serial correlation): $E[\epsilon_i ^2] = \sigma^2, E[\epsilon_i \epsilon_j] = 0$]). These assumptions allows us to pull out variance.
    \begin{itemize}
        \item If $E[\epsilon_i \epsilon_j] = 0$ then Var($\sum_i \cdots) = \sum_i var(\cdots)$
        \item $var(a\cdot \epsilon_i) = a^2 var(\epsilon_i)$
        \item $var(\sum_i \epsilon_i)=\sum_i var(\epsilon_i)$ only if $\epsilon_i$ are uncorrelated. In this class they will always be uncorrelated $=\frac{\sigma^2}{\sum_i (X_i -\bar X)^2}$
    \end{itemize}
    \item $\frac{1}{\sum_i (X_i -\bar X)^2}\sum_i var( (\epsilon_i -\bar \epsilon)(X_i -\bar X))$. We can pull out $\epsilon$ as constant from $\bar \epsilon(X_i - \bar X)$. Recall $\sum_i (X_i - \bar X)=0$. Thus remove the term and simplify
    \item $\frac{1}{\sum_i (X_i -\bar X)^2}\sum_i var( (\epsilon_i)(X_i -\bar X))$.
    \item $\frac{1}{(\sum_i (X_i -\bar X)^2)^2} \sum_i (X_i -\bar X) ^2 var(\epsilon_i)$
    \begin{itemize}
        \item $var(\epsilon_i) = E[\epsilon_i^2] - (E[\epsilon_i])^2 = \sigma^2 - 0 =\sigma^2$
    \end{itemize}
    \item Intuitively. Variance of $\beta_2$ is variance of our $\epsilon$ divided by variance of data points
\end{itemize}

\subsection{Variance Summary}
\begin{itemize}
    \item $var(\hat \beta_2) = \frac{\sigma^2}{n} \left[\frac{1}{\hat \sigma^2 _X}\right]$
    \item $var(\hat \beta_1 ) = \frac{\sigma^2}{n}\left[1+\frac{\bar X^2}{\hat \sigma^2 _X}\right]$
    $cov(\hat \beta_1 ,\hat \beta_2) = \frac{\sigma^2}{n}\left[\frac{-\bar X}{\hat \sigma^2 _X}\right]$
    \item $\sigma^2$: represents variance of $\epsilon$'s
    Empirical variance: $\hat \sigma^2_X  = \frac{1}{n} \sum_i (X_i -\bar X)^2$
    \end{itemize}

\subsection{Variance Remarks}
\begin{itemize}
    \item Larger $\sigma^2 = E(\epsilon_i ^2) \rightarrow $ Larger $var(\hat \beta_1), var(\hat \beta_2)$
    \item Larger $\hat \sigma^2 _X \rightarrow $ Smaller $var(\hat \beta_1), var(\hat \beta_2)$
    \item $var(\hat \beta_1) \geq var(\bar \epsilon)$, equal iff$cov(\hat \beta_1, \hat \beta_2) =0$
    \item Larger $n\rightarrow $ Smaller $var(\bar \beta_1), var(\hat \beta_2)$ (consistency, Law of Large Numbers)
\end{itemize}

$\hat \sigma_X ^2$: empirical variance. $=\frac{\sigma^2}{\sum_I (X_i -\bar X)^2}$

\subsection{Covariance between $\hat \beta_1$ and $\hat \beta_2$}
$cov(z,w) = E[(z-E[z])(w-E[w]))$

\begin{itemize}
    \item Because betas are unbiased: $cov(\hat \beta_1,\hat \beta_2)=E[(\hat \beta_1 -\beta_1)(\hat \beta_2 -\beta_2)$. Remember: the betas being unbiased is a necessary condition for this
    \item The covariance should be correlated with $\epsilon$ since it affects both
\end{itemize}

How to (intuitively) know whether no heteroskedascity is a good assumption
\begin{itemize}
    \item Check dispersion of $\epsilon$ around our line. If the dispersion changes, then we have an issue
\end{itemize}

\subsection{Estimation of $\sigma^2$}
$E[\epsilon_i] ^2 = \sigma^2$. We hope that this good expectation is a good representation of the average: $\frac{1}{n} \sum_i \hat \epsilon_i ^2$. $\hat e_i \rightarrow \hat \epsilon_i$
\begin{itemize}
    \item When $\sigma^2$ is unknown
    \item $s^2 \equiv \hat \sigma^2 = \frac{1}{n-2}\sum \hat e_i ^2 = \frac{1}{n-2} \sum(Y_i -\hat \beta_1 -\hat \beta_2 X_i)^2$ (n-2 degrees of freedom, because $\hat \beta_1, \hat \beta_2$)
    \item \textbf{It can be shown that ????????}
\end{itemize}

\subsection{Gauss-Markov Theorem}
When we have betas with hats, we assume they are outputs for OLS regression.
$(\hat \beta_1, \hat \beta_2)$ are BLUE (Best Linear Unbiased Estimators) of ($\beta_1, \beta_2$)
\begin{itemize}
    \item That is, for any $k_1$ and $k_2 \in \R$, if $\tilde \beta_1$ and $\tilde \beta_2$ are any linear unbiased estimators of $(\beta_1,\beta_2)$
    \begin{itemize}
        \item $var(k_1\hat \beta_1 + k_2 \hat \beta_2) \leq var(k_1 \tilde \beta_1 + k_2 \tilde \beta_2)$ (minimize variance)
    \end{itemize}
\end{itemize}

\subsection{Gauss-Markov Assumptions}
{Assumptions from Unbiasedness}
\begin{itemize}
    \item $E[\epsilon_i] = 0$
    \item $E[\hat \beta_1] = \beta_1$
    \item $E[\hat \beta_2] = \beta_2$
\end{itemize}
{No serial correlation/homoskedascity}
\begin{itemize}
    \item $E[\epsilon_i\epsilon_j] = 0 \leftarrow $ no serial correlation
    \item $E[\epsilon_i^2] = \sigma^2 \leftarrow $ homoskedascity (constant variance)
\end{itemize}
The Gauss-Markov Assumptions are weak
\begin{itemize}
    \item Gauss-Markov only cares about distributions where mean = 0 and any variance (not type of variation). If these conditions apply we can throw Gauss-Markov assumptions at it.
\end{itemize}

\subsection{How good is the fit? (Introducing $R^2$)}
One measure: $RSS\equiv \sum(Y_i-\hat Y_i)^2 = \sum(Y_i -\hat \beta_1 -\hat \beta_2X_i)^2 = \sum \hat e_i^2$
\begin{itemize}
    \item RSS is the ``Residual Sum of Squares"
    \item Problem with RSS: It depends on units - if $(Y_i, X_i)$ all multiplied by some (scalar) $K$, RSS multiplied by $K^2$. It quadratically blows up values, needs to be normalized. TSS can accomplish this. (``invariant to scale")
\end{itemize}
Difference between RSS and TSS: $\hat Y$ vs $\bar Y$.
\begin{itemize}
    \item The RSS is what you \textbf{CANNOT} explain with your model. It contains the residuals
    \begin{itemize}
        \item $ESS$ (``Explained Sum of Squares") = $TSS-RSS$
    \end{itemize}
    \item TSS does not care about X values (intuitively think that all Y points are projected onto Y-axis and differences are summers and squared)
\end{itemize}

Better measure: normalize measure
\begin{itemize}
\item Let $TSS=\sum (Y_i -\bar Y)^2 \equiv n\hat \sigma^2 _Y,$ where TSS is the ``Total Sum of Squares", then:
    \item $R^2\equiv 1-\frac{RSS}{TSS}$ measures ``goodness of fit"
    \item $R^2$ is intuitively a value that tells you how good your linear assumption is
    \begin{itemize}
        \item From a minimization perspective, $\hat \beta = argmin_\beta, $ where we minimize $R^2$, the ``Residual Sum of Squares"
    \end{itemize}
\end{itemize}

\subsection{Variance Decomposition: (Why $R^2$ is reasonable)}
RECALL: Normal Equations (Derivation of OLS Estimators)
\begin{itemize}
    \item $\frac{1}{n} \sum_{i=1} ^n \hat \epsilon_i = 0$. By construction, the sum of residuals after we do OLS is 0
    \begin{itemize}
        \item The sum of squares of residuals are minimized by OLS. If we get the residuals (NOT SQUARED) and sum them, the total will be 0
    \end{itemize}
    \item $\frac{1}{n}\sum_i X_i \hat \epsilon_i = 0$
\end{itemize}

\begin{itemize}
    \item $TSS=\sum_i (Y_i -\bar Y) = \text{add and subtract $\hat Y_i$}\sum(Y_i-\hat Y_i + \hat Y_i -\hat Y)^2 \text{ \textbf{(FOIL)} }= $\\
    $=\sum(Y_i - \hat Y_i)^2 + \sum(\hat Y_i -\bar Y)^2 + 2\sum(Y_i -\hat Y_i) (\hat Y_i -\bar Y)$ (\textbf{LAST TERM: } $2\sum(Y_i -\hat Y_i) (\hat Y_i -\bar Y)$ is gone because of 2nd normal equation)
    \begin{itemize}
        \item Decompose this to $TSS =\sum(Y_i - \hat Y_i)^2 + \sum(\hat Y_i -\bar Y)^2$, which represents $TSS = RSS + ESS$
    \end{itemize}
    \item But $\sum(Y_i -\hat Y_i) (\hat Y_i)-\bar Y)=\sum \hat e_i (\hat Y_i -\hat Y)=$ (\textbf{Plug in} $\hat Y_i = \hat \beta_1 + \hat \beta_2 X_i$ \textbf{and} $\bar Y = \hat \beta_1 + \hat \beta_2 \bar X$)\\
    $=\sum \hat e_i (\hat \beta_1 + \hat \beta_2 X_i - \bar Y)=$\\
    $=\sum \hat e_i[\hat \beta_2 (X_i -\bar X)]$ since $\hat \beta_1 =\bar Y-\hat \beta_2 \bar X$\\
    $=\hat \beta_2 \{\sum\hat e_i X_i -\bar X\sum\hat e_i\}=$\\
    $=0$ by the normal equations
\end{itemize}

Thus $TSS = \sum - \sum(Y_i -\hat Y_i)^2 + \sum(\hat Y_i -\bar Y)^2 = RSS+ESS$\\
where ESS is the ``Explained Sum of Squares", as explained by the regression, while the RSS, or ``Residual Sum of Squares" is the variation in Y that is not explained by the regression. 

[Total Sum of Squares = Residual Sum of Squares + Explained Sum of Squares]

Thus
$$R^2 = 1 -\frac{RSS}{TSS} = \frac{TSS-RSS}{TSS}=\frac{ESS}{TSS}$$

Important formula (considering the variables that form the `relationship' in this equation):
$$R^2 = \frac{ESS}{TSS}=\frac{\frac{1}{n} ESS}{\hat \sigma^2 _Y} =\hat \beta_2 ^2 (\frac{\hat \sigma^2 _X}{\hat \sigma ^2 _X})$$

Why define $R^2$ as $\frac{ESS}{TSS}=1-\frac{RSS}{TSS}$, and not just as $R^2 = r^2_{XY}?$
\begin{itemize}
    \item It would break in multiple regression
\end{itemize}

\begin{itemize}
    \item $E[\epsilon_i] = 0, E[\epsilon_i \epsilon_j ] = 0, E[\epsilon_i ^2 ] = \sigma^2$
\end{itemize}

\section{The Linear Model with Gaussian Errors}
Before:
\begin{itemize}
    \item We claimed we could fit a Normal Linear Model: $Y_i = \beta_1 + \beta_2 X_i +\epsilon_i \leftarrow$ $min_{\beta_1,\beta_2} \sum_i (Y_i -\beta_1 -\beta_2 X_i)^d$, $E[\epsilon_i] = 0$
    \begin{itemize}
        \item As long as $\epsilon_i \overset{\text{iid}}{\sim} N(0,\sigma^2)$ (iid meant uncorrelated, error can be fit into a normal distribution)
    \end{itemize}
    \item With this assumption, the OLS estimators $\hat \beta_1,\hat \beta_2$ are the \textbf{minimum variance unbiased estimators} (MVUE). That is, they have minimum variance among the class of all unbiased estimators, whether linear or not. his is stronger than being BLUE, which is restricted to linear estimators
    \item The OLS estimator is the best linear, unbiased estimator (BLUE)
    \item If we prove that our data is unbiased, without serial correlation, with error fitting into Gaussian Distribution, we have MVUE?

\end{itemize}

\section{Distribution of $\hat \beta_1, \hat \beta_2$}
\begin{itemize}
    \item If $\epsilon_i \overset{iid}{\sim} N (0, \sigma^2)$
    \item $\hat \beta_ 2 = f(x_i, y_i) \leftrightarrow Y_i = \beta_1 + \beta_2X_i + \epsilon_i$
    \item How does the distribution of $\epsilon_i$ affect the distribution of $\hat \beta_2$?
    \begin{itemize}
        \item It is a normal distribution (`sums of normals are normals': linear functions of gaussian are gaussian). This also transfers to $Y_i$
        \item We now know both $\hat \beta_1,\hat \beta_2$ are normally distributed
    \end{itemize}
    \item Unbiasedness: hope ($E[\hat \beta_2 ] = \beta_2$)
    \item $\hat \beta_2 \sim N(0, \frac{\sigma}{}$We expect to get the correct $sl^2{n})$
    \item $\hat \beta_1 \sim N(\beta_1, \frac{\sigma^2}{n}(1 + \frac{X^2}{\hat \sigma ^2 _X}))$
    \item We want $\hat \beta_2 \sim N(\beta_2, Var(\hat \beta_2))$. Problem: seems circular, we need the estimator itself.$\hat \beta_2 \rightarrow f(\hat \beta_2, X_i, Y_i ) \sim N(0,1)$
    \item $\hat \beta_2 \sim N(\beta_2, var(\hat \beta_2)) = N(\beta_2, \frac{\sigma^2}{n} (\frac{1}{\hat \sigma^2 _X}))$
    \item $\frac{\hat \beta_2 - \beta_2}{\sqrt{\frac{\sigma^2}{n} (\frac{1}{\hat \sigma^2_X})}}$. The denominator here is called ``standard error''
    \begin{itemize}
        \item This is achieved by normalizing $N(\beta_2, Var(\hat \beta_2)) \rightarrow N(0,1)$
        \item Subtract $\beta_2$ to normalize the mean, divide by square root of variance of $\hat \beta_2$. 
        \item The square root of the variance of the OLS estimator (the denominator) is called the standard error: $\hat \beta_2$
        \item Is the $\sigma^2$ a problem? No, we can estimate it (replace it with $s^2$, see `Estimation of $\sigma^2$'
        \item We have an estimator for $\beta_2$, so we can test for specific values to see if $\hat \beta_2 \neq \beta_2$. For example, we can plug in $\beta_2 = 0$. If we see the resulting data not fitting ($\sim N(0,1)$), then we can reject the premise that $\hat \beta_2 = \beta_2$
        \item When we sub in $s^2$ for $\sigma^2$, we affect the distribution since $s^2$ since $s^2$ has its own distribution.
        \begin{itemize}
            \item If we know $\sigma^2$ then we are fine
            \item If we don't, we need to estimate it via $s^2$
            \item $s^2$ is \textbf{NOT} a Gaussian Distribution, because of the squared term. It is a \textbf{Chi-Square Distribution} (with n-2 degrees of freedom)
            \item What is a Chi-Square? (number 3 of `Useful Results')
            \begin{itemize}
                \item Gaussian distribution can have negative values. Chi-Square only supports non-negative numbers (due to it being squared)
            \end{itemize}
            \item Using Number 4 of `Useful Results', we find that we have standard error fitting the t-distribution?
        \end{itemize}
    \end{itemize}
\end{itemize}

\subsection{Estimation of $\sigma^2$}
\begin{itemize}
    \item As before, $s^2 \equiv \frac{1}{n-2} \sum (Y_i -\hat \beta_1 -\hat \beta_2 X_i)^2$
    \begin{itemize}
        \item We assume that via Law of Large Numbers, we get $E[\epsilon_i^2] = \sigma^2$
    \end{itemize}
\end{itemize}

\subsection{Standard Error of $\hat \beta_1, \hat \beta_2$}
\begin{itemize}
    \item The Standard Error is the square root of the estimated variance
    \item $SE(\hat \beta_1) = \sqrt{\frac{s^2}{n} (1+ \frac{\bar X}{\hat \sigma^2_X})}$
    \item \textbf{IMPORTANT:} If I know my $\sigma^2$, we have a standard normal distribution. If I don't know my $\sigma^2$ (have to estimate it), it is a t-distribution
    \item t-ratio: $t\equiv \frac{\hat \beta_2 - \beta_2}{SE(\hat \beta_2)}=\frac{\hat \beta_2 - \beta_2}{\sqrt{\frac{\sigma^2}{n} (\frac{1}{\hat \sigma ^2_X})}} / \frac{s^2}{\sigma^2} = \frac{Z}{\sqrt{W/(n-2)}}$\\
    where $Z\sim N(0,1),W\sim \chi_{n-2}^2$ (related to Useful Results 3), with Z, W independent
    \item So $\frac{\hat \beta_2 - \beta_2}{SE(\hat \beta_2)} \sim t_{n-2}$ (See Useful Results 4). Similarly for $\hat \beta_1$. n-2 because of 2 degrees of freedom (remember two beta estimators)
\end{itemize}


Setup: do linear prediction, put a line through, have good properties of the line (gauss markov is all i need)


\section{Non-normal Errors}
\begin{itemize}
    \item $\hat \beta_2 = \frac{\sum_i (Y_i - \bar Y)(X_i - \bar X)}{\sum _i (X_i -\bar X)^2}$
    \item $\hat \beta_2 = \beta_2 + \frac{\sum_i (X_i - \bar X) \epsilon_i}{ \sum_i (X_i - \bar X)^2}$
    \begin{itemize}
        \item Multiply everything by $\frac{1}{\sqrt n}$
        \item As $n\rightarrow \infty$,the fraction becomes zero. The Numerator converges to a normal distribution $N(0, Var(\epsilon_i))$ because $\epsilon_i$ is normally distributed
        \item If we scale both sides by $\frac{1}{\sqrt n}$, we allude to Central Limit Theorem, so with no unbiasedness, no homoskedascity, no serial correlation (Gauss-Markov assumptions), we have a normal distribution, proving the central limit theorem for $\hat \beta_2$
    \end{itemize}
    \item Even if errors are non-normal, as we have infinitely many points, then we approach a normal distribution for our beta estimators
    \item By a generalization of the Central Limit Theorem, as long as \{$\epsilon_i$\} are iid\\
    $\hat \beta_1 \overset{a}{\sim} N(\beta_1, \frac{\sigma^2}{n}(1+ \frac{\bar X^2}{\hat \sigma^2 _X})$ and $\hat \beta_2 \overset{a}{\sim} N(\beta_2, \frac{\sigma^2}{n} (\frac{1}{\hat \sigma^2 _X}))$
    \item where ``a" refers to asymptotically or in the limit as $n\rightarrow \infty$
    \item Also, since $\underset{n\rightarrow \infty}{lim \text{ }s^2} = \sigma^2$
    \item $\frac{\hat \beta_1 -\beta_1}{SE(\hat \beta_1)} \overset{a}{\sim}$
\end{itemize}

OLS: minimize sum of squares via gauss markov assumptions, testing by assuming gaussian distribution, have a `pivot' so we can estimate beta


\section{Hypothesis Testing}
\begin{itemize}
    \item Hypothesis: An assumption about distribution of ``population"
    \item Objective of hypothesis testing: Given a random sample from a population, is there enough evidence to contradict some assertion about that population? (Karl Popper: science can only be done by proving yourself wrong)
    \item Null hypothesis ($H_0$): The statement to be tested; often the hypothesis of ``no effect" or ``no relationship"
    \item Alternative hypothesis ($H_A$): some other possibilities than the null hypothesis, e.g. ``some effect" or ``some positive effect''
    \item We have one-sided and two-sided hypothesis tests, which influence our null and alternative hypotheses
    \item Where do $H_0, H_A$ come from?
    \begin{itemize}
        \item Prior research, theoretical expectations, etc.
    \end{itemize}
    \item Possible States of Nature: ``$H_0$" is either true or false
\end{itemize}
Types of Error
\vspace{0.1in}

\begin{tabular}{ | c | c | c| }
\hline
Decision/Nature & $H_0$ True & $H_0$ false\\
\hline
Accept $H_0$ & Correct decision & Type II error\\
\hline
Reject $H_0$ & Type I error & Correct decision\\
\hline
\end{tabular}
\begin{itemize}
    \item False positive: Type I error
    \begin{itemize}
        \item We consider this to be more severe.
    \end{itemize}
    \item False negative: Type II error
\end{itemize}
We can think of the p-value as the percentage of times we may commit a Type I error
\begin{itemize}
    \item Example: If we have alpha value $\alpha = 0.05$, this means the probability of making a Type I error is 5\%)
\end{itemize}
Other terms
\begin{itemize}
    \item Significance level: $\alpha = P(\text{Type I }| H_0\text{ true})$
    \item Confidence Level: $1-\alpha = P(\text{Accept }H_0 | H_0\text{ true})$
    \item ``Operating characteristic": $\beta = P(\text{Type II } | H_0 \text{false})$
\end{itemize}

\subsection{Hypothesis Testing: The test of significance approach}
Assumption (for now): $\epsilon_i \overset{iid}{\sim} N(0, \sigma^2), \sigma^2 unknown$
Contains Gauss-Markov assumptions
\begin{itemize}
    \item Unbiased: $E[\epsilon_i] = 0$
    \item No serial correlation: $E[\epsilon_i\epsilon_j] = 0$
    \item $E[\epsilon_i ^2] = \sigma^2$
    \item $X_i$ deterministic
    \begin{itemize}
        \item $E[\epsilon_i X_i ] = 0$
        \item $X_i E[\epsilon_i] = 0$
    \end{itemize}
\end{itemize}
Our next job is to turn statistics into proving the assumptions\\
Recall:
\begin{itemize}
    \item $\hat \beta_2 \sim N(\beta_2, Var(\hat \beta_2))$
    \item $\hat \beta_2 - \beta_2 \sim N(0, Var(\hat \beta_2))$
    \item $\frac{\hat \beta_2 - \beta_2}{\sqrt{Var(\hat \beta_2}} \sim N(0,1)$. We call the denominator the standard error of $\hat \beta_2$ ($SE[\hat \beta_2]$)
\end{itemize}
If our Gauss-Markov assumptions are correct: $\frac{\hat \beta_2 - \beta_2}{SE(\hat \beta_2) \sim t_{n-2}}$ and $\frac{\hat \beta - \beta_1}{SE(\hat \beta_1)} \sim t_{n-2}$
\begin{itemize}
    \item If the value of $\beta_2 (\beta_1)$ is specified under the null hypothesis, the t-value can be obtained from the sample, and can serve as a test statistic (a random variable that allows you to test a hypothesis). Since this test statistic follows the t-distribution, we can make confidence interval statements as follows:
    \begin{itemize}
        \item $P\left(-t_{dof, \frac{\alpha}{2}} \leq \frac{\hat \beta_2 - \beta^*_2}{SE(\hat \beta_2)}\leq t_{dof, \frac{\sigma}{2}}\right) = 1- \alpha$ (2-tailed)
        \begin{itemize}
            \item Where $\beta_2^*$ is a value of $\beta_2$ under $H_0$ and $-t_{dof, \frac{\sigma}{2}}, t_{dof, \frac{\sigma}{2}}$ are ``critical" t-values from t-table for $\alpha$ level of significance and $n-2$ degrees of freedom
        \end{itemize}
        \item Rearrange
        \item $P\left(\beta^*_2 - t_{dof, \frac{\sigma}{2}} SE(\hat \beta_2 ) \leq \hat \beta_2 \leq \beta_2 ^* + t_{dof, \frac{\sigma}{2}} SE(\hat \beta_2)\right) = 1-\alpha$
    \end{itemize}
\end{itemize}
\textbf{IMPORTANT:} $t = \frac{\hat \beta_2 - \beta^* _2}{\sqrt{Var(\hat \beta)}}$
\begin{itemize}
    \item If $\hat \beta_2 - \beta_2^* \neq 0$, we probably have the wrong $\beta$
    \item \textbf{In other words:} The further $\hat \beta_2$ is away from our initial guess $\beta_2 ^*$, and the smaller $SE(\hat \beta_2)$, the larger $|t|$ (focus on absolute value, since t can be positive or negative). How large is large enough to reject some $H_0$ depends on the choice of level of significant and the dof
    \item \textbf{The larger the difference, the greater the t statistic is}
    \begin{itemize}
        \item \textbf{Important intuition:} $t$-statistic is ratio of distance between predicted beta and actual beta compared to normalized variance of beta parameter
        \begin{itemize}
            \item $|t| = \frac{\hat \beta_2 -\beta_2 ^*}{\sqrt{Var(\hat \beta_2)}}$
        \end{itemize}
    \end{itemize}
    \item Summary of decision rules\\\\
    \begin{tabular}{|c|c|c|c|}
        \hline
        Type of hypothesis & $H_0$ & $H_A$ & Decision: Reject $H_0$ if \\
        \hline
        two-sided & $\theta = \theta^*$ & $\theta \neq \theta^*$ & $|t| > t_{dof, \frac{\alpha}{2}}$\\
        \hline
        one-sided (right tail) & $\theta = \theta^*$ & $\theta > \theta^*$ & $t > t_{dof, \alpha}$\\
        \hline
        one-sided (left tail)& $\theta = \theta^*$ & $\theta < \theta^*$ & $t < -t_{dof, \alpha}$\\
        \hline
    \end{tabular}
    \item Alternatives
    \begin{itemize}
        \item $\epsilon_i \overset{iid}{\sim} N(0, \sigma^2), \sigma^2$ known $\Rightarrow$ use $N(0,1)$
        \item Asymptotically $(n\rightarrow \infty) \Rightarrow N(0,1)$
    \end{itemize}
\end{itemize}
A linear regression implicitly does an ANOVA (analysis of variance), basically a t-test on a coefficient

\subsection{Relationship or $R^2 = r^2 _{XY}$ to t-test}
\begin{itemize}
    \item Sum of squares that entire the $R^2$ and their dof in an \textbf{analysis of variance (ANOVA)} approach:
    \item Remember that $R^2 = \frac{ESS}{TSS} = 1- \frac{RSS}{TSS}$
    \item $RSS = \sum (Y_i - \hat Y_i)^2$ has n-2 dof
    \item $ESS = \sum (\hat Y_i - \bar Y)^2=\hat \beta_2 ^2 \sum (X_i - \bar X)^2$ has 1 dof (since X's are fixed, only $\hat \beta_2$ varies as sample changes)
    \begin{itemize}
        \item Not going over derivation in class (5-10 min?)
    \end{itemize}
    \item Linear regression model and \textbf{ONLY UNDER THE TEST $\beta_2$ AND $H_A : \beta\neq 0$} (two-tailed), can we use the following test:
    \item To test $H_0 : \beta_2 = 0$ vs $H_A:\beta_2 \neq 0$, would reject if
    \begin{itemize}
        \item $\left| \frac{\hat \beta_2}{SE(\hat \beta_2-0)}\right | > t_{dof,\frac{\alpha}{2}}$
        \begin{itemize}
            \item Can also reject if $(n-2)\frac{R^2}{1-R^2} > F_{1,n-2,\alpha}$ since \\ $(n-2) \frac{R^2}{1-R^2} = (n-2) \frac{ESS/TSS}{RSS/TSS} = \frac{ESS}{\frac{1}{n-2} RSS} = \frac{\hat \beta_2 ^2 \sum(X_i - \bar X)^2}{\frac{1}{n-2} \sum (Y_i - \hat Y_i )^2} = \frac{\hat \beta_2 ^2 \sum (X_i - \bar X)^2}{\frac{1}{n-2} \sum (Y_i -\hat \beta_1 -\hat \beta_2 X_i )^2} = \frac{\hat \beta_2 ^2}{\frac{s^2}{n} \frac{1}{\hat \sigma_X ^2}} = \left(\frac{\hat \beta_2}{SE(\hat \beta_2)}\right)^2$
            \item A F-test is a squared t-distribution
            \item and (\textbf{IMPORTANT}) $F_{1,n-2,\alpha} = (t_{n-2, \frac{\alpha}{2}})^2$
            \item That is, $\frac{(n-2)}{1}\frac{R^2}{1-R^2} \sim F_{1,n-2}$ under $H_0$
            \item This is valid \textbf{only} under this specific $H_0$ and $H_1$ in a regression with intercept. Why do this in addition to t-test? Useful when have multiple regressors
            \item With the \textbf{F-test, we can only test two-sided alternatives}, because when we square the t-statistic, we CANNOT tell whether the original t-statistic was negative or positive
        \end{itemize}
    \end{itemize}
\end{itemize}

\subsection{General F-test approach}
\begin{itemize}
    \item Joint hypothesis: $H_0 : \beta_1 = \beta_1 ^*$ and $\beta_2 = \beta_2 ^*$ vs.\\ $H_1: \beta_1 \neq \beta_1 ^*$ and/or $\beta_2 \neq \beta_2^*$
    \item \textbf{Important: F-test alternative always ``two-sided"}\\
    \begin{tabular}{|c|c|c|}
        \hline
         & one-sided allowed & only two-sided \\
         \hline
         only single variable ($\beta_1, \beta_2$) & t-test & t-test/F-test \\
         \hline
         multiple variables with multiple hypotheses & ?? & F-test\\
         \hline
    \end{tabular}
    \begin{itemize}
        \item t-tests are marginal distributions $\hat \beta_1, \hat \beta_2$
        \item F-test uses joint distributions (of normal distributions)
        \item Marginal distributions are Gaussian here. t-tests use marginal distributions
    \end{itemize}
    \item Computation of F-statistic:
    \begin{itemize}
        \item Compute $RSS_U$ (unrestricted) by estimating $\hat \beta_1, \hat \beta_2$ under alternative hypothesis (ie, unrestricted) $\Rightarrow RSS_U = \sum (Y_i -\hat \beta_1 -\hat \beta_2 X_i )^2$
        \begin{itemize}
            \item The restriction comes from $H_0$ (we pretend the null hypothesis is true when testing against it). Our $RSS$ is unrestricted because we have conducted our tests and rejected the null hypothesis
        \end{itemize}
        \item Compute $RSS_R$ by imposing null hypothesis restrictions and estimating any unrestricted parameters
        \begin{itemize}
            \item RSS is restricted: Solving OLS with an additional restriction (the null hypothesis)
            \item \textbf{Example:} $H_0:\beta_2 = 0$ vs $H_A:\beta_2 \neq 0$
            \item $RSS_U\leftarrow$ just run OLS on the mode (without any hypothesis) $$Y_i = \beta_1 + \beta_2 X_i + \epsilon_i$$
            $$\hat \beta_2 = \frac{\sum_i (Y_i -\bar Y)(X_i - \bar X)}{\sum_i (X_i - \bar X)^2}$$
            \item $RSS_r$, (assume null hypothesis that $\beta_2=0$ is true, so assume $\beta_2 = 0$) run a regression where $\beta_2$ doesn't show up. $$Y_i =\beta_1 + \epsilon_i$$
            \item Comparing errors: $RSS_r \geq RSS_u$
            \item Why?
            \begin{itemize}
                \item It's a minimization problem, restrictions impede that and at best have no impact
            \end{itemize}
        \end{itemize}
        \item Compute $F= \frac{(RSS_R - RSS_U) / r}{RSS_U / (n-k)}$
    \end{itemize}
\end{itemize}

\subsection{Functional Forms of Regression}
Before, we focused on $Y_i = \beta_1 + \eta_2 X_i + \epsilon_i$. Now let's focus on some alternative linear models
\begin{itemize}
    \item We can use OLS on non-parametric predictions (i.e. machine learning). We can put non-linear terms in our linear approximation
    \item Linearity in parameters $\beta_1, \beta_2$
    \item Important log properties (assuming $A$ and $B$ are positive and $k$ is some constant)
    \begin{itemize}
        \item $ln(AB) = lnA + ln B$
        \item $ln(\frac{A}{B}) = lnA - ln B$
        \item $ln(A^k) = klnA$
        \item Derivative: $\frac{d(lnX)}{ dX} = \frac{1}{X}$ or $d(ln X) = \frac{dX}{X}$
        \item Note following terms:
        \begin{itemize}
            \item Absolute change: $X_t - X{t-1}$
            \item Relative or proportional change: $\frac{(X_t - X_{t-1})}{X_{t-1}}$
            \item Percentage change or percent growth rate: $\frac{(X_t - X_{t-1})}{X_{t-1}} * 100$
            \item \textbf{Change in log variable gives a percentage change}: $ln X_t - lnX_{t-1} \approx \frac{X_t - X_{t-1}}{X_{t-1}}$
        \end{itemize}
    \end{itemize}
\end{itemize}
\subsection{``How to measure elasticity": The log-log model}
Intuitively, think of logs as `elasticities' due to percentage change
\begin{itemize}
    \item Consider the following (non-linear regression model):
    \begin{itemize}
        \item $Y_i = \beta_1 X ^{\beta_2} exp(\epsilon_i)$
        \item Taking a (natural) log on both sides: $ln Y_i = \beta_1 X^{\beta_2} exp(\epsilon_i)$
        \item Now, this model (via a new linear relationship) may alternatively be expressed as 
        \begin{itemize}
            \item $ln Y_i = ln\beta_1 + \beta_2 lnX_i + \epsilon_i$
            \begin{itemize}
                \item ``Identification": How we do we identify what we really want ($\beta_1$) by getting something else?
            \end{itemize}
            \item Then let \\ $Y_i ^* = \alpha + \beta_2 X_i ^* + \epsilon_i$ where $Y_i ^* = lnY_i , X_i ^* = lnX_i,$ and $\alpha = ln \beta_1$
            \item Assuming that the Basic Assumptions i) - vii) are satisfied for this model, the OLS estimators $\hat \alpha, \hat \beta_2$ will be BLUE of $\alpha, \beta_2$ (note that $\hat \beta_1 = exp(\hat \alpha$)) will be a biased estimator of $\beta_1$)
            \item \textbf{Important:} $\beta_2$ measures the \textbf{elasticity} of Y with respect to X (the percentage change in Y for a given (small) percentage change in X) (since $\beta_2 = \frac{d(lnY)}{d(lnX)} = \frac{dY/Y}{dX/X}$)
        \end{itemize}
        \item Implication of linear mode: $Y_i = \beta_1 + \beta_2 X_i + \epsilon_i$
        \begin{itemize}
            \item Take derivative $\frac{dY}{dX} = \beta_2$. The slope is constant $\forall X \in \R$. We love linear regression so much because linearity means no change in $\beta_2$
            \item Let $\overset{\sim}{ \beta_1} = ln(\beta_1)$
            \item Identification: To define our $\beta_1$, we need to take an exponential: $exp(\overset{\sim}{\beta_1}) = \beta_1 = 1$
        \end{itemize}
    \end{itemize}
    \item Constant elasticity model: The model assumes that the elasticity coefficient between $Y$ and $X, \beta_2$, remains constant (linearity) throughout
    \begin{itemize}
        \item $Y_i = \beta_1 X ^{-\beta_2}$
    \end{itemize}
    \item The double-log transformation: $ln Y_i = ln\beta_1 - \beta_2 ln X_i$
    \item Expressing in units (remember log means percentage change)
    \begin{itemize}
        \item $\widehat{Y_t} = 0.7774 - 0.2530 ln(\widehat{X_t})$
        \item $0.2530\%$ percentage change in $Y_t$ with a $1\%$ increase in $X_t$
    \end{itemize}
    \item Recall: Think of $R^2$ as a prediction parameter: Taking log-log model can improve $R^2$ due to shape of the data
    \item Our $\epsilon_i$ is set up so that our $\epsilon_i \sim N(0, \sigma)$ (we get a Gaussian distribution in order to satisfy our Gauss-Markov theorem)
\end{itemize}

\subsection{How to measure the growth rate: The log-lin (Log Linear) model}
\begin{itemize}
    \item Using the compound interest formula, we can write the following model: \[Y_t = Y_0 (1+r)^t exp(\epsilon_t)\] where, for instance, $Y_t = $ real GDP at time $t, Y_0 = $ initial value of real GDP, and where r is the compound (i.e. over time) rate of growth of Y.
    \item Taking the natural log, we can rewrite the model in the following way: \[ln(Y_t) = ln(Y_0) + tln(1+r) + \epsilon_t\]
    \begin{itemize}
        \item $Y^*=ln(Y_t)$
        \item $\beta_1 = ln(Y_0)$
        \item $\beta_2 = ln(1+r)$
        \item $X = t$
    \end{itemize}
    \item Identification: identify what we want (r) by working towards getting other parameter ($\beta_2$)
    \item Once again, assuming Basic Assumptions i) - vii) are satisfied for this model, the OLS estimators $\hat \beta_1, \hat \beta_2$ will be BLUE of $\beta_1, \beta_2$
    \item $\beta_2$, the slope coefficient, measures the constant proportional or relative change in Y for a given absolute change in the value of the regressor (here, time) (since $\beta_2 = \frac{d(lnY)}{dt} = \frac{dY/Y}{dt})$
    \item Note that it is NOT possible to get an unbiased estimator of r (nonlinear)
\end{itemize}

\section{Note on the nature of the error term}
\begin{itemize}
    \item Consider a regression model without the error term: 
    \begin{itemize}
        \item $Y_i = \beta_1 X_i ^{\beta_2}$
        \item For estimation purposes, we could express this model in at least 3 different ways:
        \begin{itemize}
            \item $Y_i = \beta_1 X_i ^{\beta_2} \epsilon_i$
            \item $Y_i = \beta_1 X_i ^{\beta_2} exp(\epsilon_i)$
            \item $Y_i =\beta_1 X_i ^{\beta_2}+ \epsilon_i$
        \end{itemize}
    \end{itemize}
    \item NOTE: $ln\epsilon \not \sim N(0, \sigma^2)$
    \item $ln Y_i = ln\beta_1 + \beta_2 lnX_i + \epsilon_i \leftarrow \epsilon_i \sim N(0, \sigma^2)$
\end{itemize}

ESS: explained variation in Y
TSS: sum of explained and residual

Multiple hypotheses:
\begin{itemize}
    \item $H_0: \beta_2, \beta_3, \beta_4, \cdots = 0$
    \item $H_A:$ at least one $\neq 0$
\end{itemize}

\section{Matrix Algebra}
Recall
\begin{itemize}
    \item Matrix addition is commutative and associative
    \item Scalar multiplication
    \begin{itemize}
        \item Distributive property
    \end{itemize}
\end{itemize}

Consider model: $Y_i = \beta_1 + \beta_2 X_{i2} + \beta_3 X_{i3} + \cdots + \beta_k X_{ik} + \epsilon_i \leftarrow \hat \beta = (X^T X) ^{-1} X^T y$
\begin{itemize}
    \item $X^T X:$ Var(X)
    \item $X^T y: $ cov(X,Y)
\end{itemize}
$\beta_1$

IMPORTANT: $\hat \beta = (X^TX) ^{-1} X^T y$
% \begin{itemize}
%     \item $X^T X$ is SPD (I think)
% \end{itemize}
Other properties
\begin{itemize}
    \item Matrix multiplication is not commutative
    \item Matrix multiplication is associative
    \item Matrix multiplication is associative
    \item Note: for transpose, we will write $A^\prime$ (prime is used instead of T for econometrics)
    \begin{itemize}
        \item $(A^\prime)^\prime = A$
        \item $(A+B)^\prime = A^\prime + B^\prime$
        \item $(AB)^\prime = B^\prime A^\prime$
    \end{itemize}
    \item Inverse
    \begin{itemize}
        \item $(A^{-1})^\prime = (A\prime)^{-1}$
        \item det$(A^{-1}) = \frac{1}{det(A)}$
    \end{itemize}
    \item Idempotency
    \begin{itemize}
        \item A is said to be idempotent iff $A^2 = A$.
        \begin{itemize}
            \item Only square matrices can be idempotent
        \end{itemize}
    \end{itemize}
\end{itemize}
Matrix Differentiation
\begin{itemize}
    \item Let $a$ and $x$ be $n\times 1$ vectors and A be an $n\times n$ matrix
    \begin{itemize}
        \item $\frac{\partial (a^\prime x)}{\partial x_1} = a$
        \item $\frac{\partial (x^\prime Ax)}{\partial x} = (A+A^\prime) x$
        \item $\frac{\partial (x^\prime Ax)}{\partial x \partial x^\prime} = A+A^\prime$
    \end{itemize}
\end{itemize}
\subsection{Derivation of Least Squares Estimators}
\begin{itemize}
    \item $\sum \hat \epsilon_i ^2 = \sum(Y_i - \hat \beta_1 - \hat \beta_2 X_{i2} - \cdots - \hat \beta_k X_{ik})^2$
    \item $min \sum \hat \epsilon_i ^2$ over $\hat \beta_1 , \cdots, \hat \beta_k$ UGLY without matrix notation $\Rightarrow \underset{k\times 1}{\hat \beta} $ minimizes $\underset{1\times n}{\hat \epsilon ^\prime} \underset{n \times 1}{\hat \epsilon}$ where $\underset{n\times 1}{\hat \epsilon} = \underset{n\times 1}{Y} - \underset{n\times k}{X} \underset{k\times 1}{\hat \beta}$
    \item $\underset{(n\times 1)}{Y} = \underset{(n\times k)}{ X} \underset{(k\times 1)}{\beta} + \underset{(n\times 1)}{\epsilon}$, where each row represents an observation (so n total observations)
    \item Assumptions we need
    \begin{itemize}
        \item $E[\epsilon_i] = 0$
        \item $E[\epsilon_i^2] = \sigma^2$ (heteroskedascity)
        \item $E[\epsilon_i \epsilon_j] = 0$ (no serial correlation)
    \end{itemize}
    \item How do we represent these assumptions in matrix notation
    \begin{itemize}
        \item $E[\epsilon \epsilon^\prime] = \sigma^2 \underset{(n\times n)}{I}$. In other words, we must get a diagonal matrix to prove no serial correlation (outer product)
        \begin{itemize}
            \item Diagonal is $\sigma^2$ in order to prove constant variance/heteroskedasticity
            \item The fact that the matrix is diagonal proves no serial correlation
        \end{itemize}
        \item For first condition of unbiasedness: $E[\epsilon] = \vec 0_k$
        \item To conclude, we have two conditions
        \begin{itemize}
            \item $E[\epsilon] = \vec 0_k$
            \item $E[\epsilon \epsilon^\prime] = \sigma^2 \underset{(n\times n)}{I}$
        \end{itemize}
    \end{itemize}
\end{itemize}
\textbf{Actual Derivation}
\begin{itemize}
    \item $Y = X\beta + \epsilon \leftrightarrow \epsilon = Y- X\beta$
    \item $\sum_{i=1}^k \epsilon_i ^2 = \epsilon\prime \epsilon$ (dot product)
    \item Goal: Find $\underset{k\times 1}{\hat \beta}$ that minimizes $\hat \epsilon^\prime \hat \epsilon$ (dot product sum) where $\hat \epsilon = Y - X\hat \beta$
    \item $\underset{\hat \beta}{min(Y-X\hat \beta)^\prime(Y-X\hat \beta)}$ (subbed in for epsilon)
    \item $\underset{\hat \beta}{min} (Y^\prime - \hat \beta ^\prime X^\prime) (Y-X\hat \beta)$ (applied transpose)
    \item $\underset{\hat \beta}{min} (Y^\prime Y - \hat \beta^\prime X^\prime Y - Y^\prime X \hat \beta + \hat \beta^\prime X^\prime X\hat \beta)$ (distributed)
    \item $\underset{\hat \beta}{Y^\prime Y - 2Y^\prime X \hat \beta + \hat \beta^\prime X^\prime X \hat \beta)}$. Take partial derivative with respect to beta
    \begin{itemize}
        \item For $Y^\prime Y$ partial derivative of this is 0
        \item First matrix diff. rule: $-2Y^\prime X \hat \beta$
        \item Use second matrix diff. rule on: $\hat \beta^\prime X^\prime X \hat \beta$
        \item Recall: $(x^\prime x)^\prime = x^\prime x$ (helps with simplifying)
    \end{itemize}
    \item $-2 \frac{\partial}{\partial \hat \beta} (Y^\prime X\hat \hat \beta) + \frac{\partial}{\partial \hat \beta} (\hat \beta^\prime X^\prime X \hat \beta)=0$. $-2 X^\prime Y + 2(X^\prime X)\hat \beta = 0$ (``normal equations")
    \item $-2 X^\prime Y + 2(X^\prime X)\hat \beta = 0$
    \item Simplify via matrix algebra
    \item $\hat \beta = (X^\prime X)^{-1} X^\prime Y$
    \begin{itemize}
        \item $X^\prime Y \leftarrow$ empirical covariance between $X$ and $Y$.
        \item $X^\prime X$ is $Var(X)$
        \item \textbf{VERY IMPORTANT INTUITION: }Linear regression is the $\hat \beta = \frac{Cov(X,Y)}{Var(X)}$
        \begin{itemize}
            \item ``How much does Y vary if we only vary X?"
        \end{itemize}
    \end{itemize}
\end{itemize}
\subsection{Expectation of $\hat \beta$}
\begin{itemize}
    \item $\hat \beta = (X^\prime X)^{-1} X^\prime y = (X^\prime X)^{-1} X^\prime (X\hat \beta + \epsilon)$ (subbed in y)
    \item $\Rightarrow \hat \beta = (X^\prime X)^{-1} (X^\prime X)\beta + (X^\prime X)^{-1} X^\prime \epsilon = \beta + (X^\prime X)^{-1} X^\prime \epsilon$
    \item Take expectation: $E(\hat \beta) = \beta + E[(X^\prime X)^{-1} X^\prime \epsilon] = \beta + (X^\prime X)^{-1} X^\prime E(\epsilon) = \beta$. RECALL: $E(\epsilon) = 0$ via Gauss-Markov assumptions, leaving only $\beta$
\end{itemize}

\subsection{Covariance Matrix of $\hat \beta$}
\[\underset{k\times k}{cov(\hat \beta)} = \begin{bmatrix}
    var(\hat \beta_1) & cov(\hat \beta_1, \hat \beta_2 ) &\cdots\\
    cov(\hat \beta_2, \hat \beta_1) & var(\hat \beta_2) & \cdots \\
    \cdots & \cdots & \cdots
\end{bmatrix} = E[(\hat \beta- \beta)(\hat \beta - \beta)^\prime]\]

\textbf{TODO}

\subsection{Efficiency}
\begin{itemize}
    \item Recall: $\overset{\sim }{\beta} = \sum_i b_i Y_i$. Replace it with matrix C
    \item $\hat \beta$ is BLUE (Gauss-Markov Theorem) $\Rightarrow$ for any $\underset{k\times 1}{\overset{\sim}{\beta}} = \underset{k\times n}{C}\underset{n\times 1}{Y}$
\end{itemize}

\subsection{Asymptotitc Normality}
\begin{itemize}
    \item Under our 4 Gauss-Markov assumptions
    \item $\hat \beta \sim a N(\beta , \sigma^2 (X^\prime X)^{-1}$
    \item If $\epsilon \sim N(0, \sigma^2 I)$, this is exact distribution, not just asymptotically
\end{itemize}

\subsection{Estimation of $\sigma^2$}
\begin{itemize}
    \item Unbiased and consistent estimator for $\sigma^2$
    \item $s^2 = \frac{\hat \epsilon ^\prime \hat \epsilon}{n-k}$ where $\hat \epsilon^\prime \hat \epsilon$
\end{itemize}

\subsection{Dummy Variables and Chow Test}
Dummy Variable
\begin{itemize}
    \item A regressor $X_{ij}$ which only takes either 0 or 1 as a value is a dummy variable; indicator functions (discontinuous/discrete function that is either 1 or 0)
    \item Interaction term ($X_{i4}$ here, $X_{i4} = X_{i2} \cdot X_{i3} , \text{ 
 } X_{i2}:\text{ age}, X_{i3}: \text{ isEmory}$): Will allow to pick out and ``control for" age differences in the two groups. Age times isEmory (dummy).
    \begin{itemize}
        \item When we are NOT in the treatment group, we get the number 0, generating an interaction term
        \item We want to control for age in the treatment group (isEmory)
        \item The dataset has been split to 2 based on dummy variable
    \end{itemize}
    \item $Y_i = \beta_1 + \beta_2 X_{i2} + \beta_3 X_{i3} + \beta_4 X_{i4} + \epsilon_i \Rightarrow  \\\begin{cases}
        \beta_1 + \beta_2 X_{i2} + \epsilon_i \text{ if not Emory grad}\\
        (\beta_1 + \beta_3 ) + (\beta_2 + \beta_4) X_{i2} + \epsilon_i \text{ if Emory grad (dummy variable is 1, so simplify)}
    \end{cases}$
    \item Run two separate regressions, since we now have 2 (non-Emory is our control group, Emory is our treatment Group)
    \item Let $\alpha_1 = \beta_1 + \beta_3,\alpha_2 = \beta_2 + \beta_4$ equal our new parameters for the treatment group
    \item Recall: identification: $\beta_3 \rightarrow \alpha_1 - \beta_1, \beta_4 \rightarrow \alpha_2 - \beta_2$, where the $\beta_1, \beta_2$ come from our first regression
    \item age is a confounder. 
    \item Possible $H_0: \beta_3 = \beta_4 = 0 \Leftrightarrow$ being Emory grad has no effect on salary
    \begin{itemize}
        \item Normally, 2 tests (each beta equals 0). However, we can do this in 1 test with F-test
        \item $\beta_3 = 0 \rightarrow \alpha_1 = \beta_1$ (is the intercept of non-Emory grads the same as Emory grads)
        \item Same goes for $\beta_4$ with slope and other alpha
    \end{itemize}
    \item Notes:
    \begin{itemize}
        \item IMPORTANT (multicollinearity): When the regression contains an intercept, and there are m groups, only include m-1 dummies, for identification (otherwise case of perfect multicollinearity - discussed later), to avoid ``dummy variable trap"
        \item Multicollinearity
        \begin{itemize}
            \item Linear dependence: (design) matrix must be independent (recall: no column can be expressed as linear combination of others)
            \item If not independent, matrix is not invertible. Think full-rank matrix
            \item `Perfect multicollinearity': columns are linearly dependent
            \item If there is no intercept, we can have $m$ dummies for $m$ groups without multicollinearity. Otherwise, we need to stick to at most $m-1$
        \end{itemize}
    \end{itemize}
    \item Chow Test: Two regression equations (will not be tested on final)
    \begin{itemize}
        \item When dealing with time-series, we have a `regime shift' due to an external influence. Run linear regression on data before time T (when regime shift occurred) and after time T
        \item Change-point detection: setting T
        \item Problem: can only test one or two types of regressions
        \item In a Chow test, test that both intercept and the slope are the same for the two regressions
    \end{itemize}
\end{itemize}

\subsection{Multicollinearity}
\begin{itemize}
    \item Basic Assumptions used so far:
    \begin{itemize}
        \item $Y=X\beta + \epsilon$
        \item X fixed (non-random)
        \begin{itemize}
            \item Will relax up until end of class
        \end{itemize}
        \item X full rank (invertibility)
        \begin{itemize}
            \item Relax now
        \end{itemize}
        \item $E(\epsilon)=0$
        \item $var(\epsilon) = \sigma^2 I$
        \begin{itemize}
            \item Relax the 2 Gauss Markov assumptions in Section 11
        \end{itemize}
    \end{itemize}
\end{itemize}
GLS (generalization of OLS)
\begin{itemize}
    \item If Gauss-Markov not satisfied, then OLS not best estimator. Best lowest variance estimator is now GLS
    \item If interested, look at lecture notes
\end{itemize}
\section{Endogeneity}
\subsection{Omitted Variables/Omitted Variable Bias}
\begin{itemize}
    \item The \textbf{omission of one or more relevant variables} from the regression causes the estimates of the included variables to be \textbf{biased}. \textbf{So: $E(\tilde \beta)\neq \beta$}
    \item \textbf{Direction of the bias }depends on covariances between included and excluded variables and on the signs of the excluded variables' coefficients
    \begin{itemize}
        \item This is \textbf{bad}: we don't even know the direction of the bias. For example, with omitted variables, we can predict that chocolate consumption leads to longevity
    \end{itemize}
    \item If \textbf{every excluded variable is orthogonal} to every included variable (matrix algebra sense), then the OLS estimates of the included variables will be unbiased
    \begin{itemize}
        \item Implication of orthogonal: $\frac{Cov(X,Z)}{Var(X)} = 0$
    \end{itemize}
    \item \textbf{Endogeneity}
    \begin{itemize}
        \item X is random (itself)
        \begin{itemize}
            \item We can't do this anymore (pull out X): $E[\epsilon X] = XE[\epsilon] = 0$, since it requires X to be nonrandom. If X is not random, then it is not correlated with anything
            \item Need to deal with covariance now: $cov(X,\epsilon) = E[X\epsilon] + E[X]E[\epsilon]$ (considering first moment). We still assume $E[\epsilon] = 0$.
            \item Note: in matrix form, $E[X^\prime \epsilon] \leftarrow$ we need to transpose X to match up dimensions
            \item $E[X\epsilon] = 0$. X is an exogenous regressor iff. it is an uncorrelated regressor
            \item Endogeneity is the opposite: it says that $X$ is correlated. From now on, we will mostly handle endogeneity
            \item Endogeneity is a \textbf{first-order problem} (our estimator is biased). It doesn't deal with the best estimator in terms of best variance, we are saying we don't get the proper estimator if we don't address it ($E(\tilde \beta) \neq \beta$)
        \end{itemize}
        \item \textbf{True Model:} $Y_i = \beta X_i + \gamma Z_i + \epsilon \Rightarrow \hat \beta$
        \begin{itemize}
            \item If we have an omitted variable, we think that this is the model we should consider
            \item What we wish we could do
        \end{itemize}
        \item \textbf{Estimated model:} $Y_i = \beta X_i + u_i \Rightarrow \tilde\beta$
        \begin{itemize}
            \item What we're forced to do. This is the wrong model
            \item \textbf{Important:} respond by running an estimator on the estimated model, we call the new estimator $\tilde \beta$
            \item $\tilde \beta = \frac{\sum X_iY_i}{\sum X_i^2} = \text{Plug in the true model} \frac{\sum X_i (\beta X_i + \gamma Z_i + \epsilon_i)}{\sum X_i^2} = \beta + \gamma \frac{\sum X_i Z_i}{\sum X_i ^2} + \frac{\sum X_i \epsilon_i }{\sum X_i^2}$
            \begin{itemize}
                \item  (in order to see what we want, we need to compare it to the truth). We want to see what we're doing wrong 
                \item Take an expectation on both sides
            \end{itemize}
            \item $E\hat \beta = \beta + \gamma \frac{\sum X_i Z_i}{\sum X_i ^2} + \frac{\sum X_i E(\epsilon_i )}{ \sum X_i^2} = \beta + \gamma \frac{\hat \sigma_{XZ}}{\hat \sigma^2 _X} \neq \beta$ where $\hat \sigma_{XZ} = \frac{1}{n} \sum X_iZ_i$ (covariance) and $\hat \sigma^2 _X = \frac{1}{n}\sum X_i^2$ (variance of X)
            \item \textbf{IMPORTANT TERM}: $\gamma \frac{\hat \sigma _{XZ}}{\hat \sigma_X ^2}$. When is $\gamma = 0$ or covariance is 0? When variable has no impact, unbiased.
            \item We control for Z: we have
            data on Z we can include in our regression
        \end{itemize}
        \item An omitted variable is only bad if it affects both the outcome and affects the regressor. In other words, we are taking the effects of one regressor (which we are removing) and mistakenly attributing it to another (Causal Inference). 
        \item If the variables are uncorrelated, then there's no issue
        \item Keeping in the Z regressor controls for Z
    \end{itemize}
    \item Solution to omitted variable bias?
    \begin{itemize}
        \item No solution really, include as many terms as possible
    \end{itemize}
\end{itemize}

\subsection{Irrelevant Variables}
Context
\begin{itemize}
    \item Inclusion of irrelevant variables does NOT cause the OLS estimators of the other variables to be biased. So $E\tilde \beta = \beta$. So, it doesn't lead us to a first-order problem, but it does lead to a second-order problem.
    \item The problem that including irrelevant variables does cause is that it reduces the precision with which you estimate the coefficients on the other, relevant variables. So $var(\tilde \beta) > var(\hat \beta)$ (similar to overfitting)
    \begin{itemize}
        \item Issue: affects standard error, which is going to screw up with tests. Recall $t = \frac{\tilde \beta}{SE[\tilde \beta]}$
        \item ``kitchen sinking" regressions (throw everything into regressions, doesn't matter if it's relevant or not)
    \end{itemize}
    \item This efficiency loss does not occur when the irrelevant variable s are orthogonal to the remaining variables. So when $X^\prime Z=0$ (where X are the included relevant variables, and Z are the included irrelevant variables) $\Rightarrow \tilde \beta = \hat \beta, var(\tilde \beta) = var(\hat \beta)$
    \item \textbf{Example}: Assume that $\bar X =\bar Z = \bar Y = 0$
    \begin{itemize}
        \item True model: $Y_i = \beta X_i + \epsilon_i \Rightarrow \hat \beta$
        \item Estimated model: $Y_i = \beta X_i + \gamma Z_i + u_i \Rightarrow \tilde \beta$
        \item In our derivation, remember to plug in the true model. Note $Z_i, X_i$ are nonrandom. If Z and X are uncorrelated with epsilon: $E[Z_i \epsilon_i] = 0, E[X\epsilon_i] = 0$ (no correlation with error), then we are fine (NO PROBLEM). If there is a correlation with epsilon, then we do have a higher variance
    \end{itemize}
\end{itemize}

\subsection{Measurement Error}
\begin{itemize}
    \item $Y_i^* = \beta_1 + \beta_2 X_i ^* +\epsilon_i$
    \item The $X_i ^*$ are assumed fixed, but $X_i ^*, Y_i ^*$ are not observed directly. Instead, we observe the mismeasured $X_i, Y_i: Y_i^* + \nu_i$
    \item Likewise, we assume $X_i = X_i ^* + \omega_i$
    \item Even though the $X_i^*$ are fixed, the $X_i$ are random
    \item Greater the measurement error, the greater the variance of your regression terms, the worse your regression will be
    \item $Y_i = \beta_1 + \beta_2 X_i + \epsilon_i$
    \item True model: $Y_i = \beta_1 + \beta_2 X_i + \epsilon_i$
    \item Bad model: $Y_i^* + v_i = \beta_1 + \beta_2 (X_i ^* + \omega_i) + \epsilon_i$
    \item \textbf{Classical Measurement Error}
    \begin{itemize}
        \item We make the following assumptions for this model:
        \begin{itemize}
            \item As usual $E(\epsilon_i) = 0$
            \item $E(\nu_i)=0, E(\nu_i^2) = \sigma^2 _\nu, E(\nu_i \nu_j) = 0\forall i\neq j$
            \item $E(\omega_i) = 0, E(\omega_i ^2) = \sigma^2_\omega, E(\omega_i\omega_j) = 0\forall i\neq j$
            \begin{itemize}
                \item We need to assume that $\sigma^2 > 0$ for variance and randomness (not a point-mass)
            \end{itemize}
            \item $E(\nu_i\omega_j) = E(\nu_i \omega_j) = E(\omega_i \epsilon_j) = 0\forall i,j$
        \end{itemize}
        \item Expectation of product of error terms is 0, because uncorrelated (independent). We expect error to be 0. These assumptions need not be correct, these are modeling assumptions.
        \item $(Y_i - \nu_i ) = \beta_1 + \beta_2(X_i -\omega_i) + \epsilon_i \leftarrow $ true model: $Y^*_i = \beta_1 + \beta_2 X_i^* + \epsilon_i$
        \item $Y_i = \beta_1 + \beta_2 X_i + \epsilon_i + \nu_i - \beta_2 \omega_i$, let $\eta_i = \epsilon_i + \nu_i - \beta_2 \omega_i$. This $\eta_i$ is our new error term
        \item \textbf{Recall:} The error term accounts for everything we cannot see
        \item Compute covariance of $Cov(X_i,Y_i) = E[X_i\eta_i] - E[X_i] E[\eta_i]$
        \begin{itemize}
            \item Plug in true model
            \item \textbf{Note that (using true model):} $E(X_i \eta_i) = E[(X_i^* + \omega_i )(\epsilon_i + \nu _i - \beta_2 \omega_i)] = \beta_2 \sigma_\omega^2$
            \begin{itemize}
                \item $E[X_i^* \epsilon_i] + E[X_i^* \nu_i ] - E[X_i^* \beta_2 \omega_i] + E[\epsilon_i \omega_i] + E[\omega_i \nu_i] - \beta_2 E[\omega_i^2]$ (distributed and multiplied out everything)
                \item \textbf{Recall:} we assume that there is ZERO correlation between error terms and $X$, which reflects in those expectations being 0. This leaves: $E[\epsilon_i \omega_i] + E[\omega_i \nu_i] - \beta_2 E[\omega_i ^2]$
            \end{itemize}
        \end{itemize}
        \item \textbf{Recall:} $E[\hat \beta_2] = E\left[\frac{\sum_i (Y_i - \bar Y)(X_i - \bar X)}{\sum_i (X_i - \bar X)^2}\right]$
        \begin{itemize}
            \item Plug in our model $Y_i = \beta_1 + \beta_2 X_i + \eta_i$
            \item We want to see $E[\hat \beta] = \beta$
            \item $\tilde \beta_2 = \frac{\sum X_i Y_i }{\sum X_i^2} = \frac{X_i (\beta X_i + \eta_i)}{\sum X_i^2} = \beta_2 + \frac{\sum X_i \eta_i}{\sum X_i ^2}) $ \textbf{TODO}
            \item At the end, we show that $plin \tilde \beta = \beta + \frac{-\beta \sigma^2 _\omega}{\sigma^2 _{X^*} + \sigma^2 _\omega}$, so our estimator plus our bias term
            \item What does the bias term stand for?
            \begin{itemize}
                \item True $\beta_2$ times variance of $X_i = X_i^* + \omega_i$, where $E[\omega_i] = \sigma^2_\omega$. In denominator, $\sigma^2_{X^*}$ should be $0$, since it is deterministic
            \end{itemize}
            \item \textbf{IMPORTANT:} This is called the ``Iron Law of Econometrics"
                \begin{itemize}
                    \item $\hat \beta_2 = \beta[1- \frac{\sigma^2 _\omega}{\omega^2_{X^*} + \sigma^2 _\omega}]$
                    \item Let's call the bias term $A$, where $A\in (0,1]$. The bias term \textbf{decreases} the slope, meaning our $\hat \beta_2 \Rightarrow$ biased towards 0 
                    \item If $\sigma^2_\omega \text{ Large } \rightarrow \text{Lots of noise in data} \rightarrow \text{ undershoot the true value in magnitude!}$
                    \item \textbf{Note:} We \textbf{don't} include the measurement error in $Y_i$ as represented by $\sigma^2_\nu$, meaning that measurement error in $Y_i$ \textbf{DOES NOT} bias $\hat \beta_2$ in any way
                    \item Why? Since, $Y-i = Y^*_i +\nu_i$ gets lumped in with our $\epsilon_i$: $Y_i = \beta_1 + \beta_2 X_i^* + \nu_i + \epsilon_i$. Measurement error in $Y_i$ matters for variance, but is an independent term from $\beta_2$, so it does not affect bias
                    \item Large variance leads to undershooting on the t-statistic test, since we decrease SE[$\hat \beta$], as $SE[\hat \beta] = \sqrt{Var(\hat \beta)}$
                \end{itemize}
        \end{itemize}
    \end{itemize}
\end{itemize}

%%% FOOTER %%%
\input{footer}

\end{document}
